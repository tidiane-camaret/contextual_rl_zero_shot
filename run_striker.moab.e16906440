+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ module load devel/python/3.9.7
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_sh_dbg=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output
Shell debugging restarted
+ unset __lmod_sh_dbg
+ return 0
+ module load devel/conda
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_sh_dbg=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output
Lmod has detected the following error: Cannot load module "tools/conda/latest"
because these module(s) are loaded:
   devel/python

While processing the following module(s):
    Module fullname     Module Filename
    ---------------     ---------------
    tools/conda/latest  /opt/bwhpc/modulefiles/development/common/tools/conda/latest.lua

Shell debugging restarted
+ unset __lmod_sh_dbg
+ return 1
+ module load mpi/openmpi/4.1-gnu-9.2
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_sh_dbg=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output
Shell debugging restarted
+ unset __lmod_sh_dbg
+ return 0
+ export CPATH=/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/include
+ CPATH=/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/include
+ source miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/fr/fr_fr/fr_tn110/miniconda3/bin/conda
++ CONDA_EXE=/home/fr/fr_fr/fr_tn110/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/fr/fr_fr/fr_tn110/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/fr/fr_fr/fr_tn110/miniconda3/bin/python
++ '[' -z x ']'
+ conda activate tid_env
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate tid_env
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate tid_env
++ /home/fr/fr_fr/fr_tn110/miniconda3/bin/conda shell.posix activate tid_env
+ ask_conda='. "/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/etc/conda/deactivate.d/xgboost_deactivate.sh"
. "/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/etc/conda/deactivate.d/mpivars.deactivate.sh"
PS1='\''(tid_env) '\''
export PATH='\''/opt/bwhpc/common/compiler/gnu/9.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin:/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/condabin:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/opt/moab/bin:/opt/VirtualGL/bin:/bin:/usr/local/bin:/usr/local/sbin:/home/fr/fr_fr/fr_tn110/.local/bin:/home/fr/fr_fr/fr_tn110/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin/libfabric'\''
export CONDA_PREFIX='\''/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''tid_env'\''
export CONDA_PROMPT_MODIFIER='\''(tid_env) '\''
export CONDA_PREFIX_1='\''/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest'\''
export CONDA_EXE='\''/home/fr/fr_fr/fr_tn110/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/fr/fr_fr/fr_tn110/miniconda3/bin/python'\'''
+ eval '. "/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/etc/conda/deactivate.d/xgboost_deactivate.sh"
. "/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/etc/conda/deactivate.d/mpivars.deactivate.sh"
PS1='\''(tid_env) '\''
export PATH='\''/opt/bwhpc/common/compiler/gnu/9.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin:/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/condabin:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/opt/moab/bin:/opt/VirtualGL/bin:/bin:/usr/local/bin:/usr/local/sbin:/home/fr/fr_fr/fr_tn110/.local/bin:/home/fr/fr_fr/fr_tn110/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin/libfabric'\''
export CONDA_PREFIX='\''/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''tid_env'\''
export CONDA_PROMPT_MODIFIER='\''(tid_env) '\''
export CONDA_PREFIX_1='\''/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest'\''
export CONDA_EXE='\''/home/fr/fr_fr/fr_tn110/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/fr/fr_fr/fr_tn110/miniconda3/bin/python'\'''
++ . /opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/etc/conda/deactivate.d/xgboost_deactivate.sh
+++ '[' 1 = 1 ']'
+++ unset OCL_ICD_FILENAMES_RESET
+++ unset OCL_ICD_FILENAMES
++ . /opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/etc/conda/deactivate.d/mpivars.deactivate.sh
+++ '[' '' '!=' 1 ']'
++++ echo /opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib/mpi.jar
++++ sed 's|/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib/mpi.jar:\?||'
+++ export CLASSPATH=
+++ CLASSPATH=
++++ echo /opt/bwhpc/common/compiler/gnu/9.2.0/lib64:/opt/bwhpc/common/compiler/gnu/9.2.0/lib:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib::/home/fr/fr_fr/fr_tn110/.mujoco/mujoco210/bin:/usr/lib/nvidia:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib/libfabric
++++ sed 's|/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib/libfabric:\?||'
+++ export LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/9.2.0/lib64:/opt/bwhpc/common/compiler/gnu/9.2.0/lib:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib::/home/fr/fr_fr/fr_tn110/.mujoco/mujoco210/bin:/usr/lib/nvidia:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib:
+++ LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/9.2.0/lib64:/opt/bwhpc/common/compiler/gnu/9.2.0/lib:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib::/home/fr/fr_fr/fr_tn110/.mujoco/mujoco210/bin:/usr/lib/nvidia:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib:
++++ echo /opt/bwhpc/common/compiler/gnu/9.2.0/lib64:/opt/bwhpc/common/compiler/gnu/9.2.0/lib:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib::/home/fr/fr_fr/fr_tn110/.mujoco/mujoco210/bin:/usr/lib/nvidia:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib:
++++ sed 's|/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib:\?||'
+++ export LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/9.2.0/lib64:/opt/bwhpc/common/compiler/gnu/9.2.0/lib:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib::/home/fr/fr_fr/fr_tn110/.mujoco/mujoco210/bin:/usr/lib/nvidia:
+++ LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/9.2.0/lib64:/opt/bwhpc/common/compiler/gnu/9.2.0/lib:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/lib::/home/fr/fr_fr/fr_tn110/.mujoco/mujoco210/bin:/usr/lib/nvidia:
++++ echo /opt/bwhpc/common/compiler/gnu/9.2.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/share/man:/usr/share/lmod/lmod/share/man::/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/share/man
++++ sed 's|/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/share/man:\?||'
+++ export MANPATH=/opt/bwhpc/common/compiler/gnu/9.2.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/share/man:/usr/share/lmod/lmod/share/man::
+++ MANPATH=/opt/bwhpc/common/compiler/gnu/9.2.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/share/man:/usr/share/lmod/lmod/share/man::
++++ echo /opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib/libfabric
++++ sed 's|/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib/libfabric:\?||'
+++ export LIBRARY_PATH=/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib:
+++ LIBRARY_PATH=/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib:
++++ echo /opt/bwhpc/common/compiler/gnu/9.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/condabin:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/opt/moab/bin:/opt/VirtualGL/bin:/bin:/usr/local/bin:/usr/local/sbin:/home/fr/fr_fr/fr_tn110/.local/bin:/home/fr/fr_fr/fr_tn110/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin/libfabric
++++ sed 's|/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin/libfabric:||'
+++ export PATH=/opt/bwhpc/common/compiler/gnu/9.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/condabin:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/opt/moab/bin:/opt/VirtualGL/bin:/bin:/usr/local/bin:/usr/local/sbin:/home/fr/fr_fr/fr_tn110/.local/bin:/home/fr/fr_fr/fr_tn110/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin/libfabric
+++ PATH=/opt/bwhpc/common/compiler/gnu/9.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/condabin:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/opt/moab/bin:/opt/VirtualGL/bin:/bin:/usr/local/bin:/usr/local/sbin:/home/fr/fr_fr/fr_tn110/.local/bin:/home/fr/fr_fr/fr_tn110/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin/libfabric
++++ which fi_info
+++ FIP=/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin/fi_info
+++ echo /opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin/fi_info
+++ grep -q 'compilers_and_libraries.*mpi'
+++ echo /opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin/fi_info
+++ grep -q /mpi/
++++ echo /opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin/fi_info
++++ rev
++++ cut -f1 '-d '
++++ rev
++++ sed 's|/libfabric/bin/fi_info||'
+++ export I_MPI_ROOT=/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin/fi_info
+++ I_MPI_ROOT=/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin/fi_info
+++ echo /opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/lib/libfabric/prov:/usr/lib64/libfabric
+++ grep -q /opt/bwhpc/intel/oneapi/2022.1/intelpython/latest
+++ [[ /opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin/fi_info != '' ]]
+++ echo /opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin/fi_info
+++ grep -q 'compilers_and_libraries.*mpi'
+++ export FI_PROVIDER_PATH=/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin/fi_info/libfabric/lib/prov:/usr/lib64/libfabric
+++ FI_PROVIDER_PATH=/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin/fi_info/libfabric/lib/prov:/usr/lib64/libfabric
++ PS1='(tid_env) '
++ export PATH=/opt/bwhpc/common/compiler/gnu/9.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin:/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/condabin:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/opt/moab/bin:/opt/VirtualGL/bin:/bin:/usr/local/bin:/usr/local/sbin:/home/fr/fr_fr/fr_tn110/.local/bin:/home/fr/fr_fr/fr_tn110/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin/libfabric
++ PATH=/opt/bwhpc/common/compiler/gnu/9.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-9.2/bin:/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/condabin:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/opt/moab/bin:/opt/VirtualGL/bin:/bin:/usr/local/bin:/usr/local/sbin:/home/fr/fr_fr/fr_tn110/.local/bin:/home/fr/fr_fr/fr_tn110/bin:/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest/bin/libfabric
++ export CONDA_PREFIX=/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env
++ CONDA_PREFIX=/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=tid_env
++ CONDA_DEFAULT_ENV=tid_env
++ export 'CONDA_PROMPT_MODIFIER=(tid_env) '
++ CONDA_PROMPT_MODIFIER='(tid_env) '
++ export CONDA_PREFIX_1=/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest
++ CONDA_PREFIX_1=/opt/bwhpc/intel/oneapi/2022.1/intelpython/latest
++ export CONDA_EXE=/home/fr/fr_fr/fr_tn110/miniconda3/bin/conda
++ CONDA_EXE=/home/fr/fr_fr/fr_tn110/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/fr/fr_fr/fr_tn110/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/fr/fr_fr/fr_tn110/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ mpirun python dev/automl/meta_rl/scripts/orig_impl/striker_baselines.py --context explicit
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_012622-pr1s5nmz
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Tracking run with wandb version 0.13.5
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_012622-1mez24fw
wandb: Run `wandb offline` to turn off syncing.
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_012622-15t9fds4
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_012622-1jxiownk
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_012622-c23holdz
wandb: Run `wandb offline` to turn off syncing.
wandb: Tracking run with wandb version 0.13.5
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_012622-q658kpqh
wandb: Run `wandb offline` to turn off syncing.
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_012622-15smaiuw
wandb: Run `wandb offline` to turn off syncing.
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_012622-37wdxx29
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_012622-2fbip4iu
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_012622-2din8w6f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-jazz-32
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/2fbip4iu
wandb: Syncing run volcanic-bird-27
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/pr1s5nmz
wandb: Syncing run azure-valley-27
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/1mez24fw
wandb: Syncing run misunderstood-monkey-27
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: Syncing run colorful-cherry-27
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/q658kpqh
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/15t9fds4
wandb: Syncing run glowing-blaze-23
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/2din8w6f
wandb: Syncing run spring-surf-24
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/1jxiownk
wandb: Syncing run different-firefly-23
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/c23holdz
wandb: Syncing run wise-totem-24
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/15smaiuw
wandb: Syncing run pretty-dust-31
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/37wdxx29
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: Waiting for W&B process to finish... (success).
wandb: Waiting for W&B process to finish... (success).
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1113/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1113/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1113/rollout/ep_rew_mean ▂▁▂▃▃▅▄▆▆▆▆█
wandb:                   PPO_1113/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1113/train/approx_kl ▂▅▁██▆▄▅█▇▆
wandb:        PPO_1113/train/clip_fraction ▂▅▁▆▆▆▆▇█▇▆
wandb:           PPO_1113/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1113/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1113/train/explained_variance ▂▁▄█▇▇▅▅▁▄▆
wandb:        PPO_1113/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1113/train/loss ▃▂▁▂▂▂█▅▄▄▄
wandb: PPO_1113/train/policy_gradient_loss ▇▃█▃▁▅▆▃▆▆█
wandb:                  PPO_1113/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1113/train/value_loss ▁▄▄▃▆▆█▇▆▄█
wandb:                PPO_1123/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1123/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1123/rollout/ep_rew_mean ▁▂▃▃▄▅▅▅▆▇██
wandb:                   PPO_1123/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1123/train/approx_kl ▁▄▂▆▃▃▄▇█▇▇
wandb:        PPO_1123/train/clip_fraction ▁▅▁▄▄▄▅▇█▇▇
wandb:           PPO_1123/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1123/train/entropy_loss ▁▂▂▃▃▄▅▆▆▇█
wandb:   PPO_1123/train/explained_variance ▃▁▅█▁▁▃▅▆▃▃
wandb:        PPO_1123/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1123/train/loss ▂▆▆█▅▂▁▇▃▁▂
wandb: PPO_1123/train/policy_gradient_loss ▇▁▅▆▅▄▄▃▂▆█
wandb:                  PPO_1123/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1123/train/value_loss ▇▄█▄▄▅▅▄▂▄▁
wandb:                PPO_1134/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1134/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1134/rollout/ep_rew_mean ▂▁▃▃▄▅▅▆▅▅█▆
wandb:                   PPO_1134/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1134/train/approx_kl ▂█▂▆▅▆▇▇▁█▅
wandb:        PPO_1134/train/clip_fraction ▃▇▃█▇█▇█▁██
wandb:           PPO_1134/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1134/train/entropy_loss ▁▂▃▃▄▅▆▆▇▇█
wandb:   PPO_1134/train/explained_variance ▇▆▇▇▄█▇▆▁▃▄
wandb:        PPO_1134/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1134/train/loss ▄▆▄█▂▂▃▂▄▄▁
wandb: PPO_1134/train/policy_gradient_loss ▇▅▇▅▁▇▇█▅▆▆
wandb:                  PPO_1134/train/std █▇▆▆▅▄▄▃▃▃▁
wandb:           PPO_1134/train/value_loss ▆▃▅▂▂▁▁▁█▅▅
wandb:                PPO_1144/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1144/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1144/rollout/ep_rew_mean ▁▂▅▄▃▅▃▅▂█▅▄
wandb:                   PPO_1144/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1144/train/approx_kl ▃▇▁▄▄▂▄▇▂▂█
wandb:        PPO_1144/train/clip_fraction ▅█▂▅▅▅▁▆▁▄▅
wandb:           PPO_1144/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1144/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1144/train/explained_variance ▂▄▁▄▆▆▇▇▇▇█
wandb:        PPO_1144/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1144/train/loss ▂▂▃▇▁▁▃█▃▂▅
wandb: PPO_1144/train/policy_gradient_loss ▁▃▃▄▄█▂▁▄▃▅
wandb:                  PPO_1144/train/std █▇▇▆▅▅▄▃▃▂▁
wandb:           PPO_1144/train/value_loss ▄▁██▂▃▅▂▄▅▄
wandb:                PPO_1153/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1153/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1153/rollout/ep_rew_mean ▆█▁▄▆▂▅▅▃▅▆▆
wandb:                   PPO_1153/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1153/train/approx_kl ▁▃▂▃█▁▄▄▃▅▆
wandb:        PPO_1153/train/clip_fraction ▆▄▃▄▆▁▇▇▂▅█
wandb:           PPO_1153/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1153/train/entropy_loss ▁▂▂▃▄▄▅▅▆▆█
wandb:   PPO_1153/train/explained_variance ▁▃▆▄▅▅▄▇▇▇█
wandb:        PPO_1153/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1153/train/loss ▂▁▂▂▁█▁▁█▃▄
wandb: PPO_1153/train/policy_gradient_loss ▇▆██▇▆▇▇▁▇▆
wandb:                  PPO_1153/train/std █▇▇▆▅▅▄▃▃▃▁
wandb:           PPO_1153/train/value_loss ▅▅▃▆▅█▆▁▅▆▂
wandb:                PPO_1163/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1163/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1163/rollout/ep_rew_mean ▅▆▂▁▃▃▅▅▃▆▆█
wandb:                   PPO_1163/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1163/train/approx_kl ▁▆▃▂▃▃▇▅▄▇█
wandb:        PPO_1163/train/clip_fraction ▂▅▂▁▅▃▄▄▅█▇
wandb:           PPO_1163/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1163/train/entropy_loss ▁▂▂▂▃▄▄▄▅▆█
wandb:   PPO_1163/train/explained_variance ▅▄▄▁▆▆▄▂█▆▅
wandb:        PPO_1163/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1163/train/loss ▇█▆▇█▂▃▅▄▂▁
wandb: PPO_1163/train/policy_gradient_loss ▁▂▁▅▆█▄▂▇▃▃
wandb:                  PPO_1163/train/std █▇▇▇▆▅▅▄▄▃▁
wandb:           PPO_1163/train/value_loss ▃▃▄█▃▄▃▄▂▂▁
wandb:                PPO_1173/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1173/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1173/rollout/ep_rew_mean ▃▁▅▅▄▃▃▃▂▇▄█
wandb:                   PPO_1173/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1173/train/approx_kl ▃▃▄▅▃▅▅█▁▆█
wandb:        PPO_1173/train/clip_fraction ▃▃▄▂▄▃▅█▁▆▆
wandb:           PPO_1173/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1173/train/entropy_loss ▁▁▂▃▄▄▅▅▆▇█
wandb:   PPO_1173/train/explained_variance ▅▆▄▆▆▆▅▇▄█▁
wandb:        PPO_1173/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1173/train/loss ▇▄▂▂▂█▁▁▃▂▂
wandb: PPO_1173/train/policy_gradient_loss ▅▄▄▁▆▃▆▆▂█▆
wandb:                  PPO_1173/train/std █▇▇▆▅▄▄▄▃▂▁
wandb:           PPO_1173/train/value_loss ▆▅▄▆▄▇▆▁▆▁█
wandb:                PPO_1183/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1183/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1183/rollout/ep_rew_mean ▆▃▁▆▅▄▆▆▅▄██
wandb:                   PPO_1183/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1183/train/approx_kl ▂▂▁▃▄▄▄▆▅█▅
wandb:        PPO_1183/train/clip_fraction ▆▅▁▇██▇▆▇█▆
wandb:           PPO_1183/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1183/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1183/train/explained_variance ▅▆▁▅▂▆█▇▇▆▅
wandb:        PPO_1183/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1183/train/loss ▄▄▄▆▅▁▃▃▂▃█
wandb: PPO_1183/train/policy_gradient_loss ▅▃▁█▆▆▄▅▁█▂
wandb:                  PPO_1183/train/std █▇▇▆▅▅▄▄▃▃▁
wandb:           PPO_1183/train/value_loss ▅▅█▄▃▃▁▂▁▃▃
wandb:                PPO_1193/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1193/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1193/rollout/ep_rew_mean ▃▄▁▅▅▄▅▅▆▅▄█
wandb:                   PPO_1193/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1193/train/approx_kl ▅█▅▃▃▄▄▁▅▃▁
wandb:        PPO_1193/train/clip_fraction ▄█▆▆▁▇▅▄▆▆▅
wandb:           PPO_1193/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1193/train/entropy_loss ▁▂▂▃▃▄▅▅▆▆█
wandb:   PPO_1193/train/explained_variance ▇▃▇▆▁▅█▇▇█▇
wandb:        PPO_1193/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1193/train/loss ▂▄▃▁█▁▂▆▂▅▁
wandb: PPO_1193/train/policy_gradient_loss ▁▄▄▅▁█▅▂▄▅▅
wandb:                  PPO_1193/train/std █▇▇▆▆▅▄▄▃▃▁
wandb:           PPO_1193/train/value_loss ▂▆▄▄█▅▁▂▂▁▃
wandb:                    global_mean_eval ▁▃▄▆▅▆▇▇██
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▂▄▆▅▆▇▇██
wandb:                       mean_reward_1 ▁▂▄▅▅▆▆▇██
wandb:                      mean_reward_10 ▁▂▄▆▅▆▇▇██
wandb:                      mean_reward_11 ▁▂▄▅▅▆▆▇▇█
wandb:                      mean_reward_12 ▁▃▅▅▅▆▇▇██
wandb:                      mean_reward_13 ▁▃▄▆▅▆▆▇██
wandb:                      mean_reward_14 ▁▂▄▆▆▆▆███
wandb:                      mean_reward_15 ▁▂▄▆▅▆▆▇██
wandb:                      mean_reward_16 ▁▃▄▆▅▆▇▇██
wandb:                      mean_reward_17 ▁▂▄▅▅▆▇█▇█
wandb:                      mean_reward_18 ▁▃▅▆▅▆▇▇██
wandb:                      mean_reward_19 ▁▃▄▅▅▇▇▇▇█
wandb:                       mean_reward_2 ▁▃▄▆▅▆▇███
wandb:                      mean_reward_20 ▁▂▄▅▅▅▆▆▇█
wandb:                      mean_reward_21 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_22 ▁▃▄▆▅▇▇▇█▇
wandb:                      mean_reward_23 ▁▃▅▆▅▆▇▇██
wandb:                      mean_reward_24 ▁▂▄▆▆▇▆▇▇█
wandb:                      mean_reward_25 ▁▃▅▆▆▇████
wandb:                      mean_reward_26 ▁▃▄▅▅▆▆█▇█
wandb:                      mean_reward_27 ▁▃▄▆▅▆▇▇██
wandb:                      mean_reward_28 ▁▃▄▆▅▇▆▇▇█
wandb:                      mean_reward_29 ▁▃▅▆▅▇▇███
wandb:                       mean_reward_3 ▁▂▄▅▅▆▇▇██
wandb:                      mean_reward_30 ▁▃▄▆▅▇▇███
wandb:                      mean_reward_31 ▁▂▄▆▅▆▇▇██
wandb:                      mean_reward_32 ▁▃▄▅▅▆▆▇▇█
wandb:                      mean_reward_33 ▁▃▄▅▅▆▇▇██
wandb:                      mean_reward_34 ▁▃▅▆▆▇▆███
wandb:                      mean_reward_35 ▁▃▅▆▅▆▇▇██
wandb:                       mean_reward_4 ▁▃▄▆▅▆▇▇██
wandb:                       mean_reward_5 ▁▃▅▆▅▆▇▇██
wandb:                       mean_reward_6 ▁▃▅▆▆▆▇███
wandb:                       mean_reward_7 ▁▃▅▆▅▇▇███
wandb:                       mean_reward_8 ▁▂▄▅▆▅▆▇▇█
wandb:                       mean_reward_9 ▁▃▄▆▅▇▇▇▇█
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▁▂▁▃▂▃▄▅▆▆█
wandb:                        std_reward_0 ▂▁▂▃█▇▆▆▅▅
wandb:                        std_reward_1 ▃▁▂▂█▇▆▄▄▅
wandb:                       std_reward_10 ▂▁▂▃█▇▆▆▅▅
wandb:                       std_reward_11 ▂▁▂▃█▆▇▅▆▅
wandb:                       std_reward_12 ▂▁▂▃█▇▆▅▅▅
wandb:                       std_reward_13 ▂▁▂▃█▇▆▅▅▅
wandb:                       std_reward_14 ▂▁▂▃███▆▅▆
wandb:                       std_reward_15 ▃▁▂▃█▇█▇▅▆
wandb:                       std_reward_16 ▂▁▂▃█▇▇▅▅▅
wandb:                       std_reward_17 ▂▁▂▃█▆▅▄▅▄
wandb:                       std_reward_18 ▂▁▂▃█▇▆▆▅▆
wandb:                       std_reward_19 ▂▁▂▂█▆▆▅▅▅
wandb:                        std_reward_2 ▂▁▂▂██▆▅▅▆
wandb:                       std_reward_20 ▂▁▂▃▇█▆▅▅▄
wandb:                       std_reward_21 ▂▁▂▃▇█▆▅▅▅
wandb:                       std_reward_22 ▃▁▂▃█▇▆▆▆▇
wandb:                       std_reward_23 ▂▁▂▃█▇▆▅▅▅
wandb:                       std_reward_24 ▂▁▂▄▇▆█▆▇▆
wandb:                       std_reward_25 ▂▁▂▃█▇▆▅▆▆
wandb:                       std_reward_26 ▂▁▂▃█▇▇▄▅▄
wandb:                       std_reward_27 ▂▁▂▃█▇▇▅▅▅
wandb:                       std_reward_28 ▂▁▂▃█▆▇▄▅▅
wandb:                       std_reward_29 ▂▁▂▄█▅▆▄▅▅
wandb:                        std_reward_3 ▂▁▂▃█▇▆▅▅▅
wandb:                       std_reward_30 ▃▁▂▄█▇▇▅▅▆
wandb:                       std_reward_31 ▂▁▂▃█▆▅▆▅▅
wandb:                       std_reward_32 ▂▁▂▃██▇▅▆▅
wandb:                       std_reward_33 ▂▁▂▃█▇▆▆▄▆
wandb:                       std_reward_34 ▂▁▂▃█▇█▅▆▆
wandb:                       std_reward_35 ▂▁▂▂█▆▅▄▅▅
wandb:                        std_reward_4 ▂▁▂▃█▇▆▅▅▅
wandb:                        std_reward_5 ▂▁▂▂██▅▆▅▆
wandb:                        std_reward_6 ▂▁▂▃██▆▆▅▆
wandb:                        std_reward_7 ▂▁▂▃█▆▆▅▅▅
wandb:                        std_reward_8 ▂▁▂▃▇█▇▄▅▄
wandb:                        std_reward_9 ▂▁▂▃█▆▆▅▆▅
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▂▃█▁▂▂▄▃▄▆▆
wandb:                 train/clip_fraction ▃▃▃▄▄▁▃▅▅▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▃▆▇▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▃▂▂▁▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▆▆▅█▆▅▃▄▂▁▂
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1113/global_step 212992
wandb:        PPO_1113/rollout/ep_len_mean 200.0
wandb:        PPO_1113/rollout/ep_rew_mean -783.6427
wandb:                   PPO_1113/time/fps 1203.0
wandb:            PPO_1113/train/approx_kl 0.01163
wandb:        PPO_1113/train/clip_fraction 0.14694
wandb:           PPO_1113/train/clip_range 0.2
wandb:         PPO_1113/train/entropy_loss -7.62233
wandb:   PPO_1113/train/explained_variance 0.96497
wandb:        PPO_1113/train/learning_rate 0.0003
wandb:                 PPO_1113/train/loss 31.05031
wandb: PPO_1113/train/policy_gradient_loss -0.0065
wandb:                  PPO_1113/train/std 0.71703
wandb:           PPO_1113/train/value_loss 83.6244
wandb:                PPO_1123/global_step 212992
wandb:        PPO_1123/rollout/ep_len_mean 200.0
wandb:        PPO_1123/rollout/ep_rew_mean -638.77991
wandb:                   PPO_1123/time/fps 1197.0
wandb:            PPO_1123/train/approx_kl 0.01413
wandb:        PPO_1123/train/clip_fraction 0.18827
wandb:           PPO_1123/train/clip_range 0.2
wandb:         PPO_1123/train/entropy_loss -6.77483
wandb:   PPO_1123/train/explained_variance 0.96641
wandb:        PPO_1123/train/learning_rate 0.0003
wandb:                 PPO_1123/train/loss 19.96198
wandb: PPO_1123/train/policy_gradient_loss -0.00594
wandb:                  PPO_1123/train/std 0.63692
wandb:           PPO_1123/train/value_loss 53.8143
wandb:                PPO_1134/global_step 212992
wandb:        PPO_1134/rollout/ep_len_mean 200.0
wandb:        PPO_1134/rollout/ep_rew_mean -587.69714
wandb:                   PPO_1134/time/fps 1195.0
wandb:            PPO_1134/train/approx_kl 0.01521
wandb:        PPO_1134/train/clip_fraction 0.21024
wandb:           PPO_1134/train/clip_range 0.2
wandb:         PPO_1134/train/entropy_loss -5.92234
wandb:   PPO_1134/train/explained_variance 0.95604
wandb:        PPO_1134/train/learning_rate 0.0003
wandb:                 PPO_1134/train/loss 7.2029
wandb: PPO_1134/train/policy_gradient_loss -0.0041
wandb:                  PPO_1134/train/std 0.56183
wandb:           PPO_1134/train/value_loss 70.62495
wandb:                PPO_1144/global_step 212992
wandb:        PPO_1144/rollout/ep_len_mean 200.0
wandb:        PPO_1144/rollout/ep_rew_mean -542.8963
wandb:                   PPO_1144/time/fps 1201.0
wandb:            PPO_1144/train/approx_kl 0.01699
wandb:        PPO_1144/train/clip_fraction 0.21124
wandb:           PPO_1144/train/clip_range 0.2
wandb:         PPO_1144/train/entropy_loss -5.29417
wandb:   PPO_1144/train/explained_variance 0.99009
wandb:        PPO_1144/train/learning_rate 0.0003
wandb:                 PPO_1144/train/loss 26.51619
wandb: PPO_1144/train/policy_gradient_loss -0.0017
wandb:                  PPO_1144/train/std 0.51509
wandb:           PPO_1144/train/value_loss 52.94361
wandb:                PPO_1153/global_step 212992
wandb:        PPO_1153/rollout/ep_len_mean 200.0
wandb:        PPO_1153/rollout/ep_rew_mean -526.00867
wandb:                   PPO_1153/time/fps 1189.0
wandb:            PPO_1153/train/approx_kl 0.01848
wandb:        PPO_1153/train/clip_fraction 0.23991
wandb:           PPO_1153/train/clip_range 0.2
wandb:         PPO_1153/train/entropy_loss -4.75893
wandb:   PPO_1153/train/explained_variance 0.997
wandb:        PPO_1153/train/learning_rate 0.0003
wandb:                 PPO_1153/train/loss 24.30634
wandb: PPO_1153/train/policy_gradient_loss -0.0033
wandb:                  PPO_1153/train/std 0.47853
wandb:           PPO_1153/train/value_loss 29.46359
wandb:                PPO_1163/global_step 212992
wandb:        PPO_1163/rollout/ep_len_mean 200.0
wandb:        PPO_1163/rollout/ep_rew_mean -484.41364
wandb:                   PPO_1163/time/fps 1196.0
wandb:            PPO_1163/train/approx_kl 0.02016
wandb:        PPO_1163/train/clip_fraction 0.25532
wandb:           PPO_1163/train/clip_range 0.2
wandb:         PPO_1163/train/entropy_loss -4.24085
wandb:   PPO_1163/train/explained_variance 0.9968
wandb:        PPO_1163/train/learning_rate 0.0003
wandb:                 PPO_1163/train/loss 2.52387
wandb: PPO_1163/train/policy_gradient_loss -0.00223
wandb:                  PPO_1163/train/std 0.44332
wandb:           PPO_1163/train/value_loss 18.39738
wandb:                PPO_1173/global_step 212992
wandb:        PPO_1173/rollout/ep_len_mean 200.0
wandb:        PPO_1173/rollout/ep_rew_mean -437.41681
wandb:                   PPO_1173/time/fps 1196.0
wandb:            PPO_1173/train/approx_kl 0.0235
wandb:        PPO_1173/train/clip_fraction 0.27603
wandb:           PPO_1173/train/clip_range 0.2
wandb:         PPO_1173/train/entropy_loss -3.5009
wandb:   PPO_1173/train/explained_variance 0.99564
wandb:        PPO_1173/train/learning_rate 0.0003
wandb:                 PPO_1173/train/loss 6.18074
wandb: PPO_1173/train/policy_gradient_loss 0.0006
wandb:                  PPO_1173/train/std 0.39907
wandb:           PPO_1173/train/value_loss 27.60092
wandb:                PPO_1183/global_step 212992
wandb:        PPO_1183/rollout/ep_len_mean 200.0
wandb:        PPO_1183/rollout/ep_rew_mean -403.60742
wandb:                   PPO_1183/time/fps 1195.0
wandb:            PPO_1183/train/approx_kl 0.0224
wandb:        PPO_1183/train/clip_fraction 0.26947
wandb:           PPO_1183/train/clip_range 0.2
wandb:         PPO_1183/train/entropy_loss -2.71409
wandb:   PPO_1183/train/explained_variance 0.99787
wandb:        PPO_1183/train/learning_rate 0.0003
wandb:                 PPO_1183/train/loss 12.69099
wandb: PPO_1183/train/policy_gradient_loss 0.00053
wandb:                  PPO_1183/train/std 0.356
wandb:           PPO_1183/train/value_loss 11.01046
wandb:                PPO_1193/global_step 212992
wandb:        PPO_1193/rollout/ep_len_mean 200.0
wandb:        PPO_1193/rollout/ep_rew_mean -377.57779
wandb:                   PPO_1193/time/fps 1192.0
wandb:            PPO_1193/train/approx_kl 0.02257
wandb:        PPO_1193/train/clip_fraction 0.29405
wandb:           PPO_1193/train/clip_range 0.2
wandb:         PPO_1193/train/entropy_loss -2.21128
wandb:   PPO_1193/train/explained_variance 0.99677
wandb:        PPO_1193/train/learning_rate 0.0003
wandb:                 PPO_1193/train/loss 2.90452
wandb: PPO_1193/train/policy_gradient_loss 0.00367
wandb:                  PPO_1193/train/std 0.33238
wandb:           PPO_1193/train/value_loss 19.90571
wandb:                    global_mean_eval -365.40014
wandb:                         global_step 212992
wandb:                       mean_reward_0 -358.31554
wandb:                       mean_reward_1 -349.01791
wandb:                      mean_reward_10 -357.01577
wandb:                      mean_reward_11 -354.50118
wandb:                      mean_reward_12 -369.14581
wandb:                      mean_reward_13 -374.14667
wandb:                      mean_reward_14 -382.07859
wandb:                      mean_reward_15 -377.81415
wandb:                      mean_reward_16 -369.98633
wandb:                      mean_reward_17 -344.12091
wandb:                      mean_reward_18 -367.95172
wandb:                      mean_reward_19 -350.21302
wandb:                       mean_reward_2 -380.4176
wandb:                      mean_reward_20 -345.48409
wandb:                      mean_reward_21 -386.29616
wandb:                      mean_reward_22 -387.74561
wandb:                      mean_reward_23 -364.5172
wandb:                      mean_reward_24 -371.74645
wandb:                      mean_reward_25 -379.2821
wandb:                      mean_reward_26 -343.28399
wandb:                      mean_reward_27 -354.29929
wandb:                      mean_reward_28 -361.85227
wandb:                      mean_reward_29 -373.5488
wandb:                       mean_reward_3 -358.64123
wandb:                      mean_reward_30 -376.45287
wandb:                      mean_reward_31 -358.16096
wandb:                      mean_reward_32 -348.96158
wandb:                      mean_reward_33 -384.782
wandb:                      mean_reward_34 -385.27372
wandb:                      mean_reward_35 -371.36278
wandb:                       mean_reward_4 -355.29938
wandb:                       mean_reward_5 -394.05653
wandb:                       mean_reward_6 -375.5873
wandb:                       mean_reward_7 -359.23986
wandb:                       mean_reward_8 -330.01087
wandb:                       mean_reward_9 -353.79471
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -931.94031
wandb:                        std_reward_0 142.18587
wandb:                        std_reward_1 137.84523
wandb:                       std_reward_10 141.59454
wandb:                       std_reward_11 134.98508
wandb:                       std_reward_12 144.2833
wandb:                       std_reward_13 149.54605
wandb:                       std_reward_14 153.30574
wandb:                       std_reward_15 146.99208
wandb:                       std_reward_16 147.90859
wandb:                       std_reward_17 128.68278
wandb:                       std_reward_18 149.12936
wandb:                       std_reward_19 131.5914
wandb:                        std_reward_2 157.72515
wandb:                       std_reward_20 129.40073
wandb:                       std_reward_21 156.67146
wandb:                       std_reward_22 157.99776
wandb:                       std_reward_23 151.256
wandb:                       std_reward_24 142.20364
wandb:                       std_reward_25 149.27333
wandb:                       std_reward_26 120.88418
wandb:                       std_reward_27 139.31155
wandb:                       std_reward_28 144.46103
wandb:                       std_reward_29 150.37799
wandb:                        std_reward_3 144.92466
wandb:                       std_reward_30 151.04197
wandb:                       std_reward_31 141.19281
wandb:                       std_reward_32 137.44614
wandb:                       std_reward_33 155.09414
wandb:                       std_reward_34 162.90428
wandb:                       std_reward_35 151.28573
wandb:                        std_reward_4 142.87582
wandb:                        std_reward_5 161.2606
wandb:                        std_reward_6 157.36493
wandb:                        std_reward_7 140.58214
wandb:                        std_reward_8 116.59556
wandb:                        std_reward_9 134.36515
wandb:                            time/fps 1178.0
wandb:                     train/approx_kl 0.01244
wandb:                 train/clip_fraction 0.15652
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7411
wandb:            train/explained_variance 0.96718
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 11.08948
wandb:          train/policy_gradient_loss -0.01304
wandb:                           train/std 0.84138
wandb:                    train/value_loss 21.77692
wandb: 
wandb: Synced different-firefly-23: https://wandb.ai/tidiane/meta_rl_context/runs/c23holdz
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_012622-c23holdz/logs
wandb: 
wandb: Run history:
wandb:                PPO_1115/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1115/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1115/rollout/ep_rew_mean ▁▂▁▃▄▄▄▄▆▇▇█
wandb:                   PPO_1115/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1115/train/approx_kl ▁▅▃▆▅▅▅█▆▄▇
wandb:        PPO_1115/train/clip_fraction ▁▄▂▄▄▅▅▅▆▄█
wandb:           PPO_1115/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1115/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1115/train/explained_variance ▁▅▆▆█▇▄▅▅▃█
wandb:        PPO_1115/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1115/train/loss ▃▁▅▁▃▄▄█▃▂▁
wandb: PPO_1115/train/policy_gradient_loss ▅▁▇▄▅▂▅█▅▆▇
wandb:                  PPO_1115/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1115/train/value_loss ▁▃▄▄▄▃▆▇▅█▃
wandb:                PPO_1124/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1124/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1124/rollout/ep_rew_mean ▁▁▃▂▃▃▄▄▅▆▆█
wandb:                   PPO_1124/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1124/train/approx_kl ▄▃▂▅▃▁▅▆█▄▇
wandb:        PPO_1124/train/clip_fraction ▃▅▃▃▂▁▅▅▇▆█
wandb:           PPO_1124/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1124/train/entropy_loss ▁▂▂▃▃▄▅▅▆▇█
wandb:   PPO_1124/train/explained_variance ▆▄▁▄▆█▃▆▆▁▅
wandb:        PPO_1124/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1124/train/loss ▄▂▄▃▂█▂▃▂▁▂
wandb: PPO_1124/train/policy_gradient_loss ▁▃▄▃▄▃▂▃▂▅█
wandb:                  PPO_1124/train/std █▇▇▆▆▅▄▄▃▂▁
wandb:           PPO_1124/train/value_loss ▆▃▅▆▅█▅▄▂▁▁
wandb:                PPO_1133/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1133/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1133/rollout/ep_rew_mean ▁▂▂▃▃▅▅▇▅█▇▆
wandb:                   PPO_1133/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1133/train/approx_kl ▁▃▅▃▆▄▂▄▄██
wandb:        PPO_1133/train/clip_fraction ▄▅▆▅▆▅▁▆▄██
wandb:           PPO_1133/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1133/train/entropy_loss ▁▂▃▄▄▅▆▆▇▇█
wandb:   PPO_1133/train/explained_variance ▅▅▆▇▇█▆▂▁▃▁
wandb:        PPO_1133/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1133/train/loss ▇▄█▃█▅▅▄▆▇▁
wandb: PPO_1133/train/policy_gradient_loss ▃▂▁▁▄▃█▅▅▆█
wandb:                  PPO_1133/train/std █▇▆▅▅▄▃▂▂▂▁
wandb:           PPO_1133/train/value_loss ▅▃▂▁▄▃█▃▆▄▁
wandb:                PPO_1143/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1143/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1143/rollout/ep_rew_mean ▃▃▄▁▃▃█▆▆▆▆▄
wandb:                   PPO_1143/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1143/train/approx_kl ▅▂▂▃▄▇█▁█▆▂
wandb:        PPO_1143/train/clip_fraction ▇▃▄▂▃▇█▁▃▆▃
wandb:           PPO_1143/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1143/train/entropy_loss ▁▁▂▃▃▄▅▆▆▇█
wandb:   PPO_1143/train/explained_variance ▆▁▅██▄▇▅▆█▆
wandb:        PPO_1143/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1143/train/loss ▂█▂▁▂▂▂▁▂▁▃
wandb: PPO_1143/train/policy_gradient_loss ▄▄▅▆▅▃▇▁▆▇█
wandb:                  PPO_1143/train/std ██▇▆▆▅▄▃▃▂▁
wandb:           PPO_1143/train/value_loss ▆▇▃▆▆▄▁▅▄▃█
wandb:                PPO_1154/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1154/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1154/rollout/ep_rew_mean ▄▇▃▇▃▁▇█▆▄▇▇
wandb:                   PPO_1154/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1154/train/approx_kl ▆▅▁▆▄█▇▇▅▅▄
wandb:        PPO_1154/train/clip_fraction ▇▄▁▆▅▅█▆▅▄▆
wandb:           PPO_1154/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1154/train/entropy_loss ▁▁▂▂▃▃▄▅▆▆█
wandb:   PPO_1154/train/explained_variance ▁▄▅▇▇▅█████
wandb:        PPO_1154/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1154/train/loss █▂▇▃▃▄▁▁▃▃▂
wandb: PPO_1154/train/policy_gradient_loss ▇▃▄▁▆█▂▅▂▅▄
wandb:                  PPO_1154/train/std █▇▇▆▆▅▅▄▃▂▁
wandb:           PPO_1154/train/value_loss ▇▂█▂▄█▂▂▂▄▁
wandb:                PPO_1164/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1164/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1164/rollout/ep_rew_mean ▆▆▄▁▆▅▁▄▅▁▃█
wandb:                   PPO_1164/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1164/train/approx_kl ▁▆▄▁▃▂▄▂▂▃█
wandb:        PPO_1164/train/clip_fraction ▅▇▅▁▆▃▃▆▇▇█
wandb:           PPO_1164/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1164/train/entropy_loss ▁▂▃▃▄▄▄▅▆▇█
wandb:   PPO_1164/train/explained_variance ▄▄▅▁▅▃▅▅▆█▁
wandb:        PPO_1164/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1164/train/loss ▅▁▅▄▂█▄▁▆▅▃
wandb: PPO_1164/train/policy_gradient_loss ▄▆▄▂▆▁▂█▅▅▄
wandb:                  PPO_1164/train/std █▇▆▆▆▅▅▄▃▂▁
wandb:           PPO_1164/train/value_loss ▂▁▃▇▂▇█▄▄▄▅
wandb:                PPO_1174/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1174/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1174/rollout/ep_rew_mean ▁▄▃▄▅▆▆▂▄█▃▆
wandb:                   PPO_1174/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1174/train/approx_kl █▆▁████▇█▄▅
wandb:        PPO_1174/train/clip_fraction ▃▅▁▆▄▅▅▁█▃▇
wandb:           PPO_1174/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1174/train/entropy_loss ▁▂▂▃▄▅▅▅▆▇█
wandb:   PPO_1174/train/explained_variance █▁▇█▅▇▇▆▇▅▇
wandb:        PPO_1174/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1174/train/loss ▅▅█▃▂▄▁▇▁▃▁
wandb: PPO_1174/train/policy_gradient_loss ▁▄▅▇▄▇█▁▃▄▇
wandb:                  PPO_1174/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1174/train/value_loss ▅▃▆▄▃▄▁█▂▆▄
wandb:                PPO_1184/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1184/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1184/rollout/ep_rew_mean ▄▇█▅█▆▁▃▆▆▁█
wandb:                   PPO_1184/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1184/train/approx_kl ▁▆▆▃▂▃▃▆▃▂█
wandb:        PPO_1184/train/clip_fraction ▁▅▅▁▄▃▄▅▂▃█
wandb:           PPO_1184/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1184/train/entropy_loss ▁▂▄▄▅▅▆▆▇▆█
wandb:   PPO_1184/train/explained_variance ▅▄▃▄▆▁▁▆█▄█
wandb:        PPO_1184/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1184/train/loss ▆▃▁▃█▆▅▃▅▄▂
wandb: PPO_1184/train/policy_gradient_loss ▃█▁▃█▅▅▆▄▃█
wandb:                  PPO_1184/train/std █▆▅▅▄▄▃▃▂▃▁
wandb:           PPO_1184/train/value_loss ▅▃▂▅▄▇█▃▄▅▁
wandb:                PPO_1194/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1194/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1194/rollout/ep_rew_mean ▂▆▁▂▅▅▃▄▆▃█▄
wandb:                   PPO_1194/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1194/train/approx_kl ▃▆▁▃▄▃█▆▆▇█
wandb:        PPO_1194/train/clip_fraction ▃▇▁▅▇▅▆▅▅▆█
wandb:           PPO_1194/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1194/train/entropy_loss ▁▂▂▃▃▅▅▆▆▇█
wandb:   PPO_1194/train/explained_variance ▃▄▁▄▄▅█▄▂▄▇
wandb:        PPO_1194/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1194/train/loss ▂▂▅▂▂▂▂█▃▃▁
wandb: PPO_1194/train/policy_gradient_loss ▁▆▅▆▇▇▇▄▅▅█
wandb:                  PPO_1194/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1194/train/value_loss ▄▃█▃▂▃▁▄▇▅▂
wandb:                    global_mean_eval ▁▄▆▇▆▇▇▇██
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▄▆▇▆▇▇▇▇█
wandb:                       mean_reward_1 ▁▄▆▇▆▇▇▇▇█
wandb:                      mean_reward_10 ▁▄▆▇▆▇▇▇▇█
wandb:                      mean_reward_11 ▁▄▆▇▆▇▇▇██
wandb:                      mean_reward_12 ▁▄▆▇▆▇▇▇██
wandb:                      mean_reward_13 ▁▄▆▇▆▇▇▇▇█
wandb:                      mean_reward_14 ▁▄▆▇▆▇▇▇▇█
wandb:                      mean_reward_15 ▁▄▇▇▆▇▇▇██
wandb:                      mean_reward_16 ▁▄▆▇▆▇▇▇▇█
wandb:                      mean_reward_17 ▁▄▆▇▆▇▇▇██
wandb:                      mean_reward_18 ▁▄▇▇▇▇████
wandb:                      mean_reward_19 ▁▄▆▆▆▇▇▇██
wandb:                       mean_reward_2 ▁▄▇▇▇█████
wandb:                      mean_reward_20 ▁▄▆▇▆▇▇▇▇█
wandb:                      mean_reward_21 ▁▄▇▇▆▇█▇██
wandb:                      mean_reward_22 ▁▄▆▇▆▇▇▇▇█
wandb:                      mean_reward_23 ▁▄▆▇▆▇▇███
wandb:                      mean_reward_24 ▁▄▇▇▇▇██▇█
wandb:                      mean_reward_25 ▁▄▇▇▆▇▇███
wandb:                      mean_reward_26 ▁▄▇▇▇▇▇▇██
wandb:                      mean_reward_27 ▁▄▆▆▆▇▇▆██
wandb:                      mean_reward_28 ▁▄▆▇▆▇▇▇█▇
wandb:                      mean_reward_29 ▁▄▆▇▆▇▇▇▇█
wandb:                       mean_reward_3 ▁▄▆▇▆▇▇▇██
wandb:                      mean_reward_30 ▁▄▇▇▇▇█▇██
wandb:                      mean_reward_31 ▁▄▆▇▇███▇█
wandb:                      mean_reward_32 ▁▄▆▇▆▇▇▇██
wandb:                      mean_reward_33 ▁▄▆▇▇▇▇█▇█
wandb:                      mean_reward_34 ▁▄▆▇▆▇█▇██
wandb:                      mean_reward_35 ▁▄▆▇▇▇▇███
wandb:                       mean_reward_4 ▁▄▆▇▆▇▇▇██
wandb:                       mean_reward_5 ▁▄▆▆▆▇▇▇▇█
wandb:                       mean_reward_6 ▁▄▇▇▆▇█▇██
wandb:                       mean_reward_7 ▁▄▆▇▆▇▇▇▇█
wandb:                       mean_reward_8 ▁▄▆▇▆▇█▇██
wandb:                       mean_reward_9 ▁▄▆▆▆▇▇▇██
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▁▂▁▃▂▃▄▅▆▆█
wandb:                        std_reward_0 ▃▁▁▂▃▃▃▅█▇
wandb:                        std_reward_1 ▄▁▂▂▃▃▃▆█▇
wandb:                       std_reward_10 ▃▁▂▁▄▄▃▄██
wandb:                       std_reward_11 ▄▁▂▂▄▄▄▆██
wandb:                       std_reward_12 ▃▁▁▁▃▃▃▅█▇
wandb:                       std_reward_13 ▄▁▂▂▄▄▄▆█▇
wandb:                       std_reward_14 ▃▁▂▂▃▃▃▆█▇
wandb:                       std_reward_15 ▄▁▂▂▃▄▄▇██
wandb:                       std_reward_16 ▃▁▂▂▃▄▃▆██
wandb:                       std_reward_17 ▄▁▂▂▃▃▄▆██
wandb:                       std_reward_18 ▃▁▂▂▃▃▃▅██
wandb:                       std_reward_19 ▄▁▂▂▃▃▃▅█▇
wandb:                        std_reward_2 ▃▁▂▂▃▃▃▆██
wandb:                       std_reward_20 ▃▁▂▂▃▃▃▅█▆
wandb:                       std_reward_21 ▃▁▂▂▃▄▃▆██
wandb:                       std_reward_22 ▄▁▂▂▃▄▃▅█▇
wandb:                       std_reward_23 ▄▁▂▂▃▄▃▆██
wandb:                       std_reward_24 ▃▁▂▂▃▃▃▅█▆
wandb:                       std_reward_25 ▃▁▂▂▃▃▃▄██
wandb:                       std_reward_26 ▃▁▂▂▃▃▃▆█▇
wandb:                       std_reward_27 ▃▁▂▂▄▄▄▇█▇
wandb:                       std_reward_28 ▃▁▂▂▃▂▃▆▇█
wandb:                       std_reward_29 ▃▁▂▂▃▃▃▆█▇
wandb:                        std_reward_3 ▃▁▂▂▃▄▄▅█▇
wandb:                       std_reward_30 ▃▁▂▂▃▄▃▅▇█
wandb:                       std_reward_31 ▃▁▂▂▃▄▃▅██
wandb:                       std_reward_32 ▃▁▂▂▃▄▃▅█▇
wandb:                       std_reward_33 ▃▁▂▂▂▃▃▄█▇
wandb:                       std_reward_34 ▄▁▂▂▃▃▂▅██
wandb:                       std_reward_35 ▃▁▂▁▃▃▄▅██
wandb:                        std_reward_4 ▄▁▂▂▂▃▃▅▇█
wandb:                        std_reward_5 ▃▁▁▂▃▃▃▅█▆
wandb:                        std_reward_6 ▃▁▁▂▄▃▄▆██
wandb:                        std_reward_7 ▃▁▂▁▄▃▃▆█▇
wandb:                        std_reward_8 ▄▁▂▂▃▄▃▆██
wandb:                        std_reward_9 ▃▁▂▂▃▄▄▆██
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▂▃█▁▂▂▄▃▄▆▆
wandb:                 train/clip_fraction ▃▃▃▄▄▁▃▅▅▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▃▆▇▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▃▂▂▁▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▆▆▅█▆▅▃▄▂▁▂
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1115/global_step 212992
wandb:        PPO_1115/rollout/ep_len_mean 200.0
wandb:        PPO_1115/rollout/ep_rew_mean -791.05609
wandb:                   PPO_1115/time/fps 1198.0
wandb:            PPO_1115/train/approx_kl 0.01242
wandb:        PPO_1115/train/clip_fraction 0.17504
wandb:           PPO_1115/train/clip_range 0.2
wandb:         PPO_1115/train/entropy_loss -7.46013
wandb:   PPO_1115/train/explained_variance 0.97219
wandb:        PPO_1115/train/learning_rate 0.0003
wandb:                 PPO_1115/train/loss 11.58517
wandb: PPO_1115/train/policy_gradient_loss -0.00729
wandb:                  PPO_1115/train/std 0.70153
wandb:           PPO_1115/train/value_loss 45.99662
wandb:                PPO_1124/global_step 212992
wandb:        PPO_1124/rollout/ep_len_mean 200.0
wandb:        PPO_1124/rollout/ep_rew_mean -629.05542
wandb:                   PPO_1124/time/fps 1200.0
wandb:            PPO_1124/train/approx_kl 0.01527
wandb:        PPO_1124/train/clip_fraction 0.20699
wandb:           PPO_1124/train/clip_range 0.2
wandb:         PPO_1124/train/entropy_loss -6.43431
wandb:   PPO_1124/train/explained_variance 0.96586
wandb:        PPO_1124/train/learning_rate 0.0003
wandb:                 PPO_1124/train/loss 13.51613
wandb: PPO_1124/train/policy_gradient_loss -0.00479
wandb:                  PPO_1124/train/std 0.60615
wandb:           PPO_1124/train/value_loss 36.94084
wandb:                PPO_1133/global_step 212992
wandb:        PPO_1133/rollout/ep_len_mean 200.0
wandb:        PPO_1133/rollout/ep_rew_mean -566.29694
wandb:                   PPO_1133/time/fps 1197.0
wandb:            PPO_1133/train/approx_kl 0.01897
wandb:        PPO_1133/train/clip_fraction 0.23298
wandb:           PPO_1133/train/clip_range 0.2
wandb:         PPO_1133/train/entropy_loss -5.5815
wandb:   PPO_1133/train/explained_variance 0.95551
wandb:        PPO_1133/train/learning_rate 0.0003
wandb:                 PPO_1133/train/loss 7.58
wandb: PPO_1133/train/policy_gradient_loss -0.0038
wandb:                  PPO_1133/train/std 0.53761
wandb:           PPO_1133/train/value_loss 26.18486
wandb:                PPO_1143/global_step 212992
wandb:        PPO_1143/rollout/ep_len_mean 200.0
wandb:        PPO_1143/rollout/ep_rew_mean -536.89221
wandb:                   PPO_1143/time/fps 1196.0
wandb:            PPO_1143/train/approx_kl 0.01563
wandb:        PPO_1143/train/clip_fraction 0.20589
wandb:           PPO_1143/train/clip_range 0.2
wandb:         PPO_1143/train/entropy_loss -4.96122
wandb:   PPO_1143/train/explained_variance 0.96952
wandb:        PPO_1143/train/learning_rate 0.0003
wandb:                 PPO_1143/train/loss 24.87436
wandb: PPO_1143/train/policy_gradient_loss -0.00228
wandb:                  PPO_1143/train/std 0.49117
wandb:           PPO_1143/train/value_loss 64.78896
wandb:                PPO_1154/global_step 212992
wandb:        PPO_1154/rollout/ep_len_mean 200.0
wandb:        PPO_1154/rollout/ep_rew_mean -513.63818
wandb:                   PPO_1154/time/fps 1191.0
wandb:            PPO_1154/train/approx_kl 0.01761
wandb:        PPO_1154/train/clip_fraction 0.23483
wandb:           PPO_1154/train/clip_range 0.2
wandb:         PPO_1154/train/entropy_loss -4.46319
wandb:   PPO_1154/train/explained_variance 0.99098
wandb:        PPO_1154/train/learning_rate 0.0003
wandb:                 PPO_1154/train/loss 9.27769
wandb: PPO_1154/train/policy_gradient_loss -0.00378
wandb:                  PPO_1154/train/std 0.45777
wandb:           PPO_1154/train/value_loss 24.3758
wandb:                PPO_1164/global_step 212992
wandb:        PPO_1164/rollout/ep_len_mean 200.0
wandb:        PPO_1164/rollout/ep_rew_mean -490.2066
wandb:                   PPO_1164/time/fps 1191.0
wandb:            PPO_1164/train/approx_kl 0.02121
wandb:        PPO_1164/train/clip_fraction 0.25793
wandb:           PPO_1164/train/clip_range 0.2
wandb:         PPO_1164/train/entropy_loss -4.1051
wandb:   PPO_1164/train/explained_variance 0.99226
wandb:        PPO_1164/train/learning_rate 0.0003
wandb:                 PPO_1164/train/loss 8.74738
wandb: PPO_1164/train/policy_gradient_loss -0.00328
wandb:                  PPO_1164/train/std 0.43518
wandb:           PPO_1164/train/value_loss 37.40363
wandb:                PPO_1174/global_step 212992
wandb:        PPO_1174/rollout/ep_len_mean 200.0
wandb:        PPO_1174/rollout/ep_rew_mean -472.54425
wandb:                   PPO_1174/time/fps 1185.0
wandb:            PPO_1174/train/approx_kl 0.01867
wandb:        PPO_1174/train/clip_fraction 0.26879
wandb:           PPO_1174/train/clip_range 0.2
wandb:         PPO_1174/train/entropy_loss -3.50676
wandb:   PPO_1174/train/explained_variance 0.99603
wandb:        PPO_1174/train/learning_rate 0.0003
wandb:                 PPO_1174/train/loss 4.17491
wandb: PPO_1174/train/policy_gradient_loss -0.00158
wandb:                  PPO_1174/train/std 0.39871
wandb:           PPO_1174/train/value_loss 22.88469
wandb:                PPO_1184/global_step 212992
wandb:        PPO_1184/rollout/ep_len_mean 200.0
wandb:        PPO_1184/rollout/ep_rew_mean -469.15097
wandb:                   PPO_1184/time/fps 1193.0
wandb:            PPO_1184/train/approx_kl 0.02417
wandb:        PPO_1184/train/clip_fraction 0.30742
wandb:           PPO_1184/train/clip_range 0.2
wandb:         PPO_1184/train/entropy_loss -3.1403
wandb:   PPO_1184/train/explained_variance 0.99783
wandb:        PPO_1184/train/learning_rate 0.0003
wandb:                 PPO_1184/train/loss 3.57021
wandb: PPO_1184/train/policy_gradient_loss -7e-05
wandb:                  PPO_1184/train/std 0.37882
wandb:           PPO_1184/train/value_loss 11.84482
wandb:                PPO_1194/global_step 212992
wandb:        PPO_1194/rollout/ep_len_mean 200.0
wandb:        PPO_1194/rollout/ep_rew_mean -475.1344
wandb:                   PPO_1194/time/fps 1194.0
wandb:            PPO_1194/train/approx_kl 0.0239
wandb:        PPO_1194/train/clip_fraction 0.29608
wandb:           PPO_1194/train/clip_range 0.2
wandb:         PPO_1194/train/entropy_loss -2.6919
wandb:   PPO_1194/train/explained_variance 0.99828
wandb:        PPO_1194/train/learning_rate 0.0003
wandb:                 PPO_1194/train/loss 1.89926
wandb: PPO_1194/train/policy_gradient_loss 0.00146
wandb:                  PPO_1194/train/std 0.35573
wandb:           PPO_1194/train/value_loss 20.15264
wandb:                    global_mean_eval -421.16574
wandb:                         global_step 212992
wandb:                       mean_reward_0 -413.50735
wandb:                       mean_reward_1 -408.6802
wandb:                      mean_reward_10 -419.54716
wandb:                      mean_reward_11 -419.64304
wandb:                      mean_reward_12 -410.09498
wandb:                      mean_reward_13 -411.9477
wandb:                      mean_reward_14 -400.9234
wandb:                      mean_reward_15 -420.68937
wandb:                      mean_reward_16 -406.01649
wandb:                      mean_reward_17 -423.20573
wandb:                      mean_reward_18 -446.43819
wandb:                      mean_reward_19 -409.8471
wandb:                       mean_reward_2 -447.08511
wandb:                      mean_reward_20 -418.49714
wandb:                      mean_reward_21 -444.76987
wandb:                      mean_reward_22 -401.44485
wandb:                      mean_reward_23 -404.70742
wandb:                      mean_reward_24 -430.42301
wandb:                      mean_reward_25 -427.59725
wandb:                      mean_reward_26 -429.51911
wandb:                      mean_reward_27 -395.24263
wandb:                      mean_reward_28 -456.41027
wandb:                      mean_reward_29 -415.26356
wandb:                       mean_reward_3 -423.8865
wandb:                      mean_reward_30 -449.54161
wandb:                      mean_reward_31 -441.88342
wandb:                      mean_reward_32 -404.71777
wandb:                      mean_reward_33 -434.21022
wandb:                      mean_reward_34 -424.75789
wandb:                      mean_reward_35 -434.66699
wandb:                       mean_reward_4 -430.09955
wandb:                       mean_reward_5 -396.16489
wandb:                       mean_reward_6 -430.93869
wandb:                       mean_reward_7 -411.84753
wandb:                       mean_reward_8 -418.26057
wandb:                       mean_reward_9 -399.49023
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -931.94031
wandb:                        std_reward_0 170.15052
wandb:                        std_reward_1 160.58042
wandb:                       std_reward_10 161.48898
wandb:                       std_reward_11 163.34158
wandb:                       std_reward_12 164.42875
wandb:                       std_reward_13 150.46454
wandb:                       std_reward_14 147.40681
wandb:                       std_reward_15 160.92058
wandb:                       std_reward_16 157.07707
wandb:                       std_reward_17 165.77242
wandb:                       std_reward_18 190.35574
wandb:                       std_reward_19 159.39397
wandb:                        std_reward_2 171.2457
wandb:                       std_reward_20 152.54104
wandb:                       std_reward_21 173.35814
wandb:                       std_reward_22 167.47578
wandb:                       std_reward_23 153.16845
wandb:                       std_reward_24 161.87158
wandb:                       std_reward_25 188.38359
wandb:                       std_reward_26 151.85914
wandb:                       std_reward_27 140.27866
wandb:                       std_reward_28 179.09212
wandb:                       std_reward_29 167.2679
wandb:                        std_reward_3 169.60738
wandb:                       std_reward_30 190.64172
wandb:                       std_reward_31 188.90063
wandb:                       std_reward_32 151.72396
wandb:                       std_reward_33 169.17162
wandb:                       std_reward_34 182.33632
wandb:                       std_reward_35 175.10343
wandb:                        std_reward_4 187.59591
wandb:                        std_reward_5 137.94553
wandb:                        std_reward_6 177.91098
wandb:                        std_reward_7 159.14002
wandb:                        std_reward_8 162.00112
wandb:                        std_reward_9 160.87443
wandb:                            time/fps 1178.0
wandb:                     train/approx_kl 0.01244
wandb:                 train/clip_fraction 0.15652
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7411
wandb:            train/explained_variance 0.96718
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 11.08948
wandb:          train/policy_gradient_loss -0.01304
wandb:                           train/std 0.84138
wandb:                    train/value_loss 21.77692
wandb: 
wandb: Synced volcanic-bird-27: https://wandb.ai/tidiane/meta_rl_context/runs/pr1s5nmz
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_012622-pr1s5nmz/logs
wandb: 
wandb: Run history:
wandb:                PPO_1114/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1114/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1114/rollout/ep_rew_mean ▁▂▂▃▂▄▄▅▅▆▇█
wandb:                   PPO_1114/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1114/train/approx_kl ▆▁▇▅▄▅▄▄▅▄█
wandb:        PPO_1114/train/clip_fraction ▅▁█▄▄▁▃▃▄▄▇
wandb:           PPO_1114/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1114/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1114/train/explained_variance █▇█▄▄█▅▅▂▇▁
wandb:        PPO_1114/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1114/train/loss ▁▄▂▆▅▇▃█▇▃▆
wandb: PPO_1114/train/policy_gradient_loss ▂█▁▇▆█▇█▅▅▆
wandb:                  PPO_1114/train/std ██▇▆▅▅▄▄▃▂▁
wandb:           PPO_1114/train/value_loss ▁▃▂▄▄▆▆███▆
wandb:                PPO_1126/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1126/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1126/rollout/ep_rew_mean ▁▂▃▄▃▃▅▄▅▅▆█
wandb:                   PPO_1126/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1126/train/approx_kl ▁▄▄▆▇▇▆▆▆█▇
wandb:        PPO_1126/train/clip_fraction ▁▅▅▃▄▇█▄▄▇▇
wandb:           PPO_1126/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1126/train/entropy_loss ▁▂▃▄▅▅▆▇▇▇█
wandb:   PPO_1126/train/explained_variance ▇█▇█▆▇█▁▆▆█
wandb:        PPO_1126/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1126/train/loss ▇▄▆█▄▄▁▂▆▂▁
wandb: PPO_1126/train/policy_gradient_loss ▆▁▃▄▇▄▃▅▇▆█
wandb:                  PPO_1126/train/std █▇▆▅▄▄▃▂▂▂▁
wandb:           PPO_1126/train/value_loss █▄▄▅▇▃▂▄▄▅▁
wandb:                PPO_1138/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1138/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1138/rollout/ep_rew_mean ▂▄▄▁▃▄▇▄▂▅▇█
wandb:                   PPO_1138/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1138/train/approx_kl ▅▁▅▄█▅▇▅▅▄▇
wandb:        PPO_1138/train/clip_fraction ▅▁▅▃█▆█▃▄▇▄
wandb:           PPO_1138/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1138/train/entropy_loss ▁▁▂▂▃▄▅▆▆▇█
wandb:   PPO_1138/train/explained_variance ▂▁▅▂▃▆█▇█▇█
wandb:        PPO_1138/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1138/train/loss ▂▃▂▃█▃▁▃▃▄▃
wandb: PPO_1138/train/policy_gradient_loss ▄▆▆▅▆▆▄▄▂█▁
wandb:                  PPO_1138/train/std ██▇▇▆▅▄▃▃▃▁
wandb:           PPO_1138/train/value_loss ▂▅▃█▇▄▁▃▃▃▃
wandb:                PPO_1147/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1147/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1147/rollout/ep_rew_mean ▂▁▂▂▆▄▂▆▅█▇█
wandb:                   PPO_1147/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1147/train/approx_kl ▂▁▁▂▅▁▅▅▁█▄
wandb:        PPO_1147/train/clip_fraction ▃▁▃▄▅▁▆▅▂█▅
wandb:           PPO_1147/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1147/train/entropy_loss ▁▂▂▂▃▄▄▅▆▆█
wandb:   PPO_1147/train/explained_variance ▃▃▆█▅▆▅▃█▅▁
wandb:        PPO_1147/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1147/train/loss ▃█▂▅▄▄▃▂▄▁▂
wandb: PPO_1147/train/policy_gradient_loss ▂█▅▃▇▁▃▂▄▃▃
wandb:                  PPO_1147/train/std █▇▇▆▆▅▅▄▃▃▁
wandb:           PPO_1147/train/value_loss ▅█▅▃▃▇▄▂▃▁▃
wandb:                PPO_1156/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1156/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1156/rollout/ep_rew_mean ▂▃▆▄▆█▂▁▃▄▄▆
wandb:                   PPO_1156/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1156/train/approx_kl ▄▂▃▁▆▁▃▄▅▇█
wandb:        PPO_1156/train/clip_fraction ▂▆▃▄▆▄▁▅█▄█
wandb:           PPO_1156/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1156/train/entropy_loss ▁▂▂▃▄▄▄▅▆▆█
wandb:   PPO_1156/train/explained_variance ▆▅▁▅▇▄▄▇▇█▇
wandb:        PPO_1156/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1156/train/loss █▂▄▁▂▅▅▃▁▅▂
wandb: PPO_1156/train/policy_gradient_loss ▂▁▃▂▃▅▆█▇▇▅
wandb:                  PPO_1156/train/std █▇▇▆▅▅▅▄▃▂▁
wandb:           PPO_1156/train/value_loss ▇▂▃▃▁▂█▄▂▃▁
wandb:                PPO_1165/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1165/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1165/rollout/ep_rew_mean ▄▁▁▂▄▇▇▆█▆▄▄
wandb:                   PPO_1165/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1165/train/approx_kl ▁▅▃▅▃█▃▄▅▅▃
wandb:        PPO_1165/train/clip_fraction ▂▂▁▆▄█▄▃▅▄▁
wandb:           PPO_1165/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1165/train/entropy_loss ▁▁▂▂▃▄▅▆▆▇█
wandb:   PPO_1165/train/explained_variance ▄▁▁▃▄▆▇▅▆█▆
wandb:        PPO_1165/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1165/train/loss ▃▂▁▆█▂▃▃▂▅▂
wandb: PPO_1165/train/policy_gradient_loss ▂▁▂▅▂▇▇▆▅█▄
wandb:                  PPO_1165/train/std █▇▇▆▆▅▄▃▃▂▁
wandb:           PPO_1165/train/value_loss ▄██▅▄▂▁▃▁▂▂
wandb:                PPO_1175/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1175/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1175/rollout/ep_rew_mean ▂▂▃▅▆▄▁▁▃▇▂█
wandb:                   PPO_1175/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1175/train/approx_kl ▂▃▂▄▃▃▁▂▄▂█
wandb:        PPO_1175/train/clip_fraction ▃▃▁▃▃▇▂▃█▄▃
wandb:           PPO_1175/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1175/train/entropy_loss ▁▂▂▃▃▄▅▆▆▇█
wandb:   PPO_1175/train/explained_variance ▄▇▄▁▂▆▇█▆█▁
wandb:        PPO_1175/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1175/train/loss ▂▄▅▅▂█▃▅▁▃▃
wandb: PPO_1175/train/policy_gradient_loss ▇▃▁▂▂▃▅▁█▂▇
wandb:                  PPO_1175/train/std █▇▇▆▆▅▄▃▂▂▁
wandb:           PPO_1175/train/value_loss █▂▃▆▆▃▂▃▂▁█
wandb:                PPO_1185/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1185/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1185/rollout/ep_rew_mean ▁▄▅▆▃▇▆▆▆█▆▅
wandb:                   PPO_1185/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1185/train/approx_kl ▁▃▁▂▃▁▂█▄▂▄
wandb:        PPO_1185/train/clip_fraction ▄█▁▆▄▃▄▃▄▄▃
wandb:           PPO_1185/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1185/train/entropy_loss ▁▂▃▃▄▄▅▆▇▇█
wandb:   PPO_1185/train/explained_variance ▁▃▂▄▇▅█▄▆▃▃
wandb:        PPO_1185/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1185/train/loss █▂▃▁▂▂▂▂▃▄▂
wandb: PPO_1185/train/policy_gradient_loss ▄▇▂▆▃▅█▁▂▄▇
wandb:                  PPO_1185/train/std █▇▆▆▅▅▄▃▂▂▁
wandb:           PPO_1185/train/value_loss █▄▅▁▂▁▁▄▃▃▄
wandb:                PPO_1195/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1195/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1195/rollout/ep_rew_mean ▁▆▃▃█▅▄▃▃▆▅█
wandb:                   PPO_1195/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1195/train/approx_kl ▁▂▁▂▂▂▃▃█▄▂
wandb:        PPO_1195/train/clip_fraction ▅▁▅▃▅▆▅▅▇█▇
wandb:           PPO_1195/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1195/train/entropy_loss ▁▁▂▃▄▄▅▅▆▇█
wandb:   PPO_1195/train/explained_variance █▄▇▅▃█▆█▁▇█
wandb:        PPO_1195/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1195/train/loss ▂▄▁▂█▁▁▂▅▄▂
wandb: PPO_1195/train/policy_gradient_loss ▁▃▄▃▅▆▂▆▇▇█
wandb:                  PPO_1195/train/std █▇▇▆▅▄▄▄▃▂▁
wandb:           PPO_1195/train/value_loss ▂▄▁▆▆▂▃▂█▂▁
wandb:                    global_mean_eval ▁▅▆▆▇▇█▇██
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▅▆▅▇▇▇███
wandb:                       mean_reward_1 ▁▅▅▅▆▇▇███
wandb:                      mean_reward_10 ▁▄▆▆▇▇▇▇██
wandb:                      mean_reward_11 ▁▅▆▆▇▇█▇██
wandb:                      mean_reward_12 ▁▅▅▅▇▇▇▇██
wandb:                      mean_reward_13 ▁▅▆▆▇▇▇▇█▇
wandb:                      mean_reward_14 ▁▄▅▆▆▇▇▇█▇
wandb:                      mean_reward_15 ▁▅▆▆▇▇▇▇██
wandb:                      mean_reward_16 ▁▅▆▆▇▇▇▇██
wandb:                      mean_reward_17 ▁▅▆▆▇▇████
wandb:                      mean_reward_18 ▁▅▅▆▇▇█▇██
wandb:                      mean_reward_19 ▁▅▆▆▇█▇███
wandb:                       mean_reward_2 ▁▅▆▆▇▇████
wandb:                      mean_reward_20 ▁▅▅▆▆▇▇▇██
wandb:                      mean_reward_21 ▁▅▆▆▇▇▇▇██
wandb:                      mean_reward_22 ▁▅▆▆▆▇▇▇██
wandb:                      mean_reward_23 ▁▅▅▆▆▇▇▇█▇
wandb:                      mean_reward_24 ▁▅▅▅▆▇▇▇█▇
wandb:                      mean_reward_25 ▁▅▆▆▇▇████
wandb:                      mean_reward_26 ▁▅▆▆▆█▇▇██
wandb:                      mean_reward_27 ▁▅▅▆▇█████
wandb:                      mean_reward_28 ▁▅▆▆▇▇▇▇██
wandb:                      mean_reward_29 ▁▅▅▆▇▇████
wandb:                       mean_reward_3 ▁▄▆▆▇▇▇▇█▇
wandb:                      mean_reward_30 ▁▅▆▆▇▇█▇██
wandb:                      mean_reward_31 ▁▅▆▆▇▇████
wandb:                      mean_reward_32 ▁▅▅▆▇▇▇███
wandb:                      mean_reward_33 ▁▅▆▆▇█████
wandb:                      mean_reward_34 ▁▅▅▆▇▇▇▇██
wandb:                      mean_reward_35 ▁▅▆▆▇▇████
wandb:                       mean_reward_4 ▁▄▆▆▇▇▇▇██
wandb:                       mean_reward_5 ▁▅▅▆▇▇▇███
wandb:                       mean_reward_6 ▁▅▅▆▇▇▇▇██
wandb:                       mean_reward_7 ▁▅▆▆▇▇████
wandb:                       mean_reward_8 ▁▅▆▆▇█████
wandb:                       mean_reward_9 ▁▅▅▆▇▇▇▇██
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▁▂▁▃▂▃▄▅▆▆█
wandb:                        std_reward_0 ▃▁▆█▅▄▄▄▄▄
wandb:                        std_reward_1 ▄▁▆█▄▄▄▄▄▄
wandb:                       std_reward_10 ▄▁▆█▅▄▅▅▅▅
wandb:                       std_reward_11 ▄▁▇█▆▅▄▆▆▅
wandb:                       std_reward_12 ▄▁▆█▄▄▅▄▄▄
wandb:                       std_reward_13 ▄▁▅█▅▄▅▅▅▅
wandb:                       std_reward_14 ▄▁▆█▄▅▅▅▄▅
wandb:                       std_reward_15 ▃▁▇█▅▅▅▅▄▅
wandb:                       std_reward_16 ▄▁▆█▅▄▄▅▅▄
wandb:                       std_reward_17 ▄▁▆█▄▄▄▅▅▅
wandb:                       std_reward_18 ▄▁▇█▅▅▄▅▅▅
wandb:                       std_reward_19 ▄▁▆█▅▄▅▅▅▆
wandb:                        std_reward_2 ▄▁▇█▅▄▄▅▅▅
wandb:                       std_reward_20 ▄▁██▆▅▅▅▅▅
wandb:                       std_reward_21 ▃▁▆█▅▄▅▅▄▄
wandb:                       std_reward_22 ▄▁▆█▆▄▅▆▅▅
wandb:                       std_reward_23 ▄▁▇█▆▅▅▅▅▆
wandb:                       std_reward_24 ▃▁▅█▅▄▄▄▃▅
wandb:                       std_reward_25 ▄▁██▄▅▄▅▄▅
wandb:                       std_reward_26 ▄▁▆█▅▃▅▅▅▅
wandb:                       std_reward_27 ▄▁▇█▅▅▄▅▆▆
wandb:                       std_reward_28 ▄▁▇█▇▅▅▆▆▅
wandb:                       std_reward_29 ▄▁██▅▅▅▅▅▆
wandb:                        std_reward_3 ▄▁▅█▅▅▅▄▄▆
wandb:                       std_reward_30 ▃▁▅█▅▄▄▅▅▅
wandb:                       std_reward_31 ▃▁▇█▅▅▅▄▆▅
wandb:                       std_reward_32 ▅▁██▅▄▅▅▅▅
wandb:                       std_reward_33 ▄▁▅█▄▄▄▄▅▆
wandb:                       std_reward_34 ▃▁▇█▄▄▄▅▄▅
wandb:                       std_reward_35 ▃▁██▅▅▅▄▆▆
wandb:                        std_reward_4 ▅▁██▅▅▅▆▆▅
wandb:                        std_reward_5 ▄▁▆█▅▄▅▄▅▅
wandb:                        std_reward_6 ▄▁██▄▅▅▅▅▆
wandb:                        std_reward_7 ▄▁▆█▄▄▃▄▅▅
wandb:                        std_reward_8 ▄▁▇█▅▄▅▅▆▆
wandb:                        std_reward_9 ▃▁▇█▄▄▄▅▅▄
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▂▃█▁▂▂▄▃▄▆▆
wandb:                 train/clip_fraction ▃▃▃▄▄▁▃▅▅▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▃▆▇▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▃▂▂▁▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▆▆▅█▆▅▃▄▂▁▂
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1114/global_step 212992
wandb:        PPO_1114/rollout/ep_len_mean 200.0
wandb:        PPO_1114/rollout/ep_rew_mean -703.21051
wandb:                   PPO_1114/time/fps 1207.0
wandb:            PPO_1114/train/approx_kl 0.01318
wandb:        PPO_1114/train/clip_fraction 0.15922
wandb:           PPO_1114/train/clip_range 0.2
wandb:         PPO_1114/train/entropy_loss -7.66603
wandb:   PPO_1114/train/explained_variance 0.96404
wandb:        PPO_1114/train/learning_rate 0.0003
wandb:                 PPO_1114/train/loss 71.12028
wandb: PPO_1114/train/policy_gradient_loss -0.00814
wandb:                  PPO_1114/train/std 0.72107
wandb:           PPO_1114/train/value_loss 128.5985
wandb:                PPO_1126/global_step 212992
wandb:        PPO_1126/rollout/ep_len_mean 200.0
wandb:        PPO_1126/rollout/ep_rew_mean -536.15851
wandb:                   PPO_1126/time/fps 1196.0
wandb:            PPO_1126/train/approx_kl 0.01392
wandb:        PPO_1126/train/clip_fraction 0.18828
wandb:           PPO_1126/train/clip_range 0.2
wandb:         PPO_1126/train/entropy_loss -6.95315
wandb:   PPO_1126/train/explained_variance 0.96846
wandb:        PPO_1126/train/learning_rate 0.0003
wandb:                 PPO_1126/train/loss 12.71604
wandb: PPO_1126/train/policy_gradient_loss -0.00368
wandb:                  PPO_1126/train/std 0.653
wandb:           PPO_1126/train/value_loss 45.62604
wandb:                PPO_1138/global_step 212992
wandb:        PPO_1138/rollout/ep_len_mean 200.0
wandb:        PPO_1138/rollout/ep_rew_mean -520.81738
wandb:                   PPO_1138/time/fps 1196.0
wandb:            PPO_1138/train/approx_kl 0.01388
wandb:        PPO_1138/train/clip_fraction 0.1644
wandb:           PPO_1138/train/clip_range 0.2
wandb:         PPO_1138/train/entropy_loss -6.30299
wandb:   PPO_1138/train/explained_variance 0.99426
wandb:        PPO_1138/train/learning_rate 0.0003
wandb:                 PPO_1138/train/loss 17.64169
wandb: PPO_1138/train/policy_gradient_loss -0.00546
wandb:                  PPO_1138/train/std 0.59397
wandb:           PPO_1138/train/value_loss 50.84108
wandb:                PPO_1147/global_step 212992
wandb:        PPO_1147/rollout/ep_len_mean 200.0
wandb:        PPO_1147/rollout/ep_rew_mean -464.25616
wandb:                   PPO_1147/time/fps 1196.0
wandb:            PPO_1147/train/approx_kl 0.01597
wandb:        PPO_1147/train/clip_fraction 0.2075
wandb:           PPO_1147/train/clip_range 0.2
wandb:         PPO_1147/train/entropy_loss -5.56567
wandb:   PPO_1147/train/explained_variance 0.99431
wandb:        PPO_1147/train/learning_rate 0.0003
wandb:                 PPO_1147/train/loss 8.39946
wandb: PPO_1147/train/policy_gradient_loss -0.00436
wandb:                  PPO_1147/train/std 0.53497
wandb:           PPO_1147/train/value_loss 33.34117
wandb:                PPO_1156/global_step 212992
wandb:        PPO_1156/rollout/ep_len_mean 200.0
wandb:        PPO_1156/rollout/ep_rew_mean -439.71552
wandb:                   PPO_1156/time/fps 1198.0
wandb:            PPO_1156/train/approx_kl 0.02053
wandb:        PPO_1156/train/clip_fraction 0.25289
wandb:           PPO_1156/train/clip_range 0.2
wandb:         PPO_1156/train/entropy_loss -4.75811
wandb:   PPO_1156/train/explained_variance 0.99647
wandb:        PPO_1156/train/learning_rate 0.0003
wandb:                 PPO_1156/train/loss 4.66335
wandb: PPO_1156/train/policy_gradient_loss -0.00172
wandb:                  PPO_1156/train/std 0.47748
wandb:           PPO_1156/train/value_loss 16.57761
wandb:                PPO_1165/global_step 212992
wandb:        PPO_1165/rollout/ep_len_mean 200.0
wandb:        PPO_1165/rollout/ep_rew_mean -428.6185
wandb:                   PPO_1165/time/fps 1194.0
wandb:            PPO_1165/train/approx_kl 0.01949
wandb:        PPO_1165/train/clip_fraction 0.24171
wandb:           PPO_1165/train/clip_range 0.2
wandb:         PPO_1165/train/entropy_loss -3.84728
wandb:   PPO_1165/train/explained_variance 0.99748
wandb:        PPO_1165/train/learning_rate 0.0003
wandb:                 PPO_1165/train/loss 4.71458
wandb: PPO_1165/train/policy_gradient_loss -0.00061
wandb:                  PPO_1165/train/std 0.41899
wandb:           PPO_1165/train/value_loss 12.23629
wandb:                PPO_1175/global_step 212992
wandb:        PPO_1175/rollout/ep_len_mean 200.0
wandb:        PPO_1175/rollout/ep_rew_mean -377.16266
wandb:                   PPO_1175/time/fps 1195.0
wandb:            PPO_1175/train/approx_kl 0.03107
wandb:        PPO_1175/train/clip_fraction 0.27437
wandb:           PPO_1175/train/clip_range 0.2
wandb:         PPO_1175/train/entropy_loss -3.13258
wandb:   PPO_1175/train/explained_variance 0.99524
wandb:        PPO_1175/train/learning_rate 0.0003
wandb:                 PPO_1175/train/loss 4.41541
wandb: PPO_1175/train/policy_gradient_loss 0.00361
wandb:                  PPO_1175/train/std 0.37862
wandb:           PPO_1175/train/value_loss 19.09766
wandb:                PPO_1185/global_step 212992
wandb:        PPO_1185/rollout/ep_len_mean 200.0
wandb:        PPO_1185/rollout/ep_rew_mean -389.19592
wandb:                   PPO_1185/time/fps 1195.0
wandb:            PPO_1185/train/approx_kl 0.02661
wandb:        PPO_1185/train/clip_fraction 0.2843
wandb:           PPO_1185/train/clip_range 0.2
wandb:         PPO_1185/train/entropy_loss -2.54536
wandb:   PPO_1185/train/explained_variance 0.99772
wandb:        PPO_1185/train/learning_rate 0.0003
wandb:                 PPO_1185/train/loss 4.15061
wandb: PPO_1185/train/policy_gradient_loss 0.00303
wandb:                  PPO_1185/train/std 0.34765
wandb:           PPO_1185/train/value_loss 12.66768
wandb:                PPO_1195/global_step 212992
wandb:        PPO_1195/rollout/ep_len_mean 200.0
wandb:        PPO_1195/rollout/ep_rew_mean -353.10489
wandb:                   PPO_1195/time/fps 1194.0
wandb:            PPO_1195/train/approx_kl 0.02439
wandb:        PPO_1195/train/clip_fraction 0.31754
wandb:           PPO_1195/train/clip_range 0.2
wandb:         PPO_1195/train/entropy_loss -1.91607
wandb:   PPO_1195/train/explained_variance 0.99887
wandb:        PPO_1195/train/learning_rate 0.0003
wandb:                 PPO_1195/train/loss 2.63006
wandb: PPO_1195/train/policy_gradient_loss 0.0058
wandb:                  PPO_1195/train/std 0.31829
wandb:           PPO_1195/train/value_loss 6.62768
wandb:                    global_mean_eval -341.18154
wandb:                         global_step 212992
wandb:                       mean_reward_0 -326.136
wandb:                       mean_reward_1 -332.09492
wandb:                      mean_reward_10 -331.08255
wandb:                      mean_reward_11 -332.17283
wandb:                      mean_reward_12 -330.20268
wandb:                      mean_reward_13 -370.3351
wandb:                      mean_reward_14 -351.14262
wandb:                      mean_reward_15 -342.73866
wandb:                      mean_reward_16 -325.1418
wandb:                      mean_reward_17 -356.24562
wandb:                      mean_reward_18 -342.89978
wandb:                      mean_reward_19 -353.55012
wandb:                       mean_reward_2 -336.19576
wandb:                      mean_reward_20 -323.53559
wandb:                      mean_reward_21 -322.03314
wandb:                      mean_reward_22 -332.21529
wandb:                      mean_reward_23 -349.06733
wandb:                      mean_reward_24 -357.63981
wandb:                      mean_reward_25 -343.78098
wandb:                      mean_reward_26 -351.96419
wandb:                      mean_reward_27 -360.3013
wandb:                      mean_reward_28 -323.0223
wandb:                      mean_reward_29 -341.2533
wandb:                       mean_reward_3 -351.31822
wandb:                      mean_reward_30 -345.45712
wandb:                      mean_reward_31 -350.37594
wandb:                      mean_reward_32 -336.50445
wandb:                      mean_reward_33 -358.30991
wandb:                      mean_reward_34 -341.36133
wandb:                      mean_reward_35 -354.37479
wandb:                       mean_reward_4 -326.15725
wandb:                       mean_reward_5 -326.95886
wandb:                       mean_reward_6 -347.09049
wandb:                       mean_reward_7 -346.32905
wandb:                       mean_reward_8 -348.42416
wandb:                       mean_reward_9 -315.12238
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -931.94031
wandb:                        std_reward_0 143.42389
wandb:                        std_reward_1 151.28965
wandb:                       std_reward_10 145.22301
wandb:                       std_reward_11 150.16194
wandb:                       std_reward_12 150.61779
wandb:                       std_reward_13 169.74624
wandb:                       std_reward_14 166.63421
wandb:                       std_reward_15 160.08396
wandb:                       std_reward_16 140.8612
wandb:                       std_reward_17 166.79809
wandb:                       std_reward_18 158.28814
wandb:                       std_reward_19 157.36151
wandb:                        std_reward_2 155.42378
wandb:                       std_reward_20 141.14471
wandb:                       std_reward_21 143.60515
wandb:                       std_reward_22 146.37147
wandb:                       std_reward_23 160.04671
wandb:                       std_reward_24 165.34072
wandb:                       std_reward_25 154.9056
wandb:                       std_reward_26 161.96065
wandb:                       std_reward_27 170.52009
wandb:                       std_reward_28 139.63152
wandb:                       std_reward_29 155.18486
wandb:                        std_reward_3 158.36099
wandb:                       std_reward_30 162.02093
wandb:                       std_reward_31 163.02311
wandb:                       std_reward_32 153.91329
wandb:                       std_reward_33 169.37521
wandb:                       std_reward_34 154.49601
wandb:                       std_reward_35 163.46833
wandb:                        std_reward_4 141.57333
wandb:                        std_reward_5 146.76976
wandb:                        std_reward_6 156.25331
wandb:                        std_reward_7 157.64086
wandb:                        std_reward_8 158.59148
wandb:                        std_reward_9 138.78751
wandb:                            time/fps 1178.0
wandb:                     train/approx_kl 0.01244
wandb:                 train/clip_fraction 0.15652
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7411
wandb:            train/explained_variance 0.96718
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 11.08948
wandb:          train/policy_gradient_loss -0.01304
wandb:                           train/std 0.84138
wandb:                    train/value_loss 21.77692
wandb: 
wandb: Synced glowing-blaze-23: https://wandb.ai/tidiane/meta_rl_context/runs/2din8w6f
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_012622-2din8w6f/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1117/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1117/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1117/rollout/ep_rew_mean ▁▁▂▂▃▃▄▅▄▆██
wandb:                   PPO_1117/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1117/train/approx_kl ▁▂▄▄▃▆█▄▆▃▇
wandb:        PPO_1117/train/clip_fraction ▁▂▇▃▄█▇▅▇▄█
wandb:           PPO_1117/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1117/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1117/train/explained_variance ▄▅█▅▇▇▇▇▅▁▄
wandb:        PPO_1117/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1117/train/loss ▁▁▂▅▂▅▄▃▃▇█
wandb: PPO_1117/train/policy_gradient_loss ▄▅▃█▄▅▁▅▅▄▂
wandb:                  PPO_1117/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1117/train/value_loss ▁▂▁▆▁▃▃▄▃█▆
wandb:                PPO_1127/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1127/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1127/rollout/ep_rew_mean ▁▂▂▂▃▃▄▄▅▆▆█
wandb:                   PPO_1127/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1127/train/approx_kl ▂▂▅▆▁▂▅█▃▆▇
wandb:        PPO_1127/train/clip_fraction ▁▃▄▆▁▅▄█▆▄█
wandb:           PPO_1127/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1127/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1127/train/explained_variance ▄▃▃▁▃▃▅▂▆█▇
wandb:        PPO_1127/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1127/train/loss ▆▅▇▄▃▁▄▂█▃▁
wandb: PPO_1127/train/policy_gradient_loss ▂▃▆▄█▄▁▃▁▄▅
wandb:                  PPO_1127/train/std █▇▆▆▅▅▄▄▃▂▁
wandb:           PPO_1127/train/value_loss ▆██▅▇▄█▅▆▅▁
wandb:                PPO_1135/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1135/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1135/rollout/ep_rew_mean ▁▁▃▃▃▅▄▅▅▅▇█
wandb:                   PPO_1135/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1135/train/approx_kl ▁▂▅▅▄█▇▅▄▅▇
wandb:        PPO_1135/train/clip_fraction ▁▂▅▄▃█▄▂▃▅█
wandb:           PPO_1135/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1135/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1135/train/explained_variance ▄▅▄▆▅▇█▅▁▅█
wandb:        PPO_1135/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1135/train/loss ▃█▂▃▂▁▁▁▂▄▄
wandb: PPO_1135/train/policy_gradient_loss ▁▃▄▃▄▂▃▆█▃▇
wandb:                  PPO_1135/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1135/train/value_loss █▆▃▃▄▂▁▃▅▄▃
wandb:                PPO_1145/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1145/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1145/rollout/ep_rew_mean ▂▄▁▂▅▅█▄▃▃▄▅
wandb:                   PPO_1145/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1145/train/approx_kl ▁▄▃▅▆▅█▅▆▃█
wandb:        PPO_1145/train/clip_fraction ▁▃▄▇▇▄▇▆▃█▅
wandb:           PPO_1145/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1145/train/entropy_loss ▁▁▂▂▃▄▅▅▆▆█
wandb:   PPO_1145/train/explained_variance ▅▁▂█▇▆█▅▃▆▅
wandb:        PPO_1145/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1145/train/loss ▄▅█▃▅▅▁▁▇▁▃
wandb: PPO_1145/train/policy_gradient_loss ▇▇▆█▁▆▃▃▂█▄
wandb:                  PPO_1145/train/std ██▇▇▆▅▅▄▃▃▁
wandb:           PPO_1145/train/value_loss ▅▅█▄▂▂▁▂▅▂▃
wandb:                PPO_1155/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1155/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1155/rollout/ep_rew_mean ▆▆▇▆▇▆▆█▅█▁▅
wandb:                   PPO_1155/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1155/train/approx_kl █▂▁▄▄▃▂▇▁▄▄
wandb:        PPO_1155/train/clip_fraction ▇▄▇▄▆▅▅█▁▆▆
wandb:           PPO_1155/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1155/train/entropy_loss ▁▂▃▄▅▅▆▆▇██
wandb:   PPO_1155/train/explained_variance ▁▆▆▆▄▆▆█▆██
wandb:        PPO_1155/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1155/train/loss ▁▂▁▃▄▁▂▁▄█▇
wandb: PPO_1155/train/policy_gradient_loss ▄▂▅▁▄▄▄▄▂▅█
wandb:                  PPO_1155/train/std █▇▆▅▄▃▃▂▂▁▁
wandb:           PPO_1155/train/value_loss ▅▄▁▅▃▄▃▂▇▅█
wandb:                PPO_1166/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1166/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1166/rollout/ep_rew_mean ▆▅▄▁▄▃▂▄▄▃▆█
wandb:                   PPO_1166/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1166/train/approx_kl ▄▆▃▄▃▁█▁▇▆▆
wandb:        PPO_1166/train/clip_fraction ▄█▅▃█▁▇▂▇▆▇
wandb:           PPO_1166/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1166/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1166/train/explained_variance ▅▁▂▂█▄▅▄▂▅▆
wandb:        PPO_1166/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1166/train/loss ▂▃▁▃▂▁▃▂▁█▄
wandb: PPO_1166/train/policy_gradient_loss ▃▂▁▅▅█▅▅▂▃▇
wandb:                  PPO_1166/train/std ██▆▆▅▅▄▄▃▂▁
wandb:           PPO_1166/train/value_loss ▄▄▄▇▁█▆█▄▆▄
wandb:                PPO_1176/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1176/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1176/rollout/ep_rew_mean ▆▃██▁▆▄▄▅▇██
wandb:                   PPO_1176/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1176/train/approx_kl ▁▄▃▂▂▆▆▃█▂▆
wandb:        PPO_1176/train/clip_fraction ▅▃▇▁▄▆█▃▅▆▇
wandb:           PPO_1176/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1176/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1176/train/explained_variance ▆▁▅▃▇▃▁█▅█▄
wandb:        PPO_1176/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1176/train/loss ▇█▂▂▄▇▅▃▆▃▁
wandb: PPO_1176/train/policy_gradient_loss ▃▄█▁▄▁▄█▂▆▅
wandb:                  PPO_1176/train/std █▇▇▆▆▅▄▃▃▂▁
wandb:           PPO_1176/train/value_loss ▃█▂▆▇▇▇▃▄▃▁
wandb:                PPO_1186/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1186/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1186/rollout/ep_rew_mean ▄▃▄▃▁▄▃▅▂▅█▅
wandb:                   PPO_1186/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1186/train/approx_kl ▂▁▁▂▁▁▄▃▅█▃
wandb:        PPO_1186/train/clip_fraction ▃▆▂▃▅▄▆▃▃█▁
wandb:           PPO_1186/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1186/train/entropy_loss ▁▂▃▃▄▄▅▆▇▇█
wandb:   PPO_1186/train/explained_variance ▇█▇▇▅▇█▇██▁
wandb:        PPO_1186/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1186/train/loss █▁▂▆▃▅▅▄▆▇▃
wandb: PPO_1186/train/policy_gradient_loss ▃▃▁▄▅▇▆▆▃▇█
wandb:                  PPO_1186/train/std █▇▆▆▅▅▃▃▂▁▁
wandb:           PPO_1186/train/value_loss ▄▁▄▇▅▂▁▅▃▂█
wandb:                PPO_1196/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1196/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1196/rollout/ep_rew_mean ▁▃▃▄▅▄▃▄▄▄▅█
wandb:                   PPO_1196/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1196/train/approx_kl ▆▆▁█▆▃▆▆█▅▆
wandb:        PPO_1196/train/clip_fraction ▄▅▃▂▁▂▄▂██▁
wandb:           PPO_1196/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1196/train/entropy_loss ▁▁▃▃▄▄▅▅▆▇█
wandb:   PPO_1196/train/explained_variance ▄▇▁▄▆▅▇█▇██
wandb:        PPO_1196/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1196/train/loss ▂▁▂▁▁█▂▂▁▁▁
wandb: PPO_1196/train/policy_gradient_loss ▄█▄▇▃▁▄▃▆▇▇
wandb:                  PPO_1196/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1196/train/value_loss █▃▇▅▃▅▃▃▂▂▁
wandb:                    global_mean_eval ▁▃▅▆▆▆▆▇██
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▃▅▆▆▆▆▇▇█
wandb:                       mean_reward_1 ▁▃▅▆▆▆▆▇▇█
wandb:                      mean_reward_10 ▁▃▅▆▆▆▇▇▇█
wandb:                      mean_reward_11 ▁▃▅▆▆▅▆▇▇█
wandb:                      mean_reward_12 ▁▃▅▆▆▆▇▇▇█
wandb:                      mean_reward_13 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_14 ▁▃▅▆▆▆▆▇▇█
wandb:                      mean_reward_15 ▁▃▅▆▆▅▆▇▇█
wandb:                      mean_reward_16 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_17 ▁▃▅▆▆▆▆▇██
wandb:                      mean_reward_18 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_19 ▁▃▅▆▆▆▆███
wandb:                       mean_reward_2 ▁▃▅▆▆▆▆▇▇█
wandb:                      mean_reward_20 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_21 ▁▃▅▆▆▆▆▇▇█
wandb:                      mean_reward_22 ▁▃▅▆▆▆▆███
wandb:                      mean_reward_23 ▁▃▅▆▆▆▆▇██
wandb:                      mean_reward_24 ▁▃▅▆▆▆▆▇██
wandb:                      mean_reward_25 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_26 ▁▃▅▆▆▆▆▇▇█
wandb:                      mean_reward_27 ▁▃▅▆▆▆▆▇██
wandb:                      mean_reward_28 ▁▃▅▆▆▆▆▇██
wandb:                      mean_reward_29 ▁▃▅▆▆▅▆▇██
wandb:                       mean_reward_3 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_30 ▁▃▅▆▆▆▆▇██
wandb:                      mean_reward_31 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_32 ▁▃▅▆▆▆▆▇▇█
wandb:                      mean_reward_33 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_34 ▁▃▅▆▆▅▆▇▇█
wandb:                      mean_reward_35 ▁▃▅▆▆▆▆▇▇█
wandb:                       mean_reward_4 ▁▃▅▆▆▆▇▇██
wandb:                       mean_reward_5 ▁▃▅▆▆▆▇▇██
wandb:                       mean_reward_6 ▁▃▅▆▆▆▇▇██
wandb:                       mean_reward_7 ▁▃▅▆▆▆▆▇██
wandb:                       mean_reward_8 ▁▃▅▆▆▆▇▇██
wandb:                       mean_reward_9 ▁▃▅▆▆▆▆▇██
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▁▂▁▃▂▃▄▅▆▆█
wandb:                        std_reward_0 ▁▁▁▃▂▆█▆▅▃
wandb:                        std_reward_1 ▂▁▂▃▃▆█▆▅▄
wandb:                       std_reward_10 ▂▁▂▃▃▇█▇▆▄
wandb:                       std_reward_11 ▂▁▂▃▂█▇▆▅▃
wandb:                       std_reward_12 ▂▁▂▂▃██▆▆▄
wandb:                       std_reward_13 ▂▁▂▃▃▇█▆▅▄
wandb:                       std_reward_14 ▁▁▁▂▂▆█▆▄▄
wandb:                       std_reward_15 ▁▁▂▃▂██▆▅▃
wandb:                       std_reward_16 ▂▁▂▃▂▅█▅▄▃
wandb:                       std_reward_17 ▂▁▂▃▃▆█▅▅▄
wandb:                       std_reward_18 ▂▁▂▃▃▇█▆▅▄
wandb:                       std_reward_19 ▁▁▁▃▂▆█▅▅▄
wandb:                        std_reward_2 ▂▁▂▃▂▇█▆▅▃
wandb:                       std_reward_20 ▂▁▂▃▃██▆▆▄
wandb:                       std_reward_21 ▂▁▂▃▂██▆▅▄
wandb:                       std_reward_22 ▁▁▁▂▂▆█▅▅▃
wandb:                       std_reward_23 ▂▁▂▃▂▆█▇▅▃
wandb:                       std_reward_24 ▁▁▂▂▂▇█▅▄▄
wandb:                       std_reward_25 ▂▁▂▃▂▆█▇▆▅
wandb:                       std_reward_26 ▂▁▂▃▃▅█▅▄▃
wandb:                       std_reward_27 ▂▁▁▃▃▆█▆▄▄
wandb:                       std_reward_28 ▂▁▂▃▂▇█▆▅▃
wandb:                       std_reward_29 ▂▁▂▃▂▇█▅▅▄
wandb:                        std_reward_3 ▂▁▂▃▂▇█▅▅▄
wandb:                       std_reward_30 ▁▁▁▃▂▅█▅▄▃
wandb:                       std_reward_31 ▁▁▂▂▃▇██▆▄
wandb:                       std_reward_32 ▂▁▁▃▂▆█▅▅▄
wandb:                       std_reward_33 ▁▁▂▃▃▇█▇▆▅
wandb:                       std_reward_34 ▂▁▂▃▂██▆▅▄
wandb:                       std_reward_35 ▁▁▂▃▂▆█▅▅▃
wandb:                        std_reward_4 ▂▁▂▃▃██▆▅▄
wandb:                        std_reward_5 ▂▁▂▃▂█▇▇▅▄
wandb:                        std_reward_6 ▂▁▂▂▃▆█▆▄▃
wandb:                        std_reward_7 ▁▁▂▃▂▆█▆▄▃
wandb:                        std_reward_8 ▁▁▁▃▂▇█▆▅▄
wandb:                        std_reward_9 ▂▁▁▃▂▇█▆▆▃
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▂▃█▁▂▂▄▃▄▆▆
wandb:                 train/clip_fraction ▃▃▃▄▄▁▃▅▅▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▃▆▇▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▃▂▂▁▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▆▆▅█▆▅▃▄▂▁▂
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1117/global_step 212992
wandb:        PPO_1117/rollout/ep_len_mean 200.0
wandb:        PPO_1117/rollout/ep_rew_mean -806.53497
wandb:                   PPO_1117/time/fps 1183.0
wandb:            PPO_1117/train/approx_kl 0.01337
wandb:        PPO_1117/train/clip_fraction 0.17341
wandb:           PPO_1117/train/clip_range 0.2
wandb:         PPO_1117/train/entropy_loss -7.74243
wandb:   PPO_1117/train/explained_variance 0.97046
wandb:        PPO_1117/train/learning_rate 0.0003
wandb:                 PPO_1117/train/loss 59.46993
wandb: PPO_1117/train/policy_gradient_loss -0.00937
wandb:                  PPO_1117/train/std 0.72989
wandb:           PPO_1117/train/value_loss 91.51332
wandb:                PPO_1127/global_step 212992
wandb:        PPO_1127/rollout/ep_len_mean 200.0
wandb:        PPO_1127/rollout/ep_rew_mean -655.13483
wandb:                   PPO_1127/time/fps 1179.0
wandb:            PPO_1127/train/approx_kl 0.01411
wandb:        PPO_1127/train/clip_fraction 0.18803
wandb:           PPO_1127/train/clip_range 0.2
wandb:         PPO_1127/train/entropy_loss -6.86193
wandb:   PPO_1127/train/explained_variance 0.97384
wandb:        PPO_1127/train/learning_rate 0.0003
wandb:                 PPO_1127/train/loss 16.71447
wandb: PPO_1127/train/policy_gradient_loss -0.00642
wandb:                  PPO_1127/train/std 0.6443
wandb:           PPO_1127/train/value_loss 55.08305
wandb:                PPO_1135/global_step 212992
wandb:        PPO_1135/rollout/ep_len_mean 200.0
wandb:        PPO_1135/rollout/ep_rew_mean -580.97168
wandb:                   PPO_1135/time/fps 1181.0
wandb:            PPO_1135/train/approx_kl 0.01635
wandb:        PPO_1135/train/clip_fraction 0.22738
wandb:           PPO_1135/train/clip_range 0.2
wandb:         PPO_1135/train/entropy_loss -6.01133
wandb:   PPO_1135/train/explained_variance 0.98148
wandb:        PPO_1135/train/learning_rate 0.0003
wandb:                 PPO_1135/train/loss 25.00337
wandb: PPO_1135/train/policy_gradient_loss -0.00306
wandb:                  PPO_1135/train/std 0.57024
wandb:           PPO_1135/train/value_loss 37.68932
wandb:                PPO_1145/global_step 212992
wandb:        PPO_1145/rollout/ep_len_mean 200.0
wandb:        PPO_1145/rollout/ep_rew_mean -548.58728
wandb:                   PPO_1145/time/fps 1182.0
wandb:            PPO_1145/train/approx_kl 0.01828
wandb:        PPO_1145/train/clip_fraction 0.21689
wandb:           PPO_1145/train/clip_range 0.2
wandb:         PPO_1145/train/entropy_loss -5.30568
wandb:   PPO_1145/train/explained_variance 0.97491
wandb:        PPO_1145/train/learning_rate 0.0003
wandb:                 PPO_1145/train/loss 13.62266
wandb: PPO_1145/train/policy_gradient_loss -0.0032
wandb:                  PPO_1145/train/std 0.51584
wandb:           PPO_1145/train/value_loss 38.10627
wandb:                PPO_1155/global_step 212992
wandb:        PPO_1155/rollout/ep_len_mean 200.0
wandb:        PPO_1155/rollout/ep_rew_mean -547.87585
wandb:                   PPO_1155/time/fps 1177.0
wandb:            PPO_1155/train/approx_kl 0.01734
wandb:        PPO_1155/train/clip_fraction 0.23331
wandb:           PPO_1155/train/clip_range 0.2
wandb:         PPO_1155/train/entropy_loss -4.79177
wandb:   PPO_1155/train/explained_variance 0.99197
wandb:        PPO_1155/train/learning_rate 0.0003
wandb:                 PPO_1155/train/loss 57.05268
wandb: PPO_1155/train/policy_gradient_loss -0.00026
wandb:                  PPO_1155/train/std 0.48004
wandb:           PPO_1155/train/value_loss 51.98913
wandb:                PPO_1166/global_step 212992
wandb:        PPO_1166/rollout/ep_len_mean 200.0
wandb:        PPO_1166/rollout/ep_rew_mean -493.93027
wandb:                   PPO_1166/time/fps 1179.0
wandb:            PPO_1166/train/approx_kl 0.01863
wandb:        PPO_1166/train/clip_fraction 0.24725
wandb:           PPO_1166/train/clip_range 0.2
wandb:         PPO_1166/train/entropy_loss -4.34192
wandb:   PPO_1166/train/explained_variance 0.99613
wandb:        PPO_1166/train/learning_rate 0.0003
wandb:                 PPO_1166/train/loss 17.94283
wandb: PPO_1166/train/policy_gradient_loss -0.00153
wandb:                  PPO_1166/train/std 0.4497
wandb:           PPO_1166/train/value_loss 30.32799
wandb:                PPO_1176/global_step 212992
wandb:        PPO_1176/rollout/ep_len_mean 200.0
wandb:        PPO_1176/rollout/ep_rew_mean -472.10199
wandb:                   PPO_1176/time/fps 1170.0
wandb:            PPO_1176/train/approx_kl 0.02057
wandb:        PPO_1176/train/clip_fraction 0.26909
wandb:           PPO_1176/train/clip_range 0.2
wandb:         PPO_1176/train/entropy_loss -3.76953
wandb:   PPO_1176/train/explained_variance 0.99618
wandb:        PPO_1176/train/learning_rate 0.0003
wandb:                 PPO_1176/train/loss 4.34526
wandb: PPO_1176/train/policy_gradient_loss -0.00075
wandb:                  PPO_1176/train/std 0.41389
wandb:           PPO_1176/train/value_loss 14.17826
wandb:                PPO_1186/global_step 212992
wandb:        PPO_1186/rollout/ep_len_mean 200.0
wandb:        PPO_1186/rollout/ep_rew_mean -442.65521
wandb:                   PPO_1186/time/fps 1172.0
wandb:            PPO_1186/train/approx_kl 0.02213
wandb:        PPO_1186/train/clip_fraction 0.24152
wandb:           PPO_1186/train/clip_range 0.2
wandb:         PPO_1186/train/entropy_loss -3.16215
wandb:   PPO_1186/train/explained_variance 0.99333
wandb:        PPO_1186/train/learning_rate 0.0003
wandb:                 PPO_1186/train/loss 5.24602
wandb: PPO_1186/train/policy_gradient_loss -0.00042
wandb:                  PPO_1186/train/std 0.38062
wandb:           PPO_1186/train/value_loss 37.33779
wandb:                PPO_1196/global_step 212992
wandb:        PPO_1196/rollout/ep_len_mean 200.0
wandb:        PPO_1196/rollout/ep_rew_mean -387.63049
wandb:                   PPO_1196/time/fps 1174.0
wandb:            PPO_1196/train/approx_kl 0.02401
wandb:        PPO_1196/train/clip_fraction 0.27295
wandb:           PPO_1196/train/clip_range 0.2
wandb:         PPO_1196/train/entropy_loss -2.62132
wandb:   PPO_1196/train/explained_variance 0.99764
wandb:        PPO_1196/train/learning_rate 0.0003
wandb:                 PPO_1196/train/loss 3.05157
wandb: PPO_1196/train/policy_gradient_loss 0.00147
wandb:                  PPO_1196/train/std 0.35239
wandb:           PPO_1196/train/value_loss 12.52542
wandb:                    global_mean_eval -344.2355
wandb:                         global_step 212992
wandb:                       mean_reward_0 -328.32249
wandb:                       mean_reward_1 -335.10796
wandb:                      mean_reward_10 -335.345
wandb:                      mean_reward_11 -319.06802
wandb:                      mean_reward_12 -324.5103
wandb:                      mean_reward_13 -352.34166
wandb:                      mean_reward_14 -340.81282
wandb:                      mean_reward_15 -320.40336
wandb:                      mean_reward_16 -345.60057
wandb:                      mean_reward_17 -352.06167
wandb:                      mean_reward_18 -337.49166
wandb:                      mean_reward_19 -357.73939
wandb:                       mean_reward_2 -332.22568
wandb:                      mean_reward_20 -347.62554
wandb:                      mean_reward_21 -333.36358
wandb:                      mean_reward_22 -355.97691
wandb:                      mean_reward_23 -336.53302
wandb:                      mean_reward_24 -358.34481
wandb:                      mean_reward_25 -355.69773
wandb:                      mean_reward_26 -333.16312
wandb:                      mean_reward_27 -345.24116
wandb:                      mean_reward_28 -344.57724
wandb:                      mean_reward_29 -345.36716
wandb:                       mean_reward_3 -363.21964
wandb:                      mean_reward_30 -338.16022
wandb:                      mean_reward_31 -350.60821
wandb:                      mean_reward_32 -336.9381
wandb:                      mean_reward_33 -369.08202
wandb:                      mean_reward_34 -338.34881
wandb:                      mean_reward_35 -344.71727
wandb:                       mean_reward_4 -356.63212
wandb:                       mean_reward_5 -367.53833
wandb:                       mean_reward_6 -336.69842
wandb:                       mean_reward_7 -355.17359
wandb:                       mean_reward_8 -357.55288
wandb:                       mean_reward_9 -340.88762
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -931.94031
wandb:                        std_reward_0 97.24383
wandb:                        std_reward_1 98.1722
wandb:                       std_reward_10 89.8402
wandb:                       std_reward_11 78.48317
wandb:                       std_reward_12 91.36386
wandb:                       std_reward_13 100.5348
wandb:                       std_reward_14 98.92139
wandb:                       std_reward_15 78.99336
wandb:                       std_reward_16 94.37805
wandb:                       std_reward_17 92.8945
wandb:                       std_reward_18 89.95932
wandb:                       std_reward_19 107.12979
wandb:                        std_reward_2 74.69759
wandb:                       std_reward_20 89.67911
wandb:                       std_reward_21 94.94468
wandb:                       std_reward_22 86.27447
wandb:                       std_reward_23 89.35972
wandb:                       std_reward_24 111.18888
wandb:                       std_reward_25 102.67087
wandb:                       std_reward_26 86.5129
wandb:                       std_reward_27 101.15824
wandb:                       std_reward_28 88.07574
wandb:                       std_reward_29 101.30665
wandb:                        std_reward_3 103.99157
wandb:                       std_reward_30 86.53874
wandb:                       std_reward_31 92.9074
wandb:                       std_reward_32 98.19385
wandb:                       std_reward_33 112.80678
wandb:                       std_reward_34 94.70476
wandb:                       std_reward_35 94.30607
wandb:                        std_reward_4 92.28082
wandb:                        std_reward_5 103.94557
wandb:                        std_reward_6 88.46338
wandb:                        std_reward_7 92.87797
wandb:                        std_reward_8 105.63941
wandb:                        std_reward_9 88.7726
wandb:                            time/fps 1178.0
wandb:                     train/approx_kl 0.01244
wandb:                 train/clip_fraction 0.15652
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7411
wandb:            train/explained_variance 0.96718
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 11.08948
wandb:          train/policy_gradient_loss -0.01304
wandb:                           train/std 0.84138
wandb:                    train/value_loss 21.77692
wandb: 
wandb: Synced dazzling-jazz-32: https://wandb.ai/tidiane/meta_rl_context/runs/2fbip4iu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_012622-2fbip4iu/logs
wandb: Waiting for W&B process to finish... (success).
wandb: Waiting for W&B process to finish... (success).
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1118/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1118/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1118/rollout/ep_rew_mean ▁▂▁▃▂▂▃▃▄▆▆█
wandb:                   PPO_1118/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1118/train/approx_kl ▂▁▂▁▃▂▄▄▇▄█
wandb:        PPO_1118/train/clip_fraction ▃▁▂▁▄▅▄▆▇▅█
wandb:           PPO_1118/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1118/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1118/train/explained_variance ▂▂▄▁▇▅▅▇█▃▃
wandb:        PPO_1118/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1118/train/loss ▂▂▃█▁▁▂▁▁▁▂
wandb: PPO_1118/train/policy_gradient_loss ▁▄▄▇▂▃█▆▁▆▅
wandb:                  PPO_1118/train/std █▇▇▆▅▅▄▃▂▂▁
wandb:           PPO_1118/train/value_loss ▁▆▂▆▄▄▆▄▃█▆
wandb:                PPO_1128/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1128/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1128/rollout/ep_rew_mean ▁▂▂▄▃▄▆▅▇▇▇█
wandb:                   PPO_1128/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1128/train/approx_kl ▃▁▅▇▅█▃▃▆▅▇
wandb:        PPO_1128/train/clip_fraction ▂▂▆█▃▅▁▂▅▂▂
wandb:           PPO_1128/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1128/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1128/train/explained_variance ▇▄▄▅█▁▂▅▃█▅
wandb:        PPO_1128/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1128/train/loss ▂██▂▃▁▂▂▁▁▂
wandb: PPO_1128/train/policy_gradient_loss ▄▅▁▄▄▅▆▆▆▆█
wandb:                  PPO_1128/train/std █▇▇▆▅▅▄▄▃▂▁
wandb:           PPO_1128/train/value_loss █▇▄▁▃▄▃▅▅▅▅
wandb:                PPO_1137/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1137/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1137/rollout/ep_rew_mean ▁▁▂▃▃▄▄▅▆▇██
wandb:                   PPO_1137/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1137/train/approx_kl ▂▁▄▄▁▄▅▄▇█▇
wandb:        PPO_1137/train/clip_fraction ▂▁▂▄▂▆▆▄▆█▆
wandb:           PPO_1137/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1137/train/entropy_loss ▁▂▂▃▃▄▄▅▅▆█
wandb:   PPO_1137/train/explained_variance ▅▃▁▄▁▅▆▇███
wandb:        PPO_1137/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1137/train/loss ▄▇█▅▃▂▄▁▁▆▁
wandb: PPO_1137/train/policy_gradient_loss ▂▃▂▆▃▁▂▅▅▅█
wandb:                  PPO_1137/train/std ██▇▆▆▅▅▄▃▃▁
wandb:           PPO_1137/train/value_loss ▇▇█▆▇▅▃▃▃▂▁
wandb:                PPO_1146/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1146/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1146/rollout/ep_rew_mean ▁▃▃▄▅▆▅▇▅▅▆█
wandb:                   PPO_1146/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1146/train/approx_kl ▁█▁▅▄▃▁▅▆▅▃
wandb:        PPO_1146/train/clip_fraction ▁█▅▆▅▇▃▇▅█▁
wandb:           PPO_1146/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1146/train/entropy_loss ▁▂▂▃▄▅▆▆▇▇█
wandb:   PPO_1146/train/explained_variance ▆▇█▇▇█▆▃▄▁▃
wandb:        PPO_1146/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1146/train/loss ▃▃▂▂▃▁▃▄█▅▃
wandb: PPO_1146/train/policy_gradient_loss ▁▅▅▅▅▄▄▅▄█▆
wandb:                  PPO_1146/train/std █▇▆▅▅▄▃▃▂▂▁
wandb:           PPO_1146/train/value_loss ▄▂▁▂▂▁▃▄█▅▅
wandb:                PPO_1158/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1158/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1158/rollout/ep_rew_mean ▃▅▄▆▇█▆▁▃▇▇▆
wandb:                   PPO_1158/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1158/train/approx_kl ▃▃▃▇▅█▁▂▂▆▇
wandb:        PPO_1158/train/clip_fraction ▁▃▂█▃▇▂▂▁▆█
wandb:           PPO_1158/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1158/train/entropy_loss ▁▂▁▂▃▄▅▅▅▇█
wandb:   PPO_1158/train/explained_variance ▁▃▂▅▅▆▅▅▇▆█
wandb:        PPO_1158/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1158/train/loss ▄▂▇█▅▂▅▇▇▁▆
wandb: PPO_1158/train/policy_gradient_loss ▅▇▅▁▅▆▅█▇▆▁
wandb:                  PPO_1158/train/std █▇█▇▅▅▄▄▃▂▁
wandb:           PPO_1158/train/value_loss █▅█▅▅▃▆█▅▃▁
wandb:                PPO_1168/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1168/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1168/rollout/ep_rew_mean ▁▂▂▂▅▅▄▆▄▃▅█
wandb:                   PPO_1168/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1168/train/approx_kl ▂▁▃▃▆▆▅▄█▅▂
wandb:        PPO_1168/train/clip_fraction ▁▂▅▂▇▅█▆▇▇▃
wandb:           PPO_1168/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1168/train/entropy_loss ▁▁▂▃▄▄▅▆▆▇█
wandb:   PPO_1168/train/explained_variance ▁▅▆▃▆▇▇▆▆█▆
wandb:        PPO_1168/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1168/train/loss █▂▁▅▂█▁▃▃▃▁
wandb: PPO_1168/train/policy_gradient_loss ▂▇▃▁▇▁▄▃▃█▄
wandb:                  PPO_1168/train/std ██▇▆▅▅▄▃▃▂▁
wandb:           PPO_1168/train/value_loss █▃▂▆▂▂▁▂▅▃▅
wandb:                PPO_1178/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1178/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1178/rollout/ep_rew_mean ▁▂▄▄▅▂▂█▆▆██
wandb:                   PPO_1178/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1178/train/approx_kl ▂▃▄▃▄▃▁▇▃█▃
wandb:        PPO_1178/train/clip_fraction ▂▂▅▃▃▁▂█▅▇▄
wandb:           PPO_1178/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1178/train/entropy_loss ▁▁▂▂▂▃▃▄▅▇█
wandb:   PPO_1178/train/explained_variance ▆██▇▆▅▁▅▆▃▁
wandb:        PPO_1178/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1178/train/loss ▂▁▂▁▅▂▅▅█▁▂
wandb: PPO_1178/train/policy_gradient_loss ▄▅▃▃▅▂▁█▅▄▂
wandb:                  PPO_1178/train/std ██▇▇▇▆▅▅▃▂▁
wandb:           PPO_1178/train/value_loss ▅▃▃▅▅█▇▅▃▁▅
wandb:                PPO_1188/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1188/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1188/rollout/ep_rew_mean ▁▂▂▄▅▆▄▃█▇▃▄
wandb:                   PPO_1188/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1188/train/approx_kl ▇▁▄▂▃▂█▄▂▃▄
wandb:        PPO_1188/train/clip_fraction ▆▃▅▂█▃█▁▄█▆
wandb:           PPO_1188/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1188/train/entropy_loss ▁▂▂▃▄▅▅▆▆▇█
wandb:   PPO_1188/train/explained_variance ▆▇▆▄▁▅▇▄▇▆█
wandb:        PPO_1188/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1188/train/loss ▁▁▁▂▁▁▂▁▁█▁
wandb: PPO_1188/train/policy_gradient_loss ▆▂▅▄▄▃▇▁▄▆█
wandb:                  PPO_1188/train/std █▇▇▆▅▄▄▄▃▂▁
wandb:           PPO_1188/train/value_loss ▆▇▆█▇▄▃▅▁▆▂
wandb:                PPO_1197/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1197/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1197/rollout/ep_rew_mean ▁▂▅▂▄▅▃▅▆▆▇█
wandb:                   PPO_1197/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1197/train/approx_kl ▄▂▅▁▃▄▅▇▅▄█
wandb:        PPO_1197/train/clip_fraction ▄▆▆▃▅▅▇█▇▁▇
wandb:           PPO_1197/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1197/train/entropy_loss ▁▁▂▂▃▄▄▅▆▆█
wandb:   PPO_1197/train/explained_variance ▇█▁▅▅▅▆▇▇▆▆
wandb:        PPO_1197/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1197/train/loss █▁▁▁▂▂▆▁▁▁▁
wandb: PPO_1197/train/policy_gradient_loss █▇▄▄▄▃▅▅▃▁▇
wandb:                  PPO_1197/train/std ██▇▇▆▅▄▄▃▃▁
wandb:           PPO_1197/train/value_loss █▆▃▄▃▄▅▁▁▄▃
wandb:                    global_mean_eval ▁▂▃▅▆▆▆▇██
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▂▃▅▆▆▇▇▇█
wandb:                       mean_reward_1 ▁▂▃▆▇▇▇███
wandb:                      mean_reward_10 ▁▂▃▆▆▆▇▇██
wandb:                      mean_reward_11 ▁▂▃▅▇▆▇███
wandb:                      mean_reward_12 ▁▂▃▆▆▆▆▇██
wandb:                      mean_reward_13 ▁▂▃▅▆▆▆▇█▇
wandb:                      mean_reward_14 ▁▂▃▆▇▅▇▇██
wandb:                      mean_reward_15 ▁▂▃▆▆▆▆▇▇█
wandb:                      mean_reward_16 ▁▂▃▆▇▆▇███
wandb:                      mean_reward_17 ▁▂▃▅▆▆▇▇██
wandb:                      mean_reward_18 ▁▂▃▅▆▆▇▇██
wandb:                      mean_reward_19 ▁▂▃▅▆▆▆▇██
wandb:                       mean_reward_2 ▁▂▃▅▆▆▅▇██
wandb:                      mean_reward_20 ▁▂▃▅▆▆▇▇▇█
wandb:                      mean_reward_21 ▁▂▃▅▆▆▇▇▇█
wandb:                      mean_reward_22 ▁▂▃▅▆▅▆▇▇█
wandb:                      mean_reward_23 ▁▂▃▅▆▅▆▇▇█
wandb:                      mean_reward_24 ▁▂▃▆▇▆▇▇██
wandb:                      mean_reward_25 ▁▂▃▆▆▆▆███
wandb:                      mean_reward_26 ▁▂▃▆▆▆▆█▇█
wandb:                      mean_reward_27 ▁▂▃▅▆▆▆▇▇█
wandb:                      mean_reward_28 ▁▂▃▆▆▆▇▇██
wandb:                      mean_reward_29 ▁▂▃▅▆▅▆▇██
wandb:                       mean_reward_3 ▁▂▃▅▆▆▆▇██
wandb:                      mean_reward_30 ▁▂▃▆▆▆▆███
wandb:                      mean_reward_31 ▁▂▃▅▆▆▇▇▇█
wandb:                      mean_reward_32 ▁▂▃▅▇▅▆▇██
wandb:                      mean_reward_33 ▁▂▃▅▆▅▆▇▇█
wandb:                      mean_reward_34 ▁▂▃▅▆▆▆▇▇█
wandb:                      mean_reward_35 ▁▂▃▅▆▆▇▇▇█
wandb:                       mean_reward_4 ▁▂▃▅▇▆▆▇▇█
wandb:                       mean_reward_5 ▁▂▃▅▇▆▇▇▇█
wandb:                       mean_reward_6 ▁▂▃▆▆▆▆▇██
wandb:                       mean_reward_7 ▁▂▃▅▆▅▆▇▇█
wandb:                       mean_reward_8 ▁▂▃▅▆▅▇▇▇█
wandb:                       mean_reward_9 ▁▂▃▅▆▇▇███
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▁▂▁▃▂▃▄▅▆▆█
wandb:                        std_reward_0 ▃▂▁▂▃█▆▅▅▄
wandb:                        std_reward_1 ▃▂▁▃▄▇█▅▇▅
wandb:                       std_reward_10 ▃▂▁▃▃█▆▄▅▅
wandb:                       std_reward_11 ▃▂▁▃▄██▅▅▅
wandb:                       std_reward_12 ▃▂▁▂▃██▄▄▄
wandb:                       std_reward_13 ▃▂▁▃▄██▅▄▅
wandb:                       std_reward_14 ▃▁▁▂▃█▆▄▄▄
wandb:                       std_reward_15 ▃▂▁▂▄██▅▅▄
wandb:                       std_reward_16 ▃▁▁▃▄██▆▅▅
wandb:                       std_reward_17 ▃▂▁▂▃█▇▆▆▅
wandb:                       std_reward_18 ▃▁▁▂▃█▆▅▄▄
wandb:                       std_reward_19 ▃▂▁▃▃██▅▅▄
wandb:                        std_reward_2 ▃▂▁▃▄██▄▅▄
wandb:                       std_reward_20 ▃▁▁▃▄█▆▅▇▄
wandb:                       std_reward_21 ▃▂▁▃▄█▇▅▅▅
wandb:                       std_reward_22 ▃▂▁▂▃█▇▅▅▄
wandb:                       std_reward_23 ▃▂▁▃▄██▅▅▃
wandb:                       std_reward_24 ▃▁▁▂▄█▇▆▅▅
wandb:                       std_reward_25 ▃▁▁▂▃██▄▅▅
wandb:                       std_reward_26 ▃▂▁▂▃█▇▄▆▅
wandb:                       std_reward_27 ▃▂▁▂▃█▇▅▅▄
wandb:                       std_reward_28 ▃▁▁▂▄█▇▅▄▅
wandb:                       std_reward_29 ▃▂▁▂▄█▇▅▄▄
wandb:                        std_reward_3 ▃▂▁▂▃█▇▅▄▄
wandb:                       std_reward_30 ▃▁▁▂▃█▇▄▅▅
wandb:                       std_reward_31 ▃▂▁▂▃█▆▅▅▅
wandb:                       std_reward_32 ▃▁▁▂▃█▇▅▄▄
wandb:                       std_reward_33 ▃▂▁▂▄██▄▆▄
wandb:                       std_reward_34 ▃▂▁▂▃▇█▄▅▄
wandb:                       std_reward_35 ▃▂▁▂▃█▇▅▅▄
wandb:                        std_reward_4 ▃▂▁▃▄█▇▅▅▅
wandb:                        std_reward_5 ▃▂▁▂▄██▅▅▅
wandb:                        std_reward_6 ▃▂▁▃▄█▇▄▅▅
wandb:                        std_reward_7 ▃▂▁▂▄█▇▅▄▄
wandb:                        std_reward_8 ▃▁▁▂▄█▆▅▅▄
wandb:                        std_reward_9 ▃▂▁▃▄██▆▆▆
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▂▃█▁▂▂▄▃▄▆▆
wandb:                 train/clip_fraction ▃▃▃▄▄▁▃▅▅▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▃▆▇▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▃▂▂▁▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▆▆▅█▆▅▃▄▂▁▂
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1118/global_step 212992
wandb:        PPO_1118/rollout/ep_len_mean 200.0
wandb:        PPO_1118/rollout/ep_rew_mean -806.60681
wandb:                   PPO_1118/time/fps 1188.0
wandb:            PPO_1118/train/approx_kl 0.01425
wandb:        PPO_1118/train/clip_fraction 0.17943
wandb:           PPO_1118/train/clip_range 0.2
wandb:         PPO_1118/train/entropy_loss -7.66831
wandb:   PPO_1118/train/explained_variance 0.96717
wandb:        PPO_1118/train/learning_rate 0.0003
wandb:                 PPO_1118/train/loss 24.69952
wandb: PPO_1118/train/policy_gradient_loss -0.00747
wandb:                  PPO_1118/train/std 0.72216
wandb:           PPO_1118/train/value_loss 64.70163
wandb:                PPO_1128/global_step 212992
wandb:        PPO_1128/rollout/ep_len_mean 200.0
wandb:        PPO_1128/rollout/ep_rew_mean -737.68671
wandb:                   PPO_1128/time/fps 1187.0
wandb:            PPO_1128/train/approx_kl 0.01426
wandb:        PPO_1128/train/clip_fraction 0.17299
wandb:           PPO_1128/train/clip_range 0.2
wandb:         PPO_1128/train/entropy_loss -6.85158
wandb:   PPO_1128/train/explained_variance 0.96573
wandb:        PPO_1128/train/learning_rate 0.0003
wandb:                 PPO_1128/train/loss 24.70881
wandb: PPO_1128/train/policy_gradient_loss -0.00618
wandb:                  PPO_1128/train/std 0.64266
wandb:           PPO_1128/train/value_loss 63.74884
wandb:                PPO_1137/global_step 212992
wandb:        PPO_1137/rollout/ep_len_mean 200.0
wandb:        PPO_1137/rollout/ep_rew_mean -611.55518
wandb:                   PPO_1137/time/fps 1181.0
wandb:            PPO_1137/train/approx_kl 0.01725
wandb:        PPO_1137/train/clip_fraction 0.21804
wandb:           PPO_1137/train/clip_range 0.2
wandb:         PPO_1137/train/entropy_loss -5.98934
wandb:   PPO_1137/train/explained_variance 0.9784
wandb:        PPO_1137/train/learning_rate 0.0003
wandb:                 PPO_1137/train/loss 5.86868
wandb: PPO_1137/train/policy_gradient_loss -0.00478
wandb:                  PPO_1137/train/std 0.56878
wandb:           PPO_1137/train/value_loss 19.70825
wandb:                PPO_1146/global_step 212992
wandb:        PPO_1146/rollout/ep_len_mean 200.0
wandb:        PPO_1146/rollout/ep_rew_mean -537.79535
wandb:                   PPO_1146/time/fps 1182.0
wandb:            PPO_1146/train/approx_kl 0.01695
wandb:        PPO_1146/train/clip_fraction 0.19887
wandb:           PPO_1146/train/clip_range 0.2
wandb:         PPO_1146/train/entropy_loss -5.29923
wandb:   PPO_1146/train/explained_variance 0.95942
wandb:        PPO_1146/train/learning_rate 0.0003
wandb:                 PPO_1146/train/loss 16.36348
wandb: PPO_1146/train/policy_gradient_loss -0.00342
wandb:                  PPO_1146/train/std 0.51557
wandb:           PPO_1146/train/value_loss 51.32154
wandb:                PPO_1158/global_step 212992
wandb:        PPO_1158/rollout/ep_len_mean 200.0
wandb:        PPO_1158/rollout/ep_rew_mean -538.49762
wandb:                   PPO_1158/time/fps 1184.0
wandb:            PPO_1158/train/approx_kl 0.01911
wandb:        PPO_1158/train/clip_fraction 0.23535
wandb:           PPO_1158/train/clip_range 0.2
wandb:         PPO_1158/train/entropy_loss -5.09153
wandb:   PPO_1158/train/explained_variance 0.99446
wandb:        PPO_1158/train/learning_rate 0.0003
wandb:                 PPO_1158/train/loss 29.89468
wandb: PPO_1158/train/policy_gradient_loss -0.00628
wandb:                  PPO_1158/train/std 0.50045
wandb:           PPO_1158/train/value_loss 27.54531
wandb:                PPO_1168/global_step 212992
wandb:        PPO_1168/rollout/ep_len_mean 200.0
wandb:        PPO_1168/rollout/ep_rew_mean -471.39389
wandb:                   PPO_1168/time/fps 1179.0
wandb:            PPO_1168/train/approx_kl 0.01606
wandb:        PPO_1168/train/clip_fraction 0.20063
wandb:           PPO_1168/train/clip_range 0.2
wandb:         PPO_1168/train/entropy_loss -4.57132
wandb:   PPO_1168/train/explained_variance 0.99565
wandb:        PPO_1168/train/learning_rate 0.0003
wandb:                 PPO_1168/train/loss 5.73586
wandb: PPO_1168/train/policy_gradient_loss -0.00375
wandb:                  PPO_1168/train/std 0.46466
wandb:           PPO_1168/train/value_loss 63.19474
wandb:                PPO_1178/global_step 212992
wandb:        PPO_1178/rollout/ep_len_mean 200.0
wandb:        PPO_1178/rollout/ep_rew_mean -474.38269
wandb:                   PPO_1178/time/fps 1180.0
wandb:            PPO_1178/train/approx_kl 0.02038
wandb:        PPO_1178/train/clip_fraction 0.26533
wandb:           PPO_1178/train/clip_range 0.2
wandb:         PPO_1178/train/entropy_loss -4.06835
wandb:   PPO_1178/train/explained_variance 0.99416
wandb:        PPO_1178/train/learning_rate 0.0003
wandb:                 PPO_1178/train/loss 12.98298
wandb: PPO_1178/train/policy_gradient_loss -0.00236
wandb:                  PPO_1178/train/std 0.43282
wandb:           PPO_1178/train/value_loss 54.4174
wandb:                PPO_1188/global_step 212992
wandb:        PPO_1188/rollout/ep_len_mean 200.0
wandb:        PPO_1188/rollout/ep_rew_mean -472.3027
wandb:                   PPO_1188/time/fps 1183.0
wandb:            PPO_1188/train/approx_kl 0.02418
wandb:        PPO_1188/train/clip_fraction 0.28844
wandb:           PPO_1188/train/clip_range 0.2
wandb:         PPO_1188/train/entropy_loss -3.55841
wandb:   PPO_1188/train/explained_variance 0.99724
wandb:        PPO_1188/train/learning_rate 0.0003
wandb:                 PPO_1188/train/loss 8.71315
wandb: PPO_1188/train/policy_gradient_loss 0.00314
wandb:                  PPO_1188/train/std 0.40193
wandb:           PPO_1188/train/value_loss 26.62407
wandb:                PPO_1197/global_step 212992
wandb:        PPO_1197/rollout/ep_len_mean 200.0
wandb:        PPO_1197/rollout/ep_rew_mean -394.75476
wandb:                   PPO_1197/time/fps 1184.0
wandb:            PPO_1197/train/approx_kl 0.02838
wandb:        PPO_1197/train/clip_fraction 0.31467
wandb:           PPO_1197/train/clip_range 0.2
wandb:         PPO_1197/train/entropy_loss -2.82475
wandb:   PPO_1197/train/explained_variance 0.99662
wandb:        PPO_1197/train/learning_rate 0.0003
wandb:                 PPO_1197/train/loss 4.69701
wandb: PPO_1197/train/policy_gradient_loss 0.00344
wandb:                  PPO_1197/train/std 0.36177
wandb:           PPO_1197/train/value_loss 23.04381
wandb:                    global_mean_eval -377.32494
wandb:                         global_step 212992
wandb:                       mean_reward_0 -381.23039
wandb:                       mean_reward_1 -404.54136
wandb:                      mean_reward_10 -383.21759
wandb:                      mean_reward_11 -401.57254
wandb:                      mean_reward_12 -362.42391
wandb:                      mean_reward_13 -397.3539
wandb:                      mean_reward_14 -386.20892
wandb:                      mean_reward_15 -356.66323
wandb:                      mean_reward_16 -403.26827
wandb:                      mean_reward_17 -387.77858
wandb:                      mean_reward_18 -375.09282
wandb:                      mean_reward_19 -378.47053
wandb:                       mean_reward_2 -361.24195
wandb:                      mean_reward_20 -357.88496
wandb:                      mean_reward_21 -355.70051
wandb:                      mean_reward_22 -356.18324
wandb:                      mean_reward_23 -329.30216
wandb:                      mean_reward_24 -395.35839
wandb:                      mean_reward_25 -397.62871
wandb:                      mean_reward_26 -394.8055
wandb:                      mean_reward_27 -355.79927
wandb:                      mean_reward_28 -391.52652
wandb:                      mean_reward_29 -373.34624
wandb:                       mean_reward_3 -372.89585
wandb:                      mean_reward_30 -388.45349
wandb:                      mean_reward_31 -372.08257
wandb:                      mean_reward_32 -380.02053
wandb:                      mean_reward_33 -376.02525
wandb:                      mean_reward_34 -381.62817
wandb:                      mean_reward_35 -366.76962
wandb:                       mean_reward_4 -380.64097
wandb:                       mean_reward_5 -377.6287
wandb:                       mean_reward_6 -380.39306
wandb:                       mean_reward_7 -359.8588
wandb:                       mean_reward_8 -363.96383
wandb:                       mean_reward_9 -396.73764
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -931.94031
wandb:                        std_reward_0 137.87462
wandb:                        std_reward_1 144.87044
wandb:                       std_reward_10 149.87105
wandb:                       std_reward_11 156.80139
wandb:                       std_reward_12 127.5912
wandb:                       std_reward_13 154.91294
wandb:                       std_reward_14 147.86673
wandb:                       std_reward_15 116.85423
wandb:                       std_reward_16 146.35836
wandb:                       std_reward_17 156.8356
wandb:                       std_reward_18 133.63184
wandb:                       std_reward_19 137.25115
wandb:                        std_reward_2 122.68329
wandb:                       std_reward_20 111.26418
wandb:                       std_reward_21 128.79909
wandb:                       std_reward_22 115.69526
wandb:                       std_reward_23 90.46953
wandb:                       std_reward_24 150.42602
wandb:                       std_reward_25 156.07459
wandb:                       std_reward_26 161.81002
wandb:                       std_reward_27 124.22723
wandb:                       std_reward_28 146.34034
wandb:                       std_reward_29 131.86359
wandb:                        std_reward_3 136.51357
wandb:                       std_reward_30 143.78728
wandb:                       std_reward_31 151.87329
wandb:                       std_reward_32 149.4344
wandb:                       std_reward_33 131.10291
wandb:                       std_reward_34 149.10726
wandb:                       std_reward_35 136.64276
wandb:                        std_reward_4 149.7594
wandb:                        std_reward_5 134.10172
wandb:                        std_reward_6 145.14849
wandb:                        std_reward_7 121.58371
wandb:                        std_reward_8 125.84304
wandb:                        std_reward_9 166.6153
wandb:                            time/fps 1178.0
wandb:                     train/approx_kl 0.01244
wandb:                 train/clip_fraction 0.15652
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7411
wandb:            train/explained_variance 0.96718
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 11.08948
wandb:          train/policy_gradient_loss -0.01304
wandb:                           train/std 0.84138
wandb:                    train/value_loss 21.77692
wandb: 
wandb: Synced pretty-dust-31: https://wandb.ai/tidiane/meta_rl_context/runs/37wdxx29
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_012622-37wdxx29/logs
wandb: 
wandb: Run history:
wandb:                PPO_1119/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1119/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1119/rollout/ep_rew_mean ▁▁▂▃▄▃▅▅▆▆▇█
wandb:                   PPO_1119/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1119/train/approx_kl ▄▁▃▆▄▄▅█▄▇▂
wandb:        PPO_1119/train/clip_fraction ▄▁▁▅▆▄█▇▄█▆
wandb:           PPO_1119/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1119/train/entropy_loss ▁▂▂▃▄▄▅▆▇▇█
wandb:   PPO_1119/train/explained_variance ▇▆▇▃█▁▅▅▇▅▅
wandb:        PPO_1119/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1119/train/loss ▂▂▁▂▅▂▃▅▆█▅
wandb: PPO_1119/train/policy_gradient_loss ▁▇▅▆▃▄▅▁▅▄█
wandb:                  PPO_1119/train/std █▇▆▆▅▅▄▃▂▂▁
wandb:           PPO_1119/train/value_loss ▁▂▃█▃▇▄▅▅▅▅
wandb:                PPO_1129/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1129/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1129/rollout/ep_rew_mean ▁▁▂▃▄▅▆▇▇▇██
wandb:                   PPO_1129/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1129/train/approx_kl ▁▃▁▃▁▁▂▂▅█▅
wandb:        PPO_1129/train/clip_fraction ▂▂▁▂▃▄▅▃▅▇█
wandb:           PPO_1129/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1129/train/entropy_loss ▁▂▃▃▄▄▅▅▆▆█
wandb:   PPO_1129/train/explained_variance ▁▄▅▄▅▇▇▆█▆▆
wandb:        PPO_1129/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1129/train/loss ▅▄▅▄██▂▃▁▂▂
wandb: PPO_1129/train/policy_gradient_loss ▁▂▄▃▄▁▄▆▅▃█
wandb:                  PPO_1129/train/std █▇▆▆▅▅▄▄▃▃▁
wandb:           PPO_1129/train/value_loss █▇█▆█▅▂▂▃▄▁
wandb:                PPO_1139/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1139/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1139/rollout/ep_rew_mean ▁▂▄▃▆▄▅▆▇▇▆█
wandb:                   PPO_1139/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1139/train/approx_kl ▂▃▁▅▄▂▄▃▄▂█
wandb:        PPO_1139/train/clip_fraction ▃▄▂▅█▁▅▄▅▄█
wandb:           PPO_1139/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1139/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1139/train/explained_variance ▃▅▃▅▁▅▁▆█▄▄
wandb:        PPO_1139/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1139/train/loss ▃█▂▁▂▁▄▃▁▂█
wandb: PPO_1139/train/policy_gradient_loss ▁▂▃▃▅▆▅▅█▇█
wandb:                  PPO_1139/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1139/train/value_loss ▆▄█▇▂▄▇▃▁▂▅
wandb:                PPO_1149/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1149/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1149/rollout/ep_rew_mean ▁▂▄▅▆█▇█▅██▅
wandb:                   PPO_1149/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1149/train/approx_kl ▃█▆▆▂▁▇▆▄▄▂
wandb:        PPO_1149/train/clip_fraction ▁▄▆█▇▄▆▄▄▄▅
wandb:           PPO_1149/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1149/train/entropy_loss ▁▂▃▄▄▅▅▆▇▇█
wandb:   PPO_1149/train/explained_variance ▇▇▇█▁▆▆▇▅▇▆
wandb:        PPO_1149/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1149/train/loss ▃▂▂▄▂▁▂▁▃█▆
wandb: PPO_1149/train/policy_gradient_loss ▄▁▄▇█▆▇▃▄▅▄
wandb:                  PPO_1149/train/std █▇▆▅▅▄▃▃▁▂▁
wandb:           PPO_1149/train/value_loss ▆▅▅▃▇▂▁▂▃▃█
wandb:                PPO_1159/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1159/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1159/rollout/ep_rew_mean ▁▃▂▃▆▃▅█▅▇▆█
wandb:                   PPO_1159/time/fps █▂▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1159/train/approx_kl ▂▄▅█▁▄▅▅▄▆▅
wandb:        PPO_1159/train/clip_fraction ▂▆▅▆▁▄▇█▂█▇
wandb:           PPO_1159/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1159/train/entropy_loss ▁▂▂▃▃▄▅▅▆▇█
wandb:   PPO_1159/train/explained_variance ▁▄▃▄▅▅▅▆▅▆█
wandb:        PPO_1159/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1159/train/loss █▃▃▃▂▅▄▁▄▇▁
wandb: PPO_1159/train/policy_gradient_loss ▅▅▁▆▂▃▄▄▃▄█
wandb:                  PPO_1159/train/std █▇▇▆▆▅▄▄▃▂▁
wandb:           PPO_1159/train/value_loss █▄▅▅▇▇▆▃█▄▁
wandb:                PPO_1169/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1169/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1169/rollout/ep_rew_mean ▇▅▄▅█▅▂▄█▁█▆
wandb:                   PPO_1169/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1169/train/approx_kl ▆▄▄▄▃▁▁▅█▅▃
wandb:        PPO_1169/train/clip_fraction █▂▃▆▄▂▁▆▇▁▂
wandb:           PPO_1169/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1169/train/entropy_loss ▁▂▂▄▄▅▅▅▆▇█
wandb:   PPO_1169/train/explained_variance ▄▁▂▅▅▄▃▄▄██
wandb:        PPO_1169/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1169/train/loss ▄█▄▂▁▅▄▅▅▃▂
wandb: PPO_1169/train/policy_gradient_loss ▆▃▄█▅▂▃█▃▁▄
wandb:                  PPO_1169/train/std █▇▆▆▅▄▄▄▂▂▁
wandb:           PPO_1169/train/value_loss ▂▄▄▁▁▃█▄▃▃▃
wandb:                PPO_1179/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1179/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1179/rollout/ep_rew_mean ▅▁▄▅▆▅▄▅▆▅█▄
wandb:                   PPO_1179/time/fps █▂▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1179/train/approx_kl ▁▃▆▃▃██▇▆▃▅
wandb:        PPO_1179/train/clip_fraction ▁▁▃▃▂▂▄█▂▄▅
wandb:           PPO_1179/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1179/train/entropy_loss ▁▂▃▃▄▄▅▅▆▇█
wandb:   PPO_1179/train/explained_variance ▄▆█▃▅▆▃▁▂▂▁
wandb:        PPO_1179/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1179/train/loss ▄▄▃█▁▇▄▂▄▅▁
wandb: PPO_1179/train/policy_gradient_loss ▂▄▁▅▃▇▄▆▄█▇
wandb:                  PPO_1179/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1179/train/value_loss ▃▃▁▁▂▃▄▅▆█▄
wandb:                PPO_1189/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1189/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1189/rollout/ep_rew_mean ▄▂▅▆▂▃▁▅█▄▅▅
wandb:                   PPO_1189/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1189/train/approx_kl ▇▁▅▆▂█▂▇▆▁▅
wandb:        PPO_1189/train/clip_fraction █▁▅▆▁█▂▇▅▂▅
wandb:           PPO_1189/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1189/train/entropy_loss ▁▂▂▃▃▄▅▆▇▇█
wandb:   PPO_1189/train/explained_variance ▇▇█▄▁▇▄▆▂▆▄
wandb:        PPO_1189/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1189/train/loss ▁▄▅▂▃▁██▂▃▆
wandb: PPO_1189/train/policy_gradient_loss ▇▂▇▆▂▆▁█▇▄▆
wandb:                  PPO_1189/train/std █▇▇▆▆▅▄▃▃▂▁
wandb:           PPO_1189/train/value_loss ▂▅▃▁█▂▇▂▂▄▄
wandb:                PPO_1199/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1199/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1199/rollout/ep_rew_mean ▂▁▂▃▄▅▅██▆▇█
wandb:                   PPO_1199/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1199/train/approx_kl ▄▂▆▃▁▃▃▂▆█▃
wandb:        PPO_1199/train/clip_fraction ▅▁▄▅▂▆▄▃▃█▁
wandb:           PPO_1199/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1199/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1199/train/explained_variance ▁▇▆▅▆▅█▇▅▆▅
wandb:        PPO_1199/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1199/train/loss ▂█▃▄▂▃▂▁▂▁▃
wandb: PPO_1199/train/policy_gradient_loss ▇▄▁▃▅█▄▃▂▅▂
wandb:                  PPO_1199/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1199/train/value_loss ▇▃▄▅█▄▁▂▃▂▄
wandb:                    global_mean_eval ▁▂▅▆▆▇▇▇██
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▂▅▆▆▆▇▇██
wandb:                       mean_reward_1 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_10 ▁▂▅▆▆▆▇▇█▇
wandb:                      mean_reward_11 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_12 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_13 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_14 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_15 ▁▂▅▆▇▆▇▇█▇
wandb:                      mean_reward_16 ▁▂▅▆▆▇▇███
wandb:                      mean_reward_17 ▁▂▅▆▆▇▇▇█▇
wandb:                      mean_reward_18 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_19 ▁▃▅▆▆▇▇▇██
wandb:                       mean_reward_2 ▁▂▅▆▆▇▇███
wandb:                      mean_reward_20 ▁▃▅▆▆▇▇▇██
wandb:                      mean_reward_21 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_22 ▁▂▅▆▆▇▇███
wandb:                      mean_reward_23 ▁▂▅▆▆▆▇███
wandb:                      mean_reward_24 ▁▂▅▆▆▇▇███
wandb:                      mean_reward_25 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_26 ▁▂▅▆▆▇▇██▇
wandb:                      mean_reward_27 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_28 ▁▂▅▆▆▆▇▇██
wandb:                      mean_reward_29 ▁▃▅▆▇▇▇▇██
wandb:                       mean_reward_3 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_30 ▁▂▅▆▆▇▇███
wandb:                      mean_reward_31 ▁▂▅▆▆▆▇███
wandb:                      mean_reward_32 ▁▂▅▆▆▆▇▇█▇
wandb:                      mean_reward_33 ▁▂▅▆▆▇▇███
wandb:                      mean_reward_34 ▁▂▅▆▆▇▇▇██
wandb:                      mean_reward_35 ▁▂▅▆▆▇▇▇█▇
wandb:                       mean_reward_4 ▁▃▅▆▆▆▇▇██
wandb:                       mean_reward_5 ▁▂▅▆▆▇▇▇██
wandb:                       mean_reward_6 ▁▂▅▆▆▆▇▇██
wandb:                       mean_reward_7 ▁▂▅▆▆▇▇▇█▇
wandb:                       mean_reward_8 ▁▂▅▆▆▇▇███
wandb:                       mean_reward_9 ▁▃▅▆▆▇▇▇██
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▁▂▁▃▂▃▄▅▆▆█
wandb:                        std_reward_0 ▃▁▁▃▄▅██▂█
wandb:                        std_reward_1 ▃▂▁▃▄▅▇█▃▆
wandb:                       std_reward_10 ▃▂▁▃▃▅▇█▂▆
wandb:                       std_reward_11 ▃▂▁▃▄▅▇█▃▇
wandb:                       std_reward_12 ▂▁▁▃▄▄▇█▂▅
wandb:                       std_reward_13 ▂▂▁▂▄▅██▃█
wandb:                       std_reward_14 ▂▁▁▃▄▄▇█▃▆
wandb:                       std_reward_15 ▃▂▁▃▄▆█▇▃█
wandb:                       std_reward_16 ▃▂▁▃▆▅█▇▃▇
wandb:                       std_reward_17 ▃▁▁▂▅▄▇█▃█
wandb:                       std_reward_18 ▃▁▁▃▅▅██▃▆
wandb:                       std_reward_19 ▃▂▁▃▄▄██▃▇
wandb:                        std_reward_2 ▃▂▁▂▄▅▅█▃▇
wandb:                       std_reward_20 ▃▂▁▃▄▅██▃▇
wandb:                       std_reward_21 ▃▁▁▃▄▃▆█▃▇
wandb:                       std_reward_22 ▃▁▁▃▅▄▇█▃▇
wandb:                       std_reward_23 ▄▂▁▃▆▆▇█▃█
wandb:                       std_reward_24 ▃▂▁▃▅▆██▄▇
wandb:                       std_reward_25 ▂▁▁▃▄▅▇█▃▆
wandb:                       std_reward_26 ▃▂▁▄▄▄██▃█
wandb:                       std_reward_27 ▃▁▁▃▄▅▇█▃▆
wandb:                       std_reward_28 ▃▁▁▃▄▅▇█▃▇
wandb:                       std_reward_29 ▃▂▁▃▄▅▇█▃█
wandb:                        std_reward_3 ▂▁▁▄▅▅▆█▃▇
wandb:                       std_reward_30 ▃▂▁▂▅▅▇█▃▇
wandb:                       std_reward_31 ▃▁▁▃▄▅█▇▃▇
wandb:                       std_reward_32 ▃▂▁▃▄▅▇▇▃█
wandb:                       std_reward_33 ▃▂▁▂▄▄▇█▃█
wandb:                       std_reward_34 ▃▂▁▃▅▅██▃█
wandb:                       std_reward_35 ▃▁▁▃▄▄▇█▃▇
wandb:                        std_reward_4 ▂▂▁▃▅▅▅█▃▇
wandb:                        std_reward_5 ▃▁▁▃▄▅██▂▆
wandb:                        std_reward_6 ▃▂▁▃▄▅██▃▇
wandb:                        std_reward_7 ▃▂▁▄▅▅██▃▇
wandb:                        std_reward_8 ▃▁▁▄▅▅▇█▃▇
wandb:                        std_reward_9 ▃▂▁▃▄▄█▇▃▆
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▂▃█▁▂▂▄▃▄▆▆
wandb:                 train/clip_fraction ▃▃▃▄▄▁▃▅▅▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▃▆▇▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▃▂▂▁▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▆▆▅█▆▅▃▄▂▁▂
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1119/global_step 212992
wandb:        PPO_1119/rollout/ep_len_mean 200.0
wandb:        PPO_1119/rollout/ep_rew_mean -801.11975
wandb:                   PPO_1119/time/fps 1191.0
wandb:            PPO_1119/train/approx_kl 0.01157
wandb:        PPO_1119/train/clip_fraction 0.15542
wandb:           PPO_1119/train/clip_range 0.2
wandb:         PPO_1119/train/entropy_loss -7.80952
wandb:   PPO_1119/train/explained_variance 0.96427
wandb:        PPO_1119/train/learning_rate 0.0003
wandb:                 PPO_1119/train/loss 44.9323
wandb: PPO_1119/train/policy_gradient_loss -0.00635
wandb:                  PPO_1119/train/std 0.7371
wandb:           PPO_1119/train/value_loss 92.43653
wandb:                PPO_1129/global_step 212992
wandb:        PPO_1129/rollout/ep_len_mean 200.0
wandb:        PPO_1129/rollout/ep_rew_mean -638.87787
wandb:                   PPO_1129/time/fps 1177.0
wandb:            PPO_1129/train/approx_kl 0.01403
wandb:        PPO_1129/train/clip_fraction 0.20406
wandb:           PPO_1129/train/clip_range 0.2
wandb:         PPO_1129/train/entropy_loss -6.92374
wandb:   PPO_1129/train/explained_variance 0.97222
wandb:        PPO_1129/train/learning_rate 0.0003
wandb:                 PPO_1129/train/loss 17.50189
wandb: PPO_1129/train/policy_gradient_loss -0.00616
wandb:                  PPO_1129/train/std 0.64982
wandb:           PPO_1129/train/value_loss 36.05498
wandb:                PPO_1139/global_step 212992
wandb:        PPO_1139/rollout/ep_len_mean 200.0
wandb:        PPO_1139/rollout/ep_rew_mean -564.62262
wandb:                   PPO_1139/time/fps 1196.0
wandb:            PPO_1139/train/approx_kl 0.01968
wandb:        PPO_1139/train/clip_fraction 0.23318
wandb:           PPO_1139/train/clip_range 0.2
wandb:         PPO_1139/train/entropy_loss -5.93181
wandb:   PPO_1139/train/explained_variance 0.97749
wandb:        PPO_1139/train/learning_rate 0.0003
wandb:                 PPO_1139/train/loss 27.73149
wandb: PPO_1139/train/policy_gradient_loss -0.00122
wandb:                  PPO_1139/train/std 0.56382
wandb:           PPO_1139/train/value_loss 34.4876
wandb:                PPO_1149/global_step 212992
wandb:        PPO_1149/rollout/ep_len_mean 200.0
wandb:        PPO_1149/rollout/ep_rew_mean -553.34308
wandb:                   PPO_1149/time/fps 1181.0
wandb:            PPO_1149/train/approx_kl 0.01598
wandb:        PPO_1149/train/clip_fraction 0.22374
wandb:           PPO_1149/train/clip_range 0.2
wandb:         PPO_1149/train/entropy_loss -5.29782
wandb:   PPO_1149/train/explained_variance 0.96823
wandb:        PPO_1149/train/learning_rate 0.0003
wandb:                 PPO_1149/train/loss 34.67154
wandb: PPO_1149/train/policy_gradient_loss -0.00249
wandb:                  PPO_1149/train/std 0.51646
wandb:           PPO_1149/train/value_loss 61.98979
wandb:                PPO_1159/global_step 212992
wandb:        PPO_1159/rollout/ep_len_mean 200.0
wandb:        PPO_1159/rollout/ep_rew_mean -506.172
wandb:                   PPO_1159/time/fps 1189.0
wandb:            PPO_1159/train/approx_kl 0.01763
wandb:        PPO_1159/train/clip_fraction 0.23568
wandb:           PPO_1159/train/clip_range 0.2
wandb:         PPO_1159/train/entropy_loss -4.67527
wandb:   PPO_1159/train/explained_variance 0.99496
wandb:        PPO_1159/train/learning_rate 0.0003
wandb:                 PPO_1159/train/loss 5.23414
wandb: PPO_1159/train/policy_gradient_loss -0.00027
wandb:                  PPO_1159/train/std 0.47207
wandb:           PPO_1159/train/value_loss 15.23505
wandb:                PPO_1169/global_step 212992
wandb:        PPO_1169/rollout/ep_len_mean 200.0
wandb:        PPO_1169/rollout/ep_rew_mean -484.87326
wandb:                   PPO_1169/time/fps 1186.0
wandb:            PPO_1169/train/approx_kl 0.01685
wandb:        PPO_1169/train/clip_fraction 0.20613
wandb:           PPO_1169/train/clip_range 0.2
wandb:         PPO_1169/train/entropy_loss -4.03637
wandb:   PPO_1169/train/explained_variance 0.99687
wandb:        PPO_1169/train/learning_rate 0.0003
wandb:                 PPO_1169/train/loss 12.61903
wandb: PPO_1169/train/policy_gradient_loss -0.0016
wandb:                  PPO_1169/train/std 0.43102
wandb:           PPO_1169/train/value_loss 29.39071
wandb:                PPO_1179/global_step 212992
wandb:        PPO_1179/rollout/ep_len_mean 200.0
wandb:        PPO_1179/rollout/ep_rew_mean -476.27716
wandb:                   PPO_1179/time/fps 1188.0
wandb:            PPO_1179/train/approx_kl 0.01908
wandb:        PPO_1179/train/clip_fraction 0.25165
wandb:           PPO_1179/train/clip_range 0.2
wandb:         PPO_1179/train/entropy_loss -3.44226
wandb:   PPO_1179/train/explained_variance 0.997
wandb:        PPO_1179/train/learning_rate 0.0003
wandb:                 PPO_1179/train/loss 2.96779
wandb: PPO_1179/train/policy_gradient_loss -7e-05
wandb:                  PPO_1179/train/std 0.39621
wandb:           PPO_1179/train/value_loss 20.61847
wandb:                PPO_1189/global_step 212992
wandb:        PPO_1189/rollout/ep_len_mean 200.0
wandb:        PPO_1189/rollout/ep_rew_mean -437.25967
wandb:                   PPO_1189/time/fps 1193.0
wandb:            PPO_1189/train/approx_kl 0.01988
wandb:        PPO_1189/train/clip_fraction 0.25717
wandb:           PPO_1189/train/clip_range 0.2
wandb:         PPO_1189/train/entropy_loss -2.95845
wandb:   PPO_1189/train/explained_variance 0.99763
wandb:        PPO_1189/train/learning_rate 0.0003
wandb:                 PPO_1189/train/loss 6.90984
wandb: PPO_1189/train/policy_gradient_loss -0.00033
wandb:                  PPO_1189/train/std 0.36918
wandb:           PPO_1189/train/value_loss 14.41674
wandb:                PPO_1199/global_step 212992
wandb:        PPO_1199/rollout/ep_len_mean 200.0
wandb:        PPO_1199/rollout/ep_rew_mean -398.87546
wandb:                   PPO_1199/time/fps 1185.0
wandb:            PPO_1199/train/approx_kl 0.02093
wandb:        PPO_1199/train/clip_fraction 0.24432
wandb:           PPO_1199/train/clip_range 0.2
wandb:         PPO_1199/train/entropy_loss -2.4954
wandb:   PPO_1199/train/explained_variance 0.99616
wandb:        PPO_1199/train/learning_rate 0.0003
wandb:                 PPO_1199/train/loss 6.66541
wandb: PPO_1199/train/policy_gradient_loss -0.0
wandb:                  PPO_1199/train/std 0.3458
wandb:           PPO_1199/train/value_loss 16.71874
wandb:                    global_mean_eval -398.05711
wandb:                         global_step 212992
wandb:                       mean_reward_0 -405.76919
wandb:                       mean_reward_1 -400.77561
wandb:                      mean_reward_10 -406.95946
wandb:                      mean_reward_11 -399.57153
wandb:                      mean_reward_12 -382.92806
wandb:                      mean_reward_13 -408.32372
wandb:                      mean_reward_14 -394.2734
wandb:                      mean_reward_15 -413.31578
wandb:                      mean_reward_16 -382.74654
wandb:                      mean_reward_17 -411.63527
wandb:                      mean_reward_18 -375.66534
wandb:                      mean_reward_19 -402.39336
wandb:                       mean_reward_2 -391.06243
wandb:                      mean_reward_20 -391.10966
wandb:                      mean_reward_21 -406.92027
wandb:                      mean_reward_22 -403.65237
wandb:                      mean_reward_23 -404.13454
wandb:                      mean_reward_24 -386.23908
wandb:                      mean_reward_25 -387.91886
wandb:                      mean_reward_26 -416.46405
wandb:                      mean_reward_27 -389.86566
wandb:                      mean_reward_28 -405.96707
wandb:                      mean_reward_29 -414.73958
wandb:                       mean_reward_3 -391.74631
wandb:                      mean_reward_30 -392.75933
wandb:                      mean_reward_31 -407.76679
wandb:                      mean_reward_32 -420.9289
wandb:                      mean_reward_33 -399.52156
wandb:                      mean_reward_34 -401.84771
wandb:                      mean_reward_35 -406.56309
wandb:                       mean_reward_4 -388.05435
wandb:                       mean_reward_5 -386.1726
wandb:                       mean_reward_6 -397.57126
wandb:                       mean_reward_7 -401.02142
wandb:                       mean_reward_8 -373.21898
wandb:                       mean_reward_9 -380.45272
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -931.94031
wandb:                        std_reward_0 139.88075
wandb:                        std_reward_1 131.12652
wandb:                       std_reward_10 121.60036
wandb:                       std_reward_11 132.17608
wandb:                       std_reward_12 111.28876
wandb:                       std_reward_13 153.91053
wandb:                       std_reward_14 121.81042
wandb:                       std_reward_15 137.15254
wandb:                       std_reward_16 125.81353
wandb:                       std_reward_17 135.01644
wandb:                       std_reward_18 113.56639
wandb:                       std_reward_19 133.0681
wandb:                        std_reward_2 135.94427
wandb:                       std_reward_20 130.32475
wandb:                       std_reward_21 137.52076
wandb:                       std_reward_22 129.25501
wandb:                       std_reward_23 133.91434
wandb:                       std_reward_24 128.06774
wandb:                       std_reward_25 112.18869
wandb:                       std_reward_26 136.7663
wandb:                       std_reward_27 123.28655
wandb:                       std_reward_28 134.15388
wandb:                       std_reward_29 141.3729
wandb:                        std_reward_3 129.91976
wandb:                       std_reward_30 124.15218
wandb:                       std_reward_31 132.46566
wandb:                       std_reward_32 159.98526
wandb:                       std_reward_33 143.87559
wandb:                       std_reward_34 134.8628
wandb:                       std_reward_35 134.99779
wandb:                        std_reward_4 132.35698
wandb:                        std_reward_5 124.38424
wandb:                        std_reward_6 126.06382
wandb:                        std_reward_7 128.57303
wandb:                        std_reward_8 121.48147
wandb:                        std_reward_9 120.7675
wandb:                            time/fps 1178.0
wandb:                     train/approx_kl 0.01244
wandb:                 train/clip_fraction 0.15652
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7411
wandb:            train/explained_variance 0.96718
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 11.08948
wandb:          train/policy_gradient_loss -0.01304
wandb:                           train/std 0.84138
wandb:                    train/value_loss 21.77692
wandb: 
wandb: Synced colorful-cherry-27: https://wandb.ai/tidiane/meta_rl_context/runs/15t9fds4
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_012622-15t9fds4/logs
wandb: 
wandb: Run history:
wandb:                PPO_1116/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1116/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1116/rollout/ep_rew_mean ▁▂▂▂▃▄▅▅▅▆▇█
wandb:                   PPO_1116/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1116/train/approx_kl █▄▁▆▄█▅▄▇▇▇
wandb:        PPO_1116/train/clip_fraction ▆▄▁▇▄▇▇▄▆██
wandb:           PPO_1116/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1116/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1116/train/explained_variance ▁▃▅█▆▄▆▆▆▃▃
wandb:        PPO_1116/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1116/train/loss ▂▇▃▁▇▄▅▂▄▅█
wandb: PPO_1116/train/policy_gradient_loss ▂▃▇▂▄▁▃█▅▃▃
wandb:                  PPO_1116/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1116/train/value_loss ▆▂▇▁▅▄▅▇███
wandb:                PPO_1125/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1125/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1125/rollout/ep_rew_mean ▂▁▃▄▄▅▆▆▇█▇█
wandb:                   PPO_1125/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1125/train/approx_kl ▅▁▃▃▃▅▆▆▇▆█
wandb:        PPO_1125/train/clip_fraction ▂▁▃▂▃▄▆▆█▅▇
wandb:           PPO_1125/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1125/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1125/train/explained_variance ▁▂▄▄▄▃▅▆▅▆█
wandb:        PPO_1125/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1125/train/loss █▇▂▅▇▅▄▁▁▁▂
wandb: PPO_1125/train/policy_gradient_loss ▅▅▅▃▄▅█▂▁▅▆
wandb:                  PPO_1125/train/std █▇▇▆▅▅▄▃▃▂▁
wandb:           PPO_1125/train/value_loss ██▄▅▆▅▃▄▁▂▄
wandb:                PPO_1136/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1136/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1136/rollout/ep_rew_mean ▁▁▂▃▄▄▆▅▅▄▅█
wandb:                   PPO_1136/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1136/train/approx_kl ▆▃▁█▁▄▅█▂▆▇
wandb:        PPO_1136/train/clip_fraction ▅▃▁▅▂▄▇▃▄█▅
wandb:           PPO_1136/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1136/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1136/train/explained_variance ▁▅▁▅▄▆▆▂▇█▂
wandb:        PPO_1136/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1136/train/loss ▂▂█▁▄▃▃▆▅▃▆
wandb: PPO_1136/train/policy_gradient_loss ▁▁▂▂▄▃▄▅█▆▄
wandb:                  PPO_1136/train/std █▇▇▆▅▅▄▄▃▂▁
wandb:           PPO_1136/train/value_loss ▅▃█▃▅▅▁▅▆▂▅
wandb:                PPO_1148/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1148/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1148/rollout/ep_rew_mean ▁▂▄▄▁▃▂▆▆▃▇█
wandb:                   PPO_1148/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1148/train/approx_kl ▆█▄▁▃▃▂▄▄▄▃
wandb:        PPO_1148/train/clip_fraction ▅█▇▁▄▄▆▆▅▄▃
wandb:           PPO_1148/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1148/train/entropy_loss ▁▂▃▃▃▄▅▆▆▇█
wandb:   PPO_1148/train/explained_variance ▇▆▅█▆██▇▇▃▁
wandb:        PPO_1148/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1148/train/loss ▄▂█▅▁▂▂▁▁▂▃
wandb: PPO_1148/train/policy_gradient_loss ▁▆▇▅▇█▇▅▆▆▇
wandb:                  PPO_1148/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1148/train/value_loss ▅▅▂▆█▅▆▁▁█▂
wandb:                PPO_1157/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1157/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1157/rollout/ep_rew_mean █▇██▇▇█▄▆▇▅▁
wandb:                   PPO_1157/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1157/train/approx_kl ▄▁▅█▂▄▅▃▄▁▃
wandb:        PPO_1157/train/clip_fraction ▅▁▆█▂▃▅▁▆▄▁
wandb:           PPO_1157/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1157/train/entropy_loss ▁▁▂▃▃▄▅▅▇▇█
wandb:   PPO_1157/train/explained_variance ▁▂▁▃▂▆▅▅▄██
wandb:        PPO_1157/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1157/train/loss █▄▄▂▃▄▃█▇▁▂
wandb: PPO_1157/train/policy_gradient_loss ▅▁▄▃▆▄▅█▆█▅
wandb:                  PPO_1157/train/std ██▇▆▆▅▄▄▂▂▁
wandb:           PPO_1157/train/value_loss ▅▇▁▂▃▇▅█▆▂▅
wandb:                PPO_1167/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1167/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1167/rollout/ep_rew_mean ▂▃▅▅▃▂▆▁▁▄█▆
wandb:                   PPO_1167/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1167/train/approx_kl ▁▁▅▆▁▄▄█▆▅▃
wandb:        PPO_1167/train/clip_fraction ▁▁▅▆▄▅▃█▅▃▆
wandb:           PPO_1167/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1167/train/entropy_loss ▁▂▂▃▃▄▄▅▆▆█
wandb:   PPO_1167/train/explained_variance ▁▂▆▆▃▇▆▆▆▄█
wandb:        PPO_1167/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1167/train/loss ▅█▃▂▃▄▃▁▃▁▂
wandb: PPO_1167/train/policy_gradient_loss ▃▅▃▇▆▇▇█▅▁▇
wandb:                  PPO_1167/train/std █▇▆▆▆▅▅▄▃▃▁
wandb:           PPO_1167/train/value_loss ▆█▃▂▄▃▄▅▄▄▁
wandb:                PPO_1177/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1177/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1177/rollout/ep_rew_mean ▁▇▇▆▅█▆▇▆▇▇▇
wandb:                   PPO_1177/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1177/train/approx_kl ▃▃▂▇▆▂▁█▇▂▂
wandb:        PPO_1177/train/clip_fraction ▃▇▃▆▄▇▁█▄▇▂
wandb:           PPO_1177/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1177/train/entropy_loss ▁▁▂▃▄▅▅▅▆▇█
wandb:   PPO_1177/train/explained_variance ▃▄▃▅▂▂▂▁▃▄█
wandb:        PPO_1177/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1177/train/loss ▂▁▁▁▄█▃▂▃▁▂
wandb: PPO_1177/train/policy_gradient_loss ▁▄▄▆▁█▄▆▆▇▆
wandb:                  PPO_1177/train/std █▇▇▆▄▄▄▄▂▁▁
wandb:           PPO_1177/train/value_loss ▂▁▄▂▆▆█▆▆▂▃
wandb:                PPO_1187/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1187/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1187/rollout/ep_rew_mean ▂▅▃▅▄▄▆▃▅▁▅█
wandb:                   PPO_1187/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1187/train/approx_kl ▄▃▁▅▅█▅█▁▄▂
wandb:        PPO_1187/train/clip_fraction ▄█▁▃▂▆▅▆▄▁▄
wandb:           PPO_1187/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1187/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1187/train/explained_variance ▇▆▅▇▅▁▇▆██▅
wandb:        PPO_1187/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1187/train/loss ▇▅█▇▆▆▅▁▃▃▃
wandb: PPO_1187/train/policy_gradient_loss ▃▄▄▁▅█▄▃▆▇▇
wandb:                  PPO_1187/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1187/train/value_loss ▇▁█▆██▂▅▄▃▄
wandb:                PPO_1198/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1198/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1198/rollout/ep_rew_mean ▆▆▆▄▆▆▆▆▁▅█▆
wandb:                   PPO_1198/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1198/train/approx_kl ▁▃▅▃█▄▆▆▃▅▆
wandb:        PPO_1198/train/clip_fraction ▄▄▅▁█▄▃▆▂▅▅
wandb:           PPO_1198/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1198/train/entropy_loss ▁▂▃▃▄▅▅▆▆▆█
wandb:   PPO_1198/train/explained_variance ▁▃▂▅▅▂▆█▁▆▄
wandb:        PPO_1198/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1198/train/loss ▆▁▂▃▁▃▁▁█▂▃
wandb: PPO_1198/train/policy_gradient_loss ▅▆▅▁█▄▄▇▃▆▂
wandb:                  PPO_1198/train/std █▇▆▆▅▄▄▃▃▃▁
wandb:           PPO_1198/train/value_loss ▃▂▂▅▁▂▃▂█▄▃
wandb:                    global_mean_eval ▁▃▄▇▇▆▇███
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▃▄▇▇▆▆▇██
wandb:                       mean_reward_1 ▁▃▄▇▇▆▇▇██
wandb:                      mean_reward_10 ▁▃▄▇▇▆▇███
wandb:                      mean_reward_11 ▁▃▄▇▇▆▆▇██
wandb:                      mean_reward_12 ▁▃▃▇▆▆▇▇██
wandb:                      mean_reward_13 ▁▃▄▇▇▆▇▇██
wandb:                      mean_reward_14 ▁▃▄▇▇▆▆███
wandb:                      mean_reward_15 ▁▃▄▇▇▆▇███
wandb:                      mean_reward_16 ▁▃▄▇▇▆▇███
wandb:                      mean_reward_17 ▁▃▃▇▇▆▆███
wandb:                      mean_reward_18 ▁▃▄▇▇▇▇███
wandb:                      mean_reward_19 ▁▃▃▇▇▆▆███
wandb:                       mean_reward_2 ▁▃▄▇▇▆▇███
wandb:                      mean_reward_20 ▁▃▄▇▇▆▇███
wandb:                      mean_reward_21 ▁▃▄▇▇▇▆███
wandb:                      mean_reward_22 ▁▃▄▇▇▆▇███
wandb:                      mean_reward_23 ▁▃▄▇▆▆▆███
wandb:                      mean_reward_24 ▁▃▄▇▇▆▆███
wandb:                      mean_reward_25 ▁▃▃▇▆▆▆▇██
wandb:                      mean_reward_26 ▁▃▃▇▇▆▆███
wandb:                      mean_reward_27 ▁▃▃▇▇▆▆███
wandb:                      mean_reward_28 ▁▃▄▇▇▆▇▇██
wandb:                      mean_reward_29 ▁▃▃▇▇▇▇▇██
wandb:                       mean_reward_3 ▁▃▄▇▇▇▇███
wandb:                      mean_reward_30 ▁▃▃▇▇▆▇▇██
wandb:                      mean_reward_31 ▁▃▃▇▇▆▇███
wandb:                      mean_reward_32 ▁▃▃▇▇▇▆▇██
wandb:                      mean_reward_33 ▁▃▄▇▇▆▇███
wandb:                      mean_reward_34 ▁▃▃▇▇▆▆███
wandb:                      mean_reward_35 ▁▃▄▇▇▇▇███
wandb:                       mean_reward_4 ▁▃▃▇▇▆▆███
wandb:                       mean_reward_5 ▁▃▃▇▇▅▇▇██
wandb:                       mean_reward_6 ▁▃▃▇▇▆▆▇██
wandb:                       mean_reward_7 ▁▃▄▇▇▆▇███
wandb:                       mean_reward_8 ▁▃▃▇▇▆▇███
wandb:                       mean_reward_9 ▁▃▄▇▇▆▇███
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▁▂▁▃▂▃▄▅▆▆█
wandb:                        std_reward_0 ▆▂▃▅▁█▅▃▃▃
wandb:                        std_reward_1 ▆▂▃▆▁█▅▃▃▄
wandb:                       std_reward_10 ▆▃▃▆▁█▄▃▃▄
wandb:                       std_reward_11 ▆▂▃▅▁█▅▃▃▃
wandb:                       std_reward_12 ▇▃▄▇▁█▆▄▄▄
wandb:                       std_reward_13 ▆▂▃▆▁█▅▄▃▃
wandb:                       std_reward_14 ▇▂▃▅▁█▄▄▃▃
wandb:                       std_reward_15 ▆▂▃▅▁█▄▄▃▄
wandb:                       std_reward_16 ▆▂▃▆▁█▅▄▄▃
wandb:                       std_reward_17 ▇▂▄▆▁█▅▄▃▃
wandb:                       std_reward_18 █▂▄▆▁█▄▄▃▄
wandb:                       std_reward_19 ▆▂▃▆▁█▅▃▃▄
wandb:                        std_reward_2 ▆▂▃▅▁█▄▄▃▃
wandb:                       std_reward_20 ▆▂▃▆▁█▅▃▃▅
wandb:                       std_reward_21 ▆▂▃▅▁█▅▃▃▄
wandb:                       std_reward_22 ▆▃▃▅▁█▅▃▂▄
wandb:                       std_reward_23 ▆▂▄▇▁█▆▄▄▅
wandb:                       std_reward_24 ▅▂▄▆▁█▆▄▃▄
wandb:                       std_reward_25 ▆▂▃▄▁█▅▃▃▄
wandb:                       std_reward_26 ▆▂▃▅▁█▅▃▃▃
wandb:                       std_reward_27 ▅▂▂▅▁█▅▃▃▃
wandb:                       std_reward_28 ▆▂▃▅▁█▅▃▃▄
wandb:                       std_reward_29 ▇▂▃▆▁█▅▃▃▅
wandb:                        std_reward_3 ▇▂▃▅▁█▅▄▃▄
wandb:                       std_reward_30 ▇▂▃▅▁█▄▃▄▄
wandb:                       std_reward_31 ▅▃▂▆▁█▅▃▃▃
wandb:                       std_reward_32 ▆▂▄▆▁█▄▃▃▃
wandb:                       std_reward_33 ▆▂▃▅▁█▄▃▃▃
wandb:                       std_reward_34 ▇▂▃▆▁█▅▃▃▄
wandb:                       std_reward_35 ▆▃▃▅▁█▅▄▂▄
wandb:                        std_reward_4 ▅▂▃▅▁█▅▃▃▄
wandb:                        std_reward_5 ▆▂▃▅▁█▄▃▃▃
wandb:                        std_reward_6 ▆▂▃▅▁█▄▃▃▄
wandb:                        std_reward_7 ▇▂▄▆▁█▅▄▄▅
wandb:                        std_reward_8 ▆▂▃▅▁█▅▃▃▃
wandb:                        std_reward_9 ▆▂▃▆▁█▆▄▂▄
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▂▃█▁▂▂▄▃▄▆▆
wandb:                 train/clip_fraction ▃▃▃▄▄▁▃▅▅▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▃▆▇▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▃▂▂▁▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▆▆▅█▆▅▃▄▂▁▂
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1116/global_step 212992
wandb:        PPO_1116/rollout/ep_len_mean 200.0
wandb:        PPO_1116/rollout/ep_rew_mean -764.61316
wandb:                   PPO_1116/time/fps 1180.0
wandb:            PPO_1116/train/approx_kl 0.01155
wandb:        PPO_1116/train/clip_fraction 0.15005
wandb:           PPO_1116/train/clip_range 0.2
wandb:         PPO_1116/train/entropy_loss -7.76708
wandb:   PPO_1116/train/explained_variance 0.95912
wandb:        PPO_1116/train/learning_rate 0.0003
wandb:                 PPO_1116/train/loss 52.23923
wandb: PPO_1116/train/policy_gradient_loss -0.0082
wandb:                  PPO_1116/train/std 0.73328
wandb:           PPO_1116/train/value_loss 92.09692
wandb:                PPO_1125/global_step 212992
wandb:        PPO_1125/rollout/ep_len_mean 200.0
wandb:        PPO_1125/rollout/ep_rew_mean -634.38531
wandb:                   PPO_1125/time/fps 1176.0
wandb:            PPO_1125/train/approx_kl 0.01532
wandb:        PPO_1125/train/clip_fraction 0.1925
wandb:           PPO_1125/train/clip_range 0.2
wandb:         PPO_1125/train/entropy_loss -6.77512
wandb:   PPO_1125/train/explained_variance 0.98027
wandb:        PPO_1125/train/learning_rate 0.0003
wandb:                 PPO_1125/train/loss 23.81393
wandb: PPO_1125/train/policy_gradient_loss -0.00674
wandb:                  PPO_1125/train/std 0.63537
wandb:           PPO_1125/train/value_loss 54.41357
wandb:                PPO_1136/global_step 212992
wandb:        PPO_1136/rollout/ep_len_mean 200.0
wandb:        PPO_1136/rollout/ep_rew_mean -544.76105
wandb:                   PPO_1136/time/fps 1171.0
wandb:            PPO_1136/train/approx_kl 0.01766
wandb:        PPO_1136/train/clip_fraction 0.21356
wandb:           PPO_1136/train/clip_range 0.2
wandb:         PPO_1136/train/entropy_loss -5.78952
wandb:   PPO_1136/train/explained_variance 0.97544
wandb:        PPO_1136/train/learning_rate 0.0003
wandb:                 PPO_1136/train/loss 16.48898
wandb: PPO_1136/train/policy_gradient_loss -0.00341
wandb:                  PPO_1136/train/std 0.55308
wandb:           PPO_1136/train/value_loss 29.82028
wandb:                PPO_1148/global_step 212992
wandb:        PPO_1148/rollout/ep_len_mean 200.0
wandb:        PPO_1148/rollout/ep_rew_mean -500.98575
wandb:                   PPO_1148/time/fps 1170.0
wandb:            PPO_1148/train/approx_kl 0.01569
wandb:        PPO_1148/train/clip_fraction 0.19767
wandb:           PPO_1148/train/clip_range 0.2
wandb:         PPO_1148/train/entropy_loss -5.00553
wandb:   PPO_1148/train/explained_variance 0.94942
wandb:        PPO_1148/train/learning_rate 0.0003
wandb:                 PPO_1148/train/loss 13.44773
wandb: PPO_1148/train/policy_gradient_loss -0.00053
wandb:                  PPO_1148/train/std 0.49403
wandb:           PPO_1148/train/value_loss 24.53321
wandb:                PPO_1157/global_step 212992
wandb:        PPO_1157/rollout/ep_len_mean 200.0
wandb:        PPO_1157/rollout/ep_rew_mean -539.59314
wandb:                   PPO_1157/time/fps 1168.0
wandb:            PPO_1157/train/approx_kl 0.01602
wandb:        PPO_1157/train/clip_fraction 0.19053
wandb:           PPO_1157/train/clip_range 0.2
wandb:         PPO_1157/train/entropy_loss -4.45727
wandb:   PPO_1157/train/explained_variance 0.98847
wandb:        PPO_1157/train/learning_rate 0.0003
wandb:                 PPO_1157/train/loss 3.81125
wandb: PPO_1157/train/policy_gradient_loss -0.00143
wandb:                  PPO_1157/train/std 0.45741
wandb:           PPO_1157/train/value_loss 21.81461
wandb:                PPO_1167/global_step 212992
wandb:        PPO_1167/rollout/ep_len_mean 200.0
wandb:        PPO_1167/rollout/ep_rew_mean -502.89282
wandb:                   PPO_1167/time/fps 1164.0
wandb:            PPO_1167/train/approx_kl 0.01644
wandb:        PPO_1167/train/clip_fraction 0.23008
wandb:           PPO_1167/train/clip_range 0.2
wandb:         PPO_1167/train/entropy_loss -3.94667
wandb:   PPO_1167/train/explained_variance 0.99489
wandb:        PPO_1167/train/learning_rate 0.0003
wandb:                 PPO_1167/train/loss 7.59322
wandb: PPO_1167/train/policy_gradient_loss -0.00033
wandb:                  PPO_1167/train/std 0.42441
wandb:           PPO_1167/train/value_loss 15.52127
wandb:                PPO_1177/global_step 212992
wandb:        PPO_1177/rollout/ep_len_mean 200.0
wandb:        PPO_1177/rollout/ep_rew_mean -483.66675
wandb:                   PPO_1177/time/fps 1166.0
wandb:            PPO_1177/train/approx_kl 0.01694
wandb:        PPO_1177/train/clip_fraction 0.21706
wandb:           PPO_1177/train/clip_range 0.2
wandb:         PPO_1177/train/entropy_loss -3.51779
wandb:   PPO_1177/train/explained_variance 0.99631
wandb:        PPO_1177/train/learning_rate 0.0003
wandb:                 PPO_1177/train/loss 6.71561
wandb: PPO_1177/train/policy_gradient_loss 5e-05
wandb:                  PPO_1177/train/std 0.40126
wandb:           PPO_1177/train/value_loss 18.40078
wandb:                PPO_1187/global_step 212992
wandb:        PPO_1187/rollout/ep_len_mean 200.0
wandb:        PPO_1187/rollout/ep_rew_mean -456.53333
wandb:                   PPO_1187/time/fps 1163.0
wandb:            PPO_1187/train/approx_kl 0.01822
wandb:        PPO_1187/train/clip_fraction 0.24057
wandb:           PPO_1187/train/clip_range 0.2
wandb:         PPO_1187/train/entropy_loss -2.99241
wandb:   PPO_1187/train/explained_variance 0.99206
wandb:        PPO_1187/train/learning_rate 0.0003
wandb:                 PPO_1187/train/loss 3.57618
wandb: PPO_1187/train/policy_gradient_loss 0.00091
wandb:                  PPO_1187/train/std 0.37118
wandb:           PPO_1187/train/value_loss 16.14437
wandb:                PPO_1198/global_step 212992
wandb:        PPO_1198/rollout/ep_len_mean 200.0
wandb:        PPO_1198/rollout/ep_rew_mean -454.65833
wandb:                   PPO_1198/time/fps 1162.0
wandb:            PPO_1198/train/approx_kl 0.02077
wandb:        PPO_1198/train/clip_fraction 0.25457
wandb:           PPO_1198/train/clip_range 0.2
wandb:         PPO_1198/train/entropy_loss -2.49996
wandb:   PPO_1198/train/explained_variance 0.99545
wandb:        PPO_1198/train/learning_rate 0.0003
wandb:                 PPO_1198/train/loss 6.32369
wandb: PPO_1198/train/policy_gradient_loss -0.00086
wandb:                  PPO_1198/train/std 0.34569
wandb:           PPO_1198/train/value_loss 14.57812
wandb:                    global_mean_eval -427.16949
wandb:                         global_step 212992
wandb:                       mean_reward_0 -426.12243
wandb:                       mean_reward_1 -426.36965
wandb:                      mean_reward_10 -426.67317
wandb:                      mean_reward_11 -421.48387
wandb:                      mean_reward_12 -425.75681
wandb:                      mean_reward_13 -420.84064
wandb:                      mean_reward_14 -430.80794
wandb:                      mean_reward_15 -424.79321
wandb:                      mean_reward_16 -423.89092
wandb:                      mean_reward_17 -420.49006
wandb:                      mean_reward_18 -422.34557
wandb:                      mean_reward_19 -427.62107
wandb:                       mean_reward_2 -429.93496
wandb:                      mean_reward_20 -436.12833
wandb:                      mean_reward_21 -431.00998
wandb:                      mean_reward_22 -426.60929
wandb:                      mean_reward_23 -424.94012
wandb:                      mean_reward_24 -433.38387
wandb:                      mean_reward_25 -425.09866
wandb:                      mean_reward_26 -433.30771
wandb:                      mean_reward_27 -419.32656
wandb:                      mean_reward_28 -426.37329
wandb:                      mean_reward_29 -430.62373
wandb:                       mean_reward_3 -429.58605
wandb:                      mean_reward_30 -434.37606
wandb:                      mean_reward_31 -422.69081
wandb:                      mean_reward_32 -419.74876
wandb:                      mean_reward_33 -431.56773
wandb:                      mean_reward_34 -426.06605
wandb:                      mean_reward_35 -433.28122
wandb:                       mean_reward_4 -435.79192
wandb:                       mean_reward_5 -429.3578
wandb:                       mean_reward_6 -429.05185
wandb:                       mean_reward_7 -426.52131
wandb:                       mean_reward_8 -423.55908
wandb:                       mean_reward_9 -422.57114
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -931.94031
wandb:                        std_reward_0 42.32701
wandb:                        std_reward_1 51.77243
wandb:                       std_reward_10 50.26745
wandb:                       std_reward_11 47.85057
wandb:                       std_reward_12 45.83664
wandb:                       std_reward_13 40.29571
wandb:                       std_reward_14 45.46179
wandb:                       std_reward_15 48.00129
wandb:                       std_reward_16 46.45779
wandb:                       std_reward_17 43.37865
wandb:                       std_reward_18 49.95932
wandb:                       std_reward_19 50.61582
wandb:                        std_reward_2 47.68979
wandb:                       std_reward_20 59.0702
wandb:                       std_reward_21 55.2789
wandb:                       std_reward_22 54.90933
wandb:                       std_reward_23 54.684
wandb:                       std_reward_24 51.9567
wandb:                       std_reward_25 54.20555
wandb:                       std_reward_26 46.74819
wandb:                       std_reward_27 43.84924
wandb:                       std_reward_28 51.20647
wandb:                       std_reward_29 56.97984
wandb:                        std_reward_3 50.62087
wandb:                       std_reward_30 51.50303
wandb:                       std_reward_31 45.44872
wandb:                       std_reward_32 42.3609
wandb:                       std_reward_33 47.81331
wandb:                       std_reward_34 52.32255
wandb:                       std_reward_35 49.34264
wandb:                        std_reward_4 52.33741
wandb:                        std_reward_5 42.3453
wandb:                        std_reward_6 52.36154
wandb:                        std_reward_7 53.1219
wandb:                        std_reward_8 46.58897
wandb:                        std_reward_9 52.69047
wandb:                            time/fps 1178.0
wandb:                     train/approx_kl 0.01244
wandb:                 train/clip_fraction 0.15652
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7411
wandb:            train/explained_variance 0.96718
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 11.08948
wandb:          train/policy_gradient_loss -0.01304
wandb:                           train/std 0.84138
wandb:                    train/value_loss 21.77692
wandb: 
wandb: Synced spring-surf-24: https://wandb.ai/tidiane/meta_rl_context/runs/1jxiownk
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_012622-1jxiownk/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1121/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1121/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1121/rollout/ep_rew_mean ▁▁▂▃▃▄▅▆▇█▇▇
wandb:                   PPO_1121/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1121/train/approx_kl ▁▅█▆▆▆▃▇▅▃▆
wandb:        PPO_1121/train/clip_fraction ▁▂▇▆▄▆▄▇▆▄█
wandb:           PPO_1121/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1121/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1121/train/explained_variance █▁▇█▇▇▇▇▆▄▅
wandb:        PPO_1121/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1121/train/loss ▁▂▁▃▂▁▂▃▁█▁
wandb: PPO_1121/train/policy_gradient_loss ██▁▄▄▅▆▇▆█▅
wandb:                  PPO_1121/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1121/train/value_loss ▁▄▂▃▅▃▅▄▄█▄
wandb:                PPO_1131/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1131/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1131/rollout/ep_rew_mean ▁▁▁▂▁▃▂▄▅▆▇█
wandb:                   PPO_1131/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1131/train/approx_kl ▁▃▁▁▅▁▆▆█▄▆
wandb:        PPO_1131/train/clip_fraction ▁▄▁▄▆▄▅██▆▇
wandb:           PPO_1131/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1131/train/entropy_loss ▁▂▂▃▃▄▄▅▆▇█
wandb:   PPO_1131/train/explained_variance ▁▄▄▃▃▃▇█▃▇▆
wandb:        PPO_1131/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1131/train/loss ▂▃▄▃▁▃▃▂▂▂█
wandb: PPO_1131/train/policy_gradient_loss ▆▄█▆▆▅▁▄▅▅▆
wandb:                  PPO_1131/train/std █▇▇▆▆▅▅▄▃▂▁
wandb:           PPO_1131/train/value_loss █▇▇▃▁▂▄▁▄▃▃
wandb:                PPO_1140/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1140/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1140/rollout/ep_rew_mean ▁▁▂▂▃▄▃▄▅▆▇█
wandb:                   PPO_1140/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1140/train/approx_kl ▁▅▃▄█▅█▃█▆▂
wandb:        PPO_1140/train/clip_fraction ▁▄▃▄▂▃█▃▅▄▇
wandb:           PPO_1140/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1140/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1140/train/explained_variance ▅▅▇▇▅▅█▇▁▄▇
wandb:        PPO_1140/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1140/train/loss ▂▁▆▁▇▄▃▁▄▅█
wandb: PPO_1140/train/policy_gradient_loss ▃▂▄▃▁▃▄▄▅▇█
wandb:                  PPO_1140/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1140/train/value_loss ▄▂▃▄▅▆▁▃▆█▂
wandb:                PPO_1150/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1150/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1150/rollout/ep_rew_mean ▄▄▁▂▄▂▄▅█▃▇▄
wandb:                   PPO_1150/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1150/train/approx_kl ▇█▁▇▄▇▇▇▇▁▅
wandb:        PPO_1150/train/clip_fraction ▆▇▁▇▆▃▆█▇▁▆
wandb:           PPO_1150/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1150/train/entropy_loss ▁▂▁▂▂▃▄▅▆▆█
wandb:   PPO_1150/train/explained_variance ▃▅▁▃▄▆▆▇█▇█
wandb:        PPO_1150/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1150/train/loss ▃▂█▅▁█▇▅▂▃▇
wandb: PPO_1150/train/policy_gradient_loss ▁▇▇▃█▇▅▃▄▄▇
wandb:                  PPO_1150/train/std ███▇▆▆▅▄▃▃▁
wandb:           PPO_1150/train/value_loss ▂▃█▃▆█▇▃▁▇▄
wandb:                PPO_1160/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1160/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1160/rollout/ep_rew_mean ▇▆▅▁▁█▆▇▃▆██
wandb:                   PPO_1160/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1160/train/approx_kl ▂▂▂▁▂▅▇▄▁▅█
wandb:        PPO_1160/train/clip_fraction ▃▃▃▁▂▅▆▄▂▃█
wandb:           PPO_1160/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1160/train/entropy_loss ▁▂▂▂▃▄▄▅▅▆█
wandb:   PPO_1160/train/explained_variance ▁▂▁▅▇▅██▇▂▆
wandb:        PPO_1160/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1160/train/loss ▅▄▂▅▅▄▂▁▄▅█
wandb: PPO_1160/train/policy_gradient_loss ▁▅▂▁▂▄▅▁▃▄█
wandb:                  PPO_1160/train/std █▇▇▇▆▅▅▄▄▃▁
wandb:           PPO_1160/train/value_loss ▃▅▅█▅▃▁▃▆▅▂
wandb:                PPO_1170/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1170/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1170/rollout/ep_rew_mean ▅▃▁▅▄▃▁▄▅▅▄█
wandb:                   PPO_1170/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1170/train/approx_kl ▄▁▁█▁▅▂▆▃█▅
wandb:        PPO_1170/train/clip_fraction ▆▁▃█▂▂▄▃▄▆▆
wandb:           PPO_1170/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1170/train/entropy_loss ▁▁▂▃▄▄▅▆▆▇█
wandb:   PPO_1170/train/explained_variance █▇█▁▇█▇▇▆▃▇
wandb:        PPO_1170/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1170/train/loss ▂▇▃▁▃█▆▄▂▃▂
wandb: PPO_1170/train/policy_gradient_loss █▂▄▆▁▅▇▃▁▅▄
wandb:                  PPO_1170/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1170/train/value_loss ▂▇▄▃▃▅▃▃█▇▁
wandb:                PPO_1180/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1180/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1180/rollout/ep_rew_mean ▄▁▃▄▃▆▇█▅▂▆▃
wandb:                   PPO_1180/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1180/train/approx_kl ▃▃▆▄▄▄▄▁█▄▇
wandb:        PPO_1180/train/clip_fraction ▁▁▅▇▂█▂▃▆▄▆
wandb:           PPO_1180/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1180/train/entropy_loss ▁▂▃▃▃▄▅▆▇▇█
wandb:   PPO_1180/train/explained_variance ▅▆▄▂▁█▁▁▃▅▃
wandb:        PPO_1180/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1180/train/loss ▂▃▂▁█▁▃▁▆▂▅
wandb: PPO_1180/train/policy_gradient_loss ▂▂▇█▄█▄▆▆▁▆
wandb:                  PPO_1180/train/std █▇▆▆▆▅▄▃▂▂▁
wandb:           PPO_1180/train/value_loss ▄▆▆▇█▁▃▃▅█▇
wandb:                PPO_1190/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1190/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1190/rollout/ep_rew_mean ▆▆▁▃▅▄█▃▅▄▇▇
wandb:                   PPO_1190/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1190/train/approx_kl ▂▁▂█▃█▁▅▅▅█
wandb:        PPO_1190/train/clip_fraction ▆▁▄█▄▄▇▄▅▄█
wandb:           PPO_1190/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1190/train/entropy_loss ▁▂▂▃▄▄▅▆▇▇█
wandb:   PPO_1190/train/explained_variance ▅▁▄▇▂▆█▄▄▆▄
wandb:        PPO_1190/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1190/train/loss ▂▃█▃▃▅▂▁▁▂▂
wandb: PPO_1190/train/policy_gradient_loss ▅▂▆█▅▃▅▆▃▁█
wandb:                  PPO_1190/train/std █▇▆▆▅▅▄▃▂▂▁
wandb:           PPO_1190/train/value_loss ▃▇▇█▅▆▁▂▄▆▄
wandb:                PPO_1200/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1200/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1200/rollout/ep_rew_mean ▅▅▆▅▆▃▂▁█▇█▅
wandb:                   PPO_1200/time/fps █▂▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1200/train/approx_kl ▆▅▁▃▄▅▆▄█▇▄
wandb:        PPO_1200/train/clip_fraction ▅▅▂▂▃▅█▁▅▄▃
wandb:           PPO_1200/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1200/train/entropy_loss ▁▂▃▃▄▅▅▆▇██
wandb:   PPO_1200/train/explained_variance ▁▇█▇▆▇█▇▄▆▇
wandb:        PPO_1200/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1200/train/loss ▄▅▄▁▅█▂▃▂▁▁
wandb: PPO_1200/train/policy_gradient_loss ▂█▅▆▅▆▆▁▅▃▆
wandb:                  PPO_1200/train/std █▇▆▆▅▄▃▃▂▁▁
wandb:           PPO_1200/train/value_loss ▇▃▁▃▂▂▅█▄▅▄
wandb:                    global_mean_eval ▁▂▄▆▅▆▇▇▇█
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▂▄▆▆▆▇▇██
wandb:                       mean_reward_1 ▁▂▄▆▆▆▇▇▇█
wandb:                      mean_reward_10 ▁▂▄▆▄▆▇█▇█
wandb:                      mean_reward_11 ▁▂▄▆▄▆▇▇▇█
wandb:                      mean_reward_12 ▁▂▄▆▅▆▇▇▇█
wandb:                      mean_reward_13 ▁▂▄▆▄▆▇▇▇█
wandb:                      mean_reward_14 ▁▂▅▇▆▆████
wandb:                      mean_reward_15 ▁▂▄▆▆▆▇███
wandb:                      mean_reward_16 ▁▂▄▆▅▆▇███
wandb:                      mean_reward_17 ▁▂▄▅▅▆▇▇██
wandb:                      mean_reward_18 ▁▂▄▆▅▆████
wandb:                      mean_reward_19 ▁▂▅▆▆▇▇███
wandb:                       mean_reward_2 ▁▂▄▆▅▇████
wandb:                      mean_reward_20 ▁▂▄▆▄▆▇▇▇█
wandb:                      mean_reward_21 ▁▂▄▆▅▇▇▇██
wandb:                      mean_reward_22 ▁▂▄▇▅▆▇█▇█
wandb:                      mean_reward_23 ▁▂▄▆▅▆▇▇▇█
wandb:                      mean_reward_24 ▁▂▄▆▅▇▇▆▇█
wandb:                      mean_reward_25 ▁▃▄▆▆▇▇█▇█
wandb:                      mean_reward_26 ▁▂▄▆▆▆▇▇▇█
wandb:                      mean_reward_27 ▁▃▅▆▅▇▇███
wandb:                      mean_reward_28 ▁▂▄▇▅▆▇█▇█
wandb:                      mean_reward_29 ▁▂▄▆▅▆▇███
wandb:                       mean_reward_3 ▁▂▄▆▅▇▇▇██
wandb:                      mean_reward_30 ▁▂▅▆▅▇▇▇██
wandb:                      mean_reward_31 ▁▂▄▆▅▅▇▇▇█
wandb:                      mean_reward_32 ▁▂▄▆▅▆▇▇▇█
wandb:                      mean_reward_33 ▁▂▅▆▆▆▇█▇█
wandb:                      mean_reward_34 ▁▂▅▆▆▆████
wandb:                      mean_reward_35 ▁▃▅▆▅▇████
wandb:                       mean_reward_4 ▁▂▄▆▅▇████
wandb:                       mean_reward_5 ▁▂▄▆▄▆▇▇▇█
wandb:                       mean_reward_6 ▁▂▅▆▅▇█▇██
wandb:                       mean_reward_7 ▁▂▄▆▅▆▇▇▇█
wandb:                       mean_reward_8 ▁▂▄▆▅▆▇███
wandb:                       mean_reward_9 ▁▂▄▆▅▆▇▇▇█
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▁▂▁▃▂▃▄▅▆▆█
wandb:                        std_reward_0 ▂▁▁▃█▅▃▄▃▄
wandb:                        std_reward_1 ▂▁▁▄█▅▄▃▄▄
wandb:                       std_reward_10 ▂▁▁▃█▄▃▃▄▃
wandb:                       std_reward_11 ▂▁▁▃█▅▃▃▃▃
wandb:                       std_reward_12 ▂▁▁▃█▅▃▃▃▃
wandb:                       std_reward_13 ▂▁▁▃█▅▂▃▃▃
wandb:                       std_reward_14 ▂▁▁▃█▆▃▃▄▄
wandb:                       std_reward_15 ▂▁▁▃█▅▃▃▃▄
wandb:                       std_reward_16 ▂▁▁▃█▆▃▃▃▃
wandb:                       std_reward_17 ▂▁▁▃█▅▃▃▃▄
wandb:                       std_reward_18 ▂▁▁▄█▅▃▃▃▄
wandb:                       std_reward_19 ▂▁▁▃█▆▄▃▄▄
wandb:                        std_reward_2 ▂▁▁▃█▅▃▃▄▄
wandb:                       std_reward_20 ▂▁▁▃█▅▃▃▃▃
wandb:                       std_reward_21 ▂▁▁▃█▄▃▃▄▃
wandb:                       std_reward_22 ▂▁▁▃█▅▃▄▄▃
wandb:                       std_reward_23 ▂▁▁▄█▆▃▄▄▄
wandb:                       std_reward_24 ▂▁▁▃█▅▃▄▄▃
wandb:                       std_reward_25 ▁▁▁▃█▄▃▃▃▃
wandb:                       std_reward_26 ▂▁▁▃█▅▃▄▃▃
wandb:                       std_reward_27 ▂▁▁▄█▅▃▃▃▄
wandb:                       std_reward_28 ▂▁▁▃█▆▃▃▄▄
wandb:                       std_reward_29 ▂▁▁▃█▆▃▃▄▃
wandb:                        std_reward_3 ▂▁▁▃█▄▃▃▃▃
wandb:                       std_reward_30 ▂▁▁▃█▅▃▄▃▄
wandb:                       std_reward_31 ▂▁▁▃█▆▃▄▄▃
wandb:                       std_reward_32 ▂▁▁▃█▅▃▃▄▃
wandb:                       std_reward_33 ▂▁▁▃█▆▃▃▄▄
wandb:                       std_reward_34 ▂▁▂▃█▆▃▃▄▄
wandb:                       std_reward_35 ▂▁▁▃█▅▃▄▃▄
wandb:                        std_reward_4 ▂▁▁▃█▄▃▃▄▄
wandb:                        std_reward_5 ▁▁▁▃█▅▃▃▃▃
wandb:                        std_reward_6 ▂▁▁▃█▅▂▃▃▄
wandb:                        std_reward_7 ▂▁▁▃█▅▃▃▄▃
wandb:                        std_reward_8 ▂▁▁▃█▅▃▃▄▄
wandb:                        std_reward_9 ▁▁▁▃█▆▃▃▄▃
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▂▃█▁▂▂▄▃▄▆▆
wandb:                 train/clip_fraction ▃▃▃▄▄▁▃▅▅▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▃▆▇▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▃▂▂▁▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▆▆▅█▆▅▃▄▂▁▂
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1121/global_step 212992
wandb:        PPO_1121/rollout/ep_len_mean 200.0
wandb:        PPO_1121/rollout/ep_rew_mean -814.53625
wandb:                   PPO_1121/time/fps 1177.0
wandb:            PPO_1121/train/approx_kl 0.01277
wandb:        PPO_1121/train/clip_fraction 0.17025
wandb:           PPO_1121/train/clip_range 0.2
wandb:         PPO_1121/train/entropy_loss -7.71907
wandb:   PPO_1121/train/explained_variance 0.95801
wandb:        PPO_1121/train/learning_rate 0.0003
wandb:                 PPO_1121/train/loss 17.68281
wandb: PPO_1121/train/policy_gradient_loss -0.00888
wandb:                  PPO_1121/train/std 0.72821
wandb:           PPO_1121/train/value_loss 85.28061
wandb:                PPO_1131/global_step 212992
wandb:        PPO_1131/rollout/ep_len_mean 200.0
wandb:        PPO_1131/rollout/ep_rew_mean -729.67847
wandb:                   PPO_1131/time/fps 1167.0
wandb:            PPO_1131/train/approx_kl 0.01495
wandb:        PPO_1131/train/clip_fraction 0.19674
wandb:           PPO_1131/train/clip_range 0.2
wandb:         PPO_1131/train/entropy_loss -6.82301
wandb:   PPO_1131/train/explained_variance 0.96536
wandb:        PPO_1131/train/learning_rate 0.0003
wandb:                 PPO_1131/train/loss 54.72609
wandb: PPO_1131/train/policy_gradient_loss -0.00735
wandb:                  PPO_1131/train/std 0.64101
wandb:           PPO_1131/train/value_loss 43.35217
wandb:                PPO_1140/global_step 212992
wandb:        PPO_1140/rollout/ep_len_mean 200.0
wandb:        PPO_1140/rollout/ep_rew_mean -555.20276
wandb:                   PPO_1140/time/fps 1164.0
wandb:            PPO_1140/train/approx_kl 0.01501
wandb:        PPO_1140/train/clip_fraction 0.21677
wandb:           PPO_1140/train/clip_range 0.2
wandb:         PPO_1140/train/entropy_loss -6.08575
wandb:   PPO_1140/train/explained_variance 0.97165
wandb:        PPO_1140/train/learning_rate 0.0003
wandb:                 PPO_1140/train/loss 38.80249
wandb: PPO_1140/train/policy_gradient_loss -0.00353
wandb:                  PPO_1140/train/std 0.57674
wandb:           PPO_1140/train/value_loss 43.11947
wandb:                PPO_1150/global_step 212992
wandb:        PPO_1150/rollout/ep_len_mean 200.0
wandb:        PPO_1150/rollout/ep_rew_mean -580.68933
wandb:                   PPO_1150/time/fps 1152.0
wandb:            PPO_1150/train/approx_kl 0.01529
wandb:        PPO_1150/train/clip_fraction 0.19775
wandb:           PPO_1150/train/clip_range 0.2
wandb:         PPO_1150/train/entropy_loss -5.61789
wandb:   PPO_1150/train/explained_variance 0.98922
wandb:        PPO_1150/train/learning_rate 0.0003
wandb:                 PPO_1150/train/loss 74.46011
wandb: PPO_1150/train/policy_gradient_loss -0.0033
wandb:                  PPO_1150/train/std 0.53993
wandb:           PPO_1150/train/value_loss 92.94354
wandb:                PPO_1160/global_step 212992
wandb:        PPO_1160/rollout/ep_len_mean 200.0
wandb:        PPO_1160/rollout/ep_rew_mean -532.73938
wandb:                   PPO_1160/time/fps 1175.0
wandb:            PPO_1160/train/approx_kl 0.01974
wandb:        PPO_1160/train/clip_fraction 0.26909
wandb:           PPO_1160/train/clip_range 0.2
wandb:         PPO_1160/train/entropy_loss -5.1251
wandb:   PPO_1160/train/explained_variance 0.99511
wandb:        PPO_1160/train/learning_rate 0.0003
wandb:                 PPO_1160/train/loss 60.86214
wandb: PPO_1160/train/policy_gradient_loss -0.0005
wandb:                  PPO_1160/train/std 0.50183
wandb:           PPO_1160/train/value_loss 52.68937
wandb:                PPO_1170/global_step 212992
wandb:        PPO_1170/rollout/ep_len_mean 200.0
wandb:        PPO_1170/rollout/ep_rew_mean -461.85205
wandb:                   PPO_1170/time/fps 1173.0
wandb:            PPO_1170/train/approx_kl 0.021
wandb:        PPO_1170/train/clip_fraction 0.2578
wandb:           PPO_1170/train/clip_range 0.2
wandb:         PPO_1170/train/entropy_loss -4.56233
wandb:   PPO_1170/train/explained_variance 0.99367
wandb:        PPO_1170/train/learning_rate 0.0003
wandb:                 PPO_1170/train/loss 13.65897
wandb: PPO_1170/train/policy_gradient_loss -0.00182
wandb:                  PPO_1170/train/std 0.46386
wandb:           PPO_1170/train/value_loss 32.60191
wandb:                PPO_1180/global_step 212992
wandb:        PPO_1180/rollout/ep_len_mean 200.0
wandb:        PPO_1180/rollout/ep_rew_mean -483.05478
wandb:                   PPO_1180/time/fps 1168.0
wandb:            PPO_1180/train/approx_kl 0.02446
wandb:        PPO_1180/train/clip_fraction 0.27141
wandb:           PPO_1180/train/clip_range 0.2
wandb:         PPO_1180/train/entropy_loss -3.91078
wandb:   PPO_1180/train/explained_variance 0.99474
wandb:        PPO_1180/train/learning_rate 0.0003
wandb:                 PPO_1180/train/loss 33.14694
wandb: PPO_1180/train/policy_gradient_loss 0.00145
wandb:                  PPO_1180/train/std 0.42259
wandb:           PPO_1180/train/value_loss 51.00081
wandb:                PPO_1190/global_step 212992
wandb:        PPO_1190/rollout/ep_len_mean 200.0
wandb:        PPO_1190/rollout/ep_rew_mean -436.42831
wandb:                   PPO_1190/time/fps 1165.0
wandb:            PPO_1190/train/approx_kl 0.02657
wandb:        PPO_1190/train/clip_fraction 0.30443
wandb:           PPO_1190/train/clip_range 0.2
wandb:         PPO_1190/train/entropy_loss -3.31628
wandb:   PPO_1190/train/explained_variance 0.99533
wandb:        PPO_1190/train/learning_rate 0.0003
wandb:                 PPO_1190/train/loss 5.53272
wandb: PPO_1190/train/policy_gradient_loss 0.0043
wandb:                  PPO_1190/train/std 0.38817
wandb:           PPO_1190/train/value_loss 36.84404
wandb:                PPO_1200/global_step 212992
wandb:        PPO_1200/rollout/ep_len_mean 200.0
wandb:        PPO_1200/rollout/ep_rew_mean -418.2193
wandb:                   PPO_1200/time/fps 1166.0
wandb:            PPO_1200/train/approx_kl 0.0228
wandb:        PPO_1200/train/clip_fraction 0.28937
wandb:           PPO_1200/train/clip_range 0.2
wandb:         PPO_1200/train/entropy_loss -2.88643
wandb:   PPO_1200/train/explained_variance 0.99632
wandb:        PPO_1200/train/learning_rate 0.0003
wandb:                 PPO_1200/train/loss 3.66067
wandb: PPO_1200/train/policy_gradient_loss 0.00329
wandb:                  PPO_1200/train/std 0.36603
wandb:           PPO_1200/train/value_loss 25.13123
wandb:                    global_mean_eval -362.8814
wandb:                         global_step 212992
wandb:                       mean_reward_0 -359.25258
wandb:                       mean_reward_1 -356.65708
wandb:                      mean_reward_10 -360.90328
wandb:                      mean_reward_11 -339.45626
wandb:                      mean_reward_12 -342.44594
wandb:                      mean_reward_13 -345.62393
wandb:                      mean_reward_14 -383.2747
wandb:                      mean_reward_15 -366.04616
wandb:                      mean_reward_16 -357.22906
wandb:                      mean_reward_17 -360.49058
wandb:                      mean_reward_18 -367.24069
wandb:                      mean_reward_19 -375.62765
wandb:                       mean_reward_2 -383.78389
wandb:                      mean_reward_20 -348.36969
wandb:                      mean_reward_21 -362.65826
wandb:                      mean_reward_22 -373.19144
wandb:                      mean_reward_23 -367.87347
wandb:                      mean_reward_24 -348.16302
wandb:                      mean_reward_25 -364.95497
wandb:                      mean_reward_26 -354.41146
wandb:                      mean_reward_27 -377.63155
wandb:                      mean_reward_28 -377.39074
wandb:                      mean_reward_29 -372.4352
wandb:                       mean_reward_3 -363.20695
wandb:                      mean_reward_30 -379.07418
wandb:                      mean_reward_31 -328.64959
wandb:                      mean_reward_32 -347.09675
wandb:                      mean_reward_33 -368.37569
wandb:                      mean_reward_34 -379.33015
wandb:                      mean_reward_35 -380.5612
wandb:                       mean_reward_4 -389.13455
wandb:                       mean_reward_5 -348.00405
wandb:                       mean_reward_6 -380.44897
wandb:                       mean_reward_7 -341.6057
wandb:                       mean_reward_8 -386.19998
wandb:                       mean_reward_9 -326.93102
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -931.94031
wandb:                        std_reward_0 143.54135
wandb:                        std_reward_1 138.61459
wandb:                       std_reward_10 145.80737
wandb:                       std_reward_11 137.59549
wandb:                       std_reward_12 134.18119
wandb:                       std_reward_13 122.65295
wandb:                       std_reward_14 149.02075
wandb:                       std_reward_15 146.65232
wandb:                       std_reward_16 135.05189
wandb:                       std_reward_17 145.72375
wandb:                       std_reward_18 143.23654
wandb:                       std_reward_19 154.14007
wandb:                        std_reward_2 160.50854
wandb:                       std_reward_20 133.13621
wandb:                       std_reward_21 145.88288
wandb:                       std_reward_22 147.83639
wandb:                       std_reward_23 149.24221
wandb:                       std_reward_24 134.44741
wandb:                       std_reward_25 140.69146
wandb:                       std_reward_26 140.97521
wandb:                       std_reward_27 153.65389
wandb:                       std_reward_28 157.14261
wandb:                       std_reward_29 153.7738
wandb:                        std_reward_3 147.90564
wandb:                       std_reward_30 155.70609
wandb:                       std_reward_31 118.7647
wandb:                       std_reward_32 139.07409
wandb:                       std_reward_33 149.96956
wandb:                       std_reward_34 150.705
wandb:                       std_reward_35 153.10904
wandb:                        std_reward_4 162.8246
wandb:                        std_reward_5 138.69479
wandb:                        std_reward_6 159.07173
wandb:                        std_reward_7 133.14694
wandb:                        std_reward_8 157.28754
wandb:                        std_reward_9 108.72189
wandb:                            time/fps 1178.0
wandb:                     train/approx_kl 0.01244
wandb:                 train/clip_fraction 0.15652
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7411
wandb:            train/explained_variance 0.96718
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 11.08948
wandb:          train/policy_gradient_loss -0.01304
wandb:                           train/std 0.84138
wandb:                    train/value_loss 21.77692
wandb: 
wandb: Synced wise-totem-24: https://wandb.ai/tidiane/meta_rl_context/runs/15smaiuw
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_012622-15smaiuw/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1120/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1120/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1120/rollout/ep_rew_mean ▁▁▂▂▃▅▆▇██▇█
wandb:                   PPO_1120/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1120/train/approx_kl ▁▂▆▆█▅▇█▆▄▃
wandb:        PPO_1120/train/clip_fraction ▂▁▆▆▆▄▅█▇▄▅
wandb:           PPO_1120/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1120/train/entropy_loss ▁▂▂▃▄▅▅▆▇▇█
wandb:   PPO_1120/train/explained_variance ▃▁▅█▇▇▅▇▄▆▅
wandb:        PPO_1120/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1120/train/loss ▂▂▂▁▂▃▃▂▃█▂
wandb: PPO_1120/train/policy_gradient_loss ▂▇▄▁▅▅▇▂▄▆█
wandb:                  PPO_1120/train/std █▇▇▆▅▄▃▃▂▂▁
wandb:           PPO_1120/train/value_loss ▁▂▃▁▃▄▇▄▆█▇
wandb:                PPO_1130/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1130/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1130/rollout/ep_rew_mean ▁▂▂▂▃▃▄▄▆▆▇█
wandb:                   PPO_1130/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1130/train/approx_kl ▁▂▃▄▅▄▅▁▅▃█
wandb:        PPO_1130/train/clip_fraction ▁▃▃▂▄▃▄▄▆▅█
wandb:           PPO_1130/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1130/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1130/train/explained_variance █▄▄▇▆▆▄▅▁▆▆
wandb:        PPO_1130/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1130/train/loss ▆▁█▃▄▇▅▂▁▃▃
wandb: PPO_1130/train/policy_gradient_loss ▃▄▃▃▁▄▄▆█▅▃
wandb:                  PPO_1130/train/std █▇▇▆▅▅▄▄▃▂▁
wandb:           PPO_1130/train/value_loss ▇▅▄▅█▅▄▆▂▁▁
wandb:                PPO_1141/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1141/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1141/rollout/ep_rew_mean ▁▂▂▃▄▄▅▅▅▇▆█
wandb:                   PPO_1141/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1141/train/approx_kl ▁▃▂▂▂▃▄▂█▅▇
wandb:        PPO_1141/train/clip_fraction ▁▃▃▃▃▃▃▁▇█▆
wandb:           PPO_1141/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1141/train/entropy_loss ▁▂▂▃▃▄▅▅▆▇█
wandb:   PPO_1141/train/explained_variance ▄▅▁▅▆██▇█▅▇
wandb:        PPO_1141/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1141/train/loss ▅▅█▂▅▁▂▂▆▃▁
wandb: PPO_1141/train/policy_gradient_loss ▄▁▅▇█▆▅▆█▆█
wandb:                  PPO_1141/train/std █▇▇▆▆▅▄▄▃▂▁
wandb:           PPO_1141/train/value_loss █▅▄▂▂▂▁▂▂▂▁
wandb:                PPO_1151/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1151/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1151/rollout/ep_rew_mean ▁▃▃▄▃▅█▅▆█▇▆
wandb:                   PPO_1151/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1151/train/approx_kl ▁▄▆▆▄▆█▄▆▇▇
wandb:        PPO_1151/train/clip_fraction ▂▃▅▆▅▆█▁█▆▄
wandb:           PPO_1151/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1151/train/entropy_loss ▁▂▂▃▄▅▅▆▆▇█
wandb:   PPO_1151/train/explained_variance █▆▆▇█▆▇▁▁▃▂
wandb:        PPO_1151/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1151/train/loss ▂▃▄▂▁▄▄▂█▃▂
wandb: PPO_1151/train/policy_gradient_loss ▃▁▄▄▁▆▆▅▆█▆
wandb:                  PPO_1151/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1151/train/value_loss ▂▇█▆▅▂▁█▇▄▆
wandb:                PPO_1161/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1161/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1161/rollout/ep_rew_mean ▅▄▅▅▇▆▆▁▁▂▃█
wandb:                   PPO_1161/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1161/train/approx_kl ▄▆▅▇█▄▃▁▂▃▄
wandb:        PPO_1161/train/clip_fraction █▄█▇▇█▁▃▄▃▆
wandb:           PPO_1161/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1161/train/entropy_loss ▁▂▂▂▃▃▃▅▆▇█
wandb:   PPO_1161/train/explained_variance ▃▂▁▆▅▇▆▅▇▇█
wandb:        PPO_1161/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1161/train/loss ▁▃█▄▂▁▄▄▅▂▄
wandb: PPO_1161/train/policy_gradient_loss █▃▅▄▂▄▁▅▃▄▄
wandb:                  PPO_1161/train/std ██▇▇▆▆▅▅▃▂▁
wandb:           PPO_1161/train/value_loss ▆█▆▅▄▁▅▇▆▅▄
wandb:                PPO_1171/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1171/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1171/rollout/ep_rew_mean ▁▆▇▅▇▅▆▇▇▆█▇
wandb:                   PPO_1171/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1171/train/approx_kl ▄▁▃▅▇▆▇▆█▁▇
wandb:        PPO_1171/train/clip_fraction ▁▅▃▄█▄▇▇▃▃█
wandb:           PPO_1171/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1171/train/entropy_loss ▁▁▂▂▃▄▆▆▇▇█
wandb:   PPO_1171/train/explained_variance ▁▆█▆▆▅▅█▆▅▆
wandb:        PPO_1171/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1171/train/loss ▂▂▂▅▃▃▂▃▁█▁
wandb: PPO_1171/train/policy_gradient_loss ▂█▇▂▄▅▄▂▁▆▆
wandb:                  PPO_1171/train/std ███▇▆▅▃▃▂▂▁
wandb:           PPO_1171/train/value_loss █▃▂▄▁▄▄▁▅▆▂
wandb:                PPO_1181/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1181/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1181/rollout/ep_rew_mean ▁▂▇▄▆█▅▂▅▅▄█
wandb:                   PPO_1181/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1181/train/approx_kl ▄▂█▁▆▆▃▅▅▂▅
wandb:        PPO_1181/train/clip_fraction ▅▂▇▁▆▅▂▃█▄▄
wandb:           PPO_1181/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1181/train/entropy_loss ▁▂▃▃▃▄▄▆▆█▆
wandb:   PPO_1181/train/explained_variance ▂▁▅▃██▇▆▂▇▅
wandb:        PPO_1181/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1181/train/loss ▆█▂▅▁▂▂▇▄▃▃
wandb: PPO_1181/train/policy_gradient_loss ▃▅▄▁▆█▄▁▇▄▅
wandb:                  PPO_1181/train/std █▆▄▆▆▅▄▃▁▁▂
wandb:           PPO_1181/train/value_loss ▆▇▃█▁▂▄█▁▃▃
wandb:                PPO_1191/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1191/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1191/rollout/ep_rew_mean ▃▁▁▁▂▄▆▄█▇▆▆
wandb:                   PPO_1191/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1191/train/approx_kl ▄█▁▃▇█▅▅▇▆▅
wandb:        PPO_1191/train/clip_fraction ▂▂▁▃▃▃▂▂█▃▂
wandb:           PPO_1191/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1191/train/entropy_loss ▁▂▂▃▃▄▄▅▆▇█
wandb:   PPO_1191/train/explained_variance ▁▃▅▅▂▅▆▃▃▁█
wandb:        PPO_1191/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1191/train/loss ▂▅▃▄▁▄▂▆▁▁█
wandb: PPO_1191/train/policy_gradient_loss ▇▄▂▅▁▄▅▄█▄▄
wandb:                  PPO_1191/train/std █▇▇▆▆▅▄▄▃▂▁
wandb:           PPO_1191/train/value_loss ▆▇█▃▄▃▄█▁▂▅
wandb:                PPO_1201/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1201/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1201/rollout/ep_rew_mean ▂▁▁▃▃▂█▅▆▆▇▆
wandb:                   PPO_1201/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1201/train/approx_kl ▅▅▇▅▄▁▅▇▅█▆
wandb:        PPO_1201/train/clip_fraction ▄▁▇▆▄▄█▆▄▆▃
wandb:           PPO_1201/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1201/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1201/train/explained_variance ▁▇▅█▅▇▇▇███
wandb:        PPO_1201/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1201/train/loss ▂█▇▃▃▄▃▅▃▄▁
wandb: PPO_1201/train/policy_gradient_loss ▆▁▆█▆▄█▃▄▅▇
wandb:                  PPO_1201/train/std █▇▇▆▅▅▄▃▃▃▁
wandb:           PPO_1201/train/value_loss ██▆▂▃▃▁▁▁▁▂
wandb:                    global_mean_eval ▁▃▅▆▆▆▇███
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▃▅▆▆▇▇▇██
wandb:                       mean_reward_1 ▁▃▅▆▆▇▇███
wandb:                      mean_reward_10 ▁▃▅▆▆▆▇█▇█
wandb:                      mean_reward_11 ▁▃▄▆▆▆▇▇██
wandb:                      mean_reward_12 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_13 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_14 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_15 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_16 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_17 ▁▃▅▆▆▆▇█▇█
wandb:                      mean_reward_18 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_19 ▁▃▅▆▆▆▆███
wandb:                       mean_reward_2 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_20 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_21 ▁▃▄▆▆▆▇▇██
wandb:                      mean_reward_22 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_23 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_24 ▁▃▄▆▆▆▇▇██
wandb:                      mean_reward_25 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_26 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_27 ▁▃▅▆▆▆▇▇██
wandb:                      mean_reward_28 ▁▃▅▆▆▇▇█▇█
wandb:                      mean_reward_29 ▁▃▅▆▆▆▇▇██
wandb:                       mean_reward_3 ▁▃▄▆▆▆▇▇██
wandb:                      mean_reward_30 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_31 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_32 ▁▃▄▆▆▆▇▇██
wandb:                      mean_reward_33 ▁▃▄▆▆▆▇▇██
wandb:                      mean_reward_34 ▁▃▅▆▆▆▇███
wandb:                      mean_reward_35 ▁▃▅▆▆▆████
wandb:                       mean_reward_4 ▁▃▄▆▆▆▆███
wandb:                       mean_reward_5 ▁▃▄▆▆▆▇▇██
wandb:                       mean_reward_6 ▁▃▅▆▆▆▇███
wandb:                       mean_reward_7 ▁▃▅▆▆▆▇███
wandb:                       mean_reward_8 ▁▃▅▆▆▆▇███
wandb:                       mean_reward_9 ▁▃▅▆▆▇▇███
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▁▂▁▃▂▃▄▅▆▆█
wandb:                        std_reward_0 ▁▁▁▂▃▇█▄▅▄
wandb:                        std_reward_1 ▂▁▁▃▄██▅▅▄
wandb:                       std_reward_10 ▂▁▁▂▄█▇▄▅▃
wandb:                       std_reward_11 ▂▁▁▂▃▇█▄▅▄
wandb:                       std_reward_12 ▂▁▁▃▃██▄▅▄
wandb:                       std_reward_13 ▁▁▁▂▄██▅▄▃
wandb:                       std_reward_14 ▂▁▁▃▄██▄▅▄
wandb:                       std_reward_15 ▂▁▁▂▄██▄▅▄
wandb:                       std_reward_16 ▂▁▁▂▃██▅▄▄
wandb:                       std_reward_17 ▂▁▁▂▃██▅▅▄
wandb:                       std_reward_18 ▂▁▁▂▄██▅▅▄
wandb:                       std_reward_19 ▂▁▁▂▃▇█▄▅▃
wandb:                        std_reward_2 ▂▁▁▃▃▆█▅▅▄
wandb:                       std_reward_20 ▂▁▁▂▄▇█▄▅▄
wandb:                       std_reward_21 ▂▁▁▂▃█▇▄▄▄
wandb:                       std_reward_22 ▂▁▁▂▃██▄▅▄
wandb:                       std_reward_23 ▂▁▁▂▃██▅▅▄
wandb:                       std_reward_24 ▂▁▁▂▃██▅▄▄
wandb:                       std_reward_25 ▂▁▁▂▃██▅▅▄
wandb:                       std_reward_26 ▂▁▁▃▄██▅▅▄
wandb:                       std_reward_27 ▂▁▁▃▄▇█▄▅▃
wandb:                       std_reward_28 ▂▁▁▂▃██▅▆▄
wandb:                       std_reward_29 ▂▁▁▂▄██▅▅▄
wandb:                        std_reward_3 ▂▁▁▂▃██▅▅▄
wandb:                       std_reward_30 ▂▁▁▂▄██▅▅▄
wandb:                       std_reward_31 ▂▁▁▂▃▇█▅▅▄
wandb:                       std_reward_32 ▂▁▁▃▃██▄▄▄
wandb:                       std_reward_33 ▂▁▁▂▃██▄▄▄
wandb:                       std_reward_34 ▂▁▁▂▃▇█▄▄▄
wandb:                       std_reward_35 ▂▁▁▃▃█▇▅▅▄
wandb:                        std_reward_4 ▂▁▁▂▃▆█▄▄▃
wandb:                        std_reward_5 ▁▁▁▂▃██▅▅▃
wandb:                        std_reward_6 ▂▁▁▂▃██▄▄▄
wandb:                        std_reward_7 ▂▁▁▂▃▇█▄▅▄
wandb:                        std_reward_8 ▂▁▁▃▄█▇▄▅▄
wandb:                        std_reward_9 ▂▁▁▂▄▇█▅▅▄
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▂▃█▁▂▂▄▃▄▆▆
wandb:                 train/clip_fraction ▃▃▃▄▄▁▃▅▅▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▃▆▇▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▃▂▂▁▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▆▆▅█▆▅▃▄▂▁▂
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1120/global_step 212992
wandb:        PPO_1120/rollout/ep_len_mean 200.0
wandb:        PPO_1120/rollout/ep_rew_mean -831.30267
wandb:                   PPO_1120/time/fps 1175.0
wandb:            PPO_1120/train/approx_kl 0.01202
wandb:        PPO_1120/train/clip_fraction 0.15948
wandb:           PPO_1120/train/clip_range 0.2
wandb:         PPO_1120/train/entropy_loss -7.63312
wandb:   PPO_1120/train/explained_variance 0.96781
wandb:        PPO_1120/train/learning_rate 0.0003
wandb:                 PPO_1120/train/loss 16.05584
wandb: PPO_1120/train/policy_gradient_loss -0.00777
wandb:                  PPO_1120/train/std 0.71941
wandb:           PPO_1120/train/value_loss 71.51579
wandb:                PPO_1130/global_step 212992
wandb:        PPO_1130/rollout/ep_len_mean 200.0
wandb:        PPO_1130/rollout/ep_rew_mean -695.57788
wandb:                   PPO_1130/time/fps 1163.0
wandb:            PPO_1130/train/approx_kl 0.01606
wandb:        PPO_1130/train/clip_fraction 0.21004
wandb:           PPO_1130/train/clip_range 0.2
wandb:         PPO_1130/train/entropy_loss -6.61702
wandb:   PPO_1130/train/explained_variance 0.96771
wandb:        PPO_1130/train/learning_rate 0.0003
wandb:                 PPO_1130/train/loss 24.09903
wandb: PPO_1130/train/policy_gradient_loss -0.00769
wandb:                  PPO_1130/train/std 0.62277
wandb:           PPO_1130/train/value_loss 47.69817
wandb:                PPO_1141/global_step 212992
wandb:        PPO_1141/rollout/ep_len_mean 200.0
wandb:        PPO_1141/rollout/ep_rew_mean -565.15503
wandb:                   PPO_1141/time/fps 1161.0
wandb:            PPO_1141/train/approx_kl 0.01834
wandb:        PPO_1141/train/clip_fraction 0.22277
wandb:           PPO_1141/train/clip_range 0.2
wandb:         PPO_1141/train/entropy_loss -5.67604
wandb:   PPO_1141/train/explained_variance 0.97753
wandb:        PPO_1141/train/learning_rate 0.0003
wandb:                 PPO_1141/train/loss 4.35276
wandb: PPO_1141/train/policy_gradient_loss -0.00412
wandb:                  PPO_1141/train/std 0.5431
wandb:           PPO_1141/train/value_loss 24.3551
wandb:                PPO_1151/global_step 212992
wandb:        PPO_1151/rollout/ep_len_mean 200.0
wandb:        PPO_1151/rollout/ep_rew_mean -547.46997
wandb:                   PPO_1151/time/fps 1169.0
wandb:            PPO_1151/train/approx_kl 0.01985
wandb:        PPO_1151/train/clip_fraction 0.23166
wandb:           PPO_1151/train/clip_range 0.2
wandb:         PPO_1151/train/entropy_loss -4.92103
wandb:   PPO_1151/train/explained_variance 0.95775
wandb:        PPO_1151/train/learning_rate 0.0003
wandb:                 PPO_1151/train/loss 10.41072
wandb: PPO_1151/train/policy_gradient_loss -0.00048
wandb:                  PPO_1151/train/std 0.48848
wandb:           PPO_1151/train/value_loss 39.32831
wandb:                PPO_1161/global_step 212992
wandb:        PPO_1161/rollout/ep_len_mean 200.0
wandb:        PPO_1161/rollout/ep_rew_mean -516.47333
wandb:                   PPO_1161/time/fps 1162.0
wandb:            PPO_1161/train/approx_kl 0.01695
wandb:        PPO_1161/train/clip_fraction 0.22978
wandb:           PPO_1161/train/clip_range 0.2
wandb:         PPO_1161/train/entropy_loss -4.48877
wandb:   PPO_1161/train/explained_variance 0.98744
wandb:        PPO_1161/train/learning_rate 0.0003
wandb:                 PPO_1161/train/loss 22.85974
wandb: PPO_1161/train/policy_gradient_loss -0.00159
wandb:                  PPO_1161/train/std 0.45895
wandb:           PPO_1161/train/value_loss 34.10161
wandb:                PPO_1171/global_step 212992
wandb:        PPO_1171/rollout/ep_len_mean 200.0
wandb:        PPO_1171/rollout/ep_rew_mean -509.65793
wandb:                   PPO_1171/time/fps 1159.0
wandb:            PPO_1171/train/approx_kl 0.01782
wandb:        PPO_1171/train/clip_fraction 0.23892
wandb:           PPO_1171/train/clip_range 0.2
wandb:         PPO_1171/train/entropy_loss -4.14891
wandb:   PPO_1171/train/explained_variance 0.98823
wandb:        PPO_1171/train/learning_rate 0.0003
wandb:                 PPO_1171/train/loss 11.52244
wandb: PPO_1171/train/policy_gradient_loss -0.00163
wandb:                  PPO_1171/train/std 0.4375
wandb:           PPO_1171/train/value_loss 47.35141
wandb:                PPO_1181/global_step 212992
wandb:        PPO_1181/rollout/ep_len_mean 200.0
wandb:        PPO_1181/rollout/ep_rew_mean -471.17471
wandb:                   PPO_1181/time/fps 1167.0
wandb:            PPO_1181/train/approx_kl 0.01951
wandb:        PPO_1181/train/clip_fraction 0.23051
wandb:           PPO_1181/train/clip_range 0.2
wandb:         PPO_1181/train/entropy_loss -4.01216
wandb:   PPO_1181/train/explained_variance 0.99176
wandb:        PPO_1181/train/learning_rate 0.0003
wandb:                 PPO_1181/train/loss 17.14367
wandb: PPO_1181/train/policy_gradient_loss -0.00146
wandb:                  PPO_1181/train/std 0.42953
wandb:           PPO_1181/train/value_loss 48.32584
wandb:                PPO_1191/global_step 212992
wandb:        PPO_1191/rollout/ep_len_mean 200.0
wandb:        PPO_1191/rollout/ep_rew_mean -430.90793
wandb:                   PPO_1191/time/fps 1167.0
wandb:            PPO_1191/train/approx_kl 0.01849
wandb:        PPO_1191/train/clip_fraction 0.23685
wandb:           PPO_1191/train/clip_range 0.2
wandb:         PPO_1191/train/entropy_loss -3.50595
wandb:   PPO_1191/train/explained_variance 0.99604
wandb:        PPO_1191/train/learning_rate 0.0003
wandb:                 PPO_1191/train/loss 19.87695
wandb: PPO_1191/train/policy_gradient_loss -0.00122
wandb:                  PPO_1191/train/std 0.39919
wandb:           PPO_1191/train/value_loss 32.35122
wandb:                PPO_1201/global_step 212992
wandb:        PPO_1201/rollout/ep_len_mean 200.0
wandb:        PPO_1201/rollout/ep_rew_mean -403.49664
wandb:                   PPO_1201/time/fps 1170.0
wandb:            PPO_1201/train/approx_kl 0.02197
wandb:        PPO_1201/train/clip_fraction 0.25259
wandb:           PPO_1201/train/clip_range 0.2
wandb:         PPO_1201/train/entropy_loss -3.11101
wandb:   PPO_1201/train/explained_variance 0.99616
wandb:        PPO_1201/train/learning_rate 0.0003
wandb:                 PPO_1201/train/loss 4.38856
wandb: PPO_1201/train/policy_gradient_loss 0.00012
wandb:                  PPO_1201/train/std 0.37704
wandb:           PPO_1201/train/value_loss 23.01128
wandb:                    global_mean_eval -383.69622
wandb:                         global_step 212992
wandb:                       mean_reward_0 -384.7851
wandb:                       mean_reward_1 -393.15757
wandb:                      mean_reward_10 -383.8183
wandb:                      mean_reward_11 -374.8733
wandb:                      mean_reward_12 -395.80335
wandb:                      mean_reward_13 -383.72752
wandb:                      mean_reward_14 -396.72029
wandb:                      mean_reward_15 -388.31478
wandb:                      mean_reward_16 -384.57075
wandb:                      mean_reward_17 -386.0883
wandb:                      mean_reward_18 -383.14235
wandb:                      mean_reward_19 -367.81066
wandb:                       mean_reward_2 -383.39261
wandb:                      mean_reward_20 -380.09824
wandb:                      mean_reward_21 -391.29962
wandb:                      mean_reward_22 -397.5307
wandb:                      mean_reward_23 -391.28321
wandb:                      mean_reward_24 -379.96522
wandb:                      mean_reward_25 -378.94007
wandb:                      mean_reward_26 -381.40968
wandb:                      mean_reward_27 -382.79076
wandb:                      mean_reward_28 -381.49204
wandb:                      mean_reward_29 -382.25025
wandb:                       mean_reward_3 -379.40084
wandb:                      mean_reward_30 -389.1704
wandb:                      mean_reward_31 -395.19848
wandb:                      mean_reward_32 -372.87368
wandb:                      mean_reward_33 -368.40089
wandb:                      mean_reward_34 -380.69193
wandb:                      mean_reward_35 -391.02783
wandb:                       mean_reward_4 -373.89832
wandb:                       mean_reward_5 -359.09469
wandb:                       mean_reward_6 -390.53849
wandb:                       mean_reward_7 -393.12019
wandb:                       mean_reward_8 -394.42131
wandb:                       mean_reward_9 -371.96206
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -931.94031
wandb:                        std_reward_0 97.19771
wandb:                        std_reward_1 98.74095
wandb:                       std_reward_10 86.31339
wandb:                       std_reward_11 89.48045
wandb:                       std_reward_12 91.0774
wandb:                       std_reward_13 87.36547
wandb:                       std_reward_14 94.53606
wandb:                       std_reward_15 92.41501
wandb:                       std_reward_16 94.58237
wandb:                       std_reward_17 97.07774
wandb:                       std_reward_18 92.50418
wandb:                       std_reward_19 86.85524
wandb:                        std_reward_2 90.80126
wandb:                       std_reward_20 87.02601
wandb:                       std_reward_21 93.48887
wandb:                       std_reward_22 92.55775
wandb:                       std_reward_23 86.84994
wandb:                       std_reward_24 101.66554
wandb:                       std_reward_25 96.28752
wandb:                       std_reward_26 89.36404
wandb:                       std_reward_27 85.42189
wandb:                       std_reward_28 85.27738
wandb:                       std_reward_29 88.20388
wandb:                        std_reward_3 90.41064
wandb:                       std_reward_30 94.73672
wandb:                       std_reward_31 92.26221
wandb:                       std_reward_32 89.18544
wandb:                       std_reward_33 90.32146
wandb:                       std_reward_34 94.94128
wandb:                       std_reward_35 102.905
wandb:                        std_reward_4 84.4848
wandb:                        std_reward_5 79.30228
wandb:                        std_reward_6 89.60618
wandb:                        std_reward_7 92.05307
wandb:                        std_reward_8 92.11589
wandb:                        std_reward_9 88.37466
wandb:                            time/fps 1178.0
wandb:                     train/approx_kl 0.01244
wandb:                 train/clip_fraction 0.15652
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7411
wandb:            train/explained_variance 0.96718
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 11.08948
wandb:          train/policy_gradient_loss -0.01304
wandb:                           train/std 0.84138
wandb:                    train/value_loss 21.77692
wandb: 
wandb: Synced misunderstood-monkey-27: https://wandb.ai/tidiane/meta_rl_context/runs/q658kpqh
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_012622-q658kpqh/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1122/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1122/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1122/rollout/ep_rew_mean ▁▂▃▃▃▄▄▅▆▇▇█
wandb:                   PPO_1122/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1122/train/approx_kl ▂▅██▆▃▄▂▂▆▁
wandb:        PPO_1122/train/clip_fraction ▃▃█▆▅▃▅▃▃▄▁
wandb:           PPO_1122/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1122/train/entropy_loss ▁▂▂▃▄▅▅▆▆▇█
wandb:   PPO_1122/train/explained_variance ▆█▇▅▅▃▆▃▁▃▂
wandb:        PPO_1122/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1122/train/loss ▂▁▂▄▁▂▂▆▅▃█
wandb: PPO_1122/train/policy_gradient_loss ▃▃▁▃▃▃▄▅▅▅█
wandb:                  PPO_1122/train/std █▇▆▅▅▄▄▃▃▂▁
wandb:           PPO_1122/train/value_loss ▁▂▁▃▂▆▃▄███
wandb:                PPO_1132/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1132/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1132/rollout/ep_rew_mean ▁▁▂▄▅▅▅▆▆▇▇█
wandb:                   PPO_1132/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1132/train/approx_kl ▄▃▆▂▆▁▂▁▃▃█
wandb:        PPO_1132/train/clip_fraction ▁▄█▅▅▄▆▃▆▄▄
wandb:           PPO_1132/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1132/train/entropy_loss ▁▂▂▃▃▄▅▅▆▇█
wandb:   PPO_1132/train/explained_variance ▁▃▂▄▃▃▅▇█▇█
wandb:        PPO_1132/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1132/train/loss ▆█▁▂█▄▄▇▂▁▂
wandb: PPO_1132/train/policy_gradient_loss ▄▄▁▂▁▅▄▇▇█▆
wandb:                  PPO_1132/train/std ██▇▆▆▅▄▄▃▂▁
wandb:           PPO_1132/train/value_loss ▇█▄▆▇▃▃▃▁▁▂
wandb:                PPO_1142/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1142/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1142/rollout/ep_rew_mean ▁▁▃▆▂▄▆▇▆▆█▆
wandb:                   PPO_1142/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1142/train/approx_kl ▅▃▅█▁▅▃▃▄▅▇
wandb:        PPO_1142/train/clip_fraction ▄▆▆▇▁█▆▅▆█▆
wandb:           PPO_1142/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1142/train/entropy_loss ▁▂▃▄▄▄▅▅▆▇█
wandb:   PPO_1142/train/explained_variance █▇▆▇▄▁▄▃▇▇▄
wandb:        PPO_1142/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1142/train/loss █▂▃▂▂▂▁▂▂▁▁
wandb: PPO_1142/train/policy_gradient_loss ▆▃▁▃▄▅▄▃█▆▅
wandb:                  PPO_1142/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1142/train/value_loss ▃▁▃▃█▃▂▃▂▁▂
wandb:                PPO_1152/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1152/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1152/rollout/ep_rew_mean ▆▂▄▂▅▄██▁▆█▁
wandb:                   PPO_1152/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1152/train/approx_kl █▆▁▂▄▃▆█▃█▂
wandb:        PPO_1152/train/clip_fraction ▆▇▆▃▄▃▇▇▁█▂
wandb:           PPO_1152/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1152/train/entropy_loss ▁▂▂▃▃▄▄▅▅▆█
wandb:   PPO_1152/train/explained_variance ▁▁▂▄▂▄▆▆▄▇█
wandb:        PPO_1152/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1152/train/loss ▃▂▁▃▄▆▂▁█▃▃
wandb: PPO_1152/train/policy_gradient_loss ▇▄▅▃▆▂▂▄▃█▁
wandb:                  PPO_1152/train/std █▇▇▆▆▅▅▄▄▃▁
wandb:           PPO_1152/train/value_loss ▃▄▄▃▄▆▁▁█▄▂
wandb:                PPO_1162/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1162/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1162/rollout/ep_rew_mean ▁▄▆██▅▄▆▆▆▄▇
wandb:                   PPO_1162/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1162/train/approx_kl ▁▂▃▆▄█▆▆▆▂▆
wandb:        PPO_1162/train/clip_fraction ▁▅▆█▅▅▆▅▆▅▅
wandb:           PPO_1162/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1162/train/entropy_loss ▁▂▂▃▄▄▅▆▇▇█
wandb:   PPO_1162/train/explained_variance ▁▄▂▆▇▅▆▇█▇█
wandb:        PPO_1162/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1162/train/loss █▂▃▄▂▄▂▁▃▆▅
wandb: PPO_1162/train/policy_gradient_loss ▄█▇▇▅▃█▁▄▄▇
wandb:                  PPO_1162/train/std █▇▇▆▅▅▄▃▂▂▁
wandb:           PPO_1162/train/value_loss █▄▄▂▁▄▂▃▃▆▃
wandb:                PPO_1172/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1172/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1172/rollout/ep_rew_mean ▇▁▃█▂▁▃▃▃▇▄▂
wandb:                   PPO_1172/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1172/train/approx_kl ▄▃▃▇▁▁▁▃▇█▆
wandb:        PPO_1172/train/clip_fraction ▆▁▃▇▂▂▄▆██▇
wandb:           PPO_1172/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1172/train/entropy_loss ▁▁▂▃▃▃▄▄▆▇█
wandb:   PPO_1172/train/explained_variance ▁▃▅▄▅▇▇▇▇██
wandb:        PPO_1172/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1172/train/loss ▁▂█▁▄▅█▅▂▁▂
wandb: PPO_1172/train/policy_gradient_loss ▁▆▅▇█▅▅▇▇█▅
wandb:                  PPO_1172/train/std ███▆▆▆▅▅▃▂▁
wandb:           PPO_1172/train/value_loss ▂█▄▃▄▅▄▃▃▁▁
wandb:                PPO_1182/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1182/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1182/rollout/ep_rew_mean ▅▁▄▄▄▅▆▆▄▇▇█
wandb:                   PPO_1182/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1182/train/approx_kl ▁▁▃▂▃█▃▄▄▆▆
wandb:        PPO_1182/train/clip_fraction ▁▁▃▅▄█▁▄▃▆▅
wandb:           PPO_1182/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1182/train/entropy_loss ▁▁▂▂▃▃▄▄▄▅█
wandb:   PPO_1182/train/explained_variance ▆█▅▅▅▆▁▇█▇▁
wandb:        PPO_1182/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1182/train/loss ▃▃▆▅▃▁▂▃█▄▅
wandb: PPO_1182/train/policy_gradient_loss ▃▂▄▃▃█▂▁▂▂▁
wandb:                  PPO_1182/train/std ██▇▇▆▅▅▅▄▄▁
wandb:           PPO_1182/train/value_loss ▄▇▆▅▆▁█▅▅▃▇
wandb:                PPO_1192/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1192/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1192/rollout/ep_rew_mean ▃▃▆▄▂▁▁▄▃▄██
wandb:                   PPO_1192/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1192/train/approx_kl ▃▄█▃▁▅██▂▆▁
wandb:        PPO_1192/train/clip_fraction ▄▆▆▁▃▅▆▆▂█▂
wandb:           PPO_1192/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1192/train/entropy_loss ▁▁▁▂▃▃▄▅▆▇█
wandb:   PPO_1192/train/explained_variance ▆█▄▆▇▂▅▃▃▅▁
wandb:        PPO_1192/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1192/train/loss ▂▂▅▁▂█▆▂▃▃▆
wandb: PPO_1192/train/policy_gradient_loss ▆▇█▅▃▅▁▅▅▇▆
wandb:                  PPO_1192/train/std ███▇▆▅▅▄▃▂▁
wandb:           PPO_1192/train/value_loss ▄▁▂▅▆▇▅▆█▄▇
wandb:                PPO_1202/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1202/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1202/rollout/ep_rew_mean ▁▃▁▁█▅▆▃▆▄▄▁
wandb:                   PPO_1202/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1202/train/approx_kl ▃▃▁▆▅█▃▆▇▂▆
wandb:        PPO_1202/train/clip_fraction ▃▃▁█▆▃▁▆▇▄▅
wandb:           PPO_1202/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1202/train/entropy_loss ▁▁▂▃▄▄▅▅▆▇█
wandb:   PPO_1202/train/explained_variance ▄▆▄▁▆▆▂▅▃▄█
wandb:        PPO_1202/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1202/train/loss ▄▃▆▄▁▄█▄▄▂▃
wandb: PPO_1202/train/policy_gradient_loss ▂▄▅██▃▄▃▆▁▂
wandb:                  PPO_1202/train/std █▇▇▆▅▅▄▄▃▂▁
wandb:           PPO_1202/train/value_loss ▆▄█▂▁▂▇▃▂▂▁
wandb:                    global_mean_eval ▁▄▆▆▆▆▆▆██
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▄▆▆▆▆▆▇██
wandb:                       mean_reward_1 ▁▄▆▆▆▆▅▇██
wandb:                      mean_reward_10 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_11 ▁▄▅▆▆▅▆▆██
wandb:                      mean_reward_12 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_13 ▁▄▅▆▆▆▅▆██
wandb:                      mean_reward_14 ▁▅▆▆▆▆▆▇██
wandb:                      mean_reward_15 ▁▄▆▆▆▅▅▆██
wandb:                      mean_reward_16 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_17 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_18 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_19 ▁▄▆▆▆▆▇▆██
wandb:                       mean_reward_2 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_20 ▁▄▆▆▆▆▅▆██
wandb:                      mean_reward_21 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_22 ▁▄▅▆▆▆▅▆██
wandb:                      mean_reward_23 ▁▄▆▆▆▆▅▆██
wandb:                      mean_reward_24 ▁▄▆▆▆▆▅▆██
wandb:                      mean_reward_25 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_26 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_27 ▁▄▆▆▆▆▆▇██
wandb:                      mean_reward_28 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_29 ▁▄▆▆▆▆▅▇██
wandb:                       mean_reward_3 ▁▄▅▆▆▆▆▅▇█
wandb:                      mean_reward_30 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_31 ▁▄▅▆▆▆▆▆██
wandb:                      mean_reward_32 ▁▄▅▆▆▆▅▆██
wandb:                      mean_reward_33 ▁▅▆▆▆▆▆▇██
wandb:                      mean_reward_34 ▁▄▆▆▆▆▆▆██
wandb:                      mean_reward_35 ▁▄▆▆▆▆▆▆██
wandb:                       mean_reward_4 ▁▄▅▆▆▆▅▆██
wandb:                       mean_reward_5 ▁▄▅▆▆▆▆▆██
wandb:                       mean_reward_6 ▁▄▆▆▆▆▆▆██
wandb:                       mean_reward_7 ▁▄▅▆▆▆▅▆██
wandb:                       mean_reward_8 ▁▄▆▆▆▆▆▆██
wandb:                       mean_reward_9 ▁▄▅▆▆▆▅▆██
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▁▂▁▃▂▃▄▅▆▆█
wandb:                        std_reward_0 ▂▁▁▁▂▅█▆▃▃
wandb:                        std_reward_1 ▂▁▂▁▂▅█▅▃▃
wandb:                       std_reward_10 ▂▁▂▁▂▆█▇▃▃
wandb:                       std_reward_11 ▂▁▁▂▂▇██▃▃
wandb:                       std_reward_12 ▂▁▂▁▂▅▇█▃▃
wandb:                       std_reward_13 ▂▁▂▁▃▅█▇▃▃
wandb:                       std_reward_14 ▂▁▂▁▂▅█▇▃▃
wandb:                       std_reward_15 ▂▁▂▁▂▅█▆▂▂
wandb:                       std_reward_16 ▁▁▂▁▂▅█▇▃▃
wandb:                       std_reward_17 ▂▁▂▁▃▅█▆▃▃
wandb:                       std_reward_18 ▂▁▂▁▃▅██▃▄
wandb:                       std_reward_19 ▂▁▂▂▃▅▇█▃▄
wandb:                        std_reward_2 ▂▁▂▁▃▆█▇▃▃
wandb:                       std_reward_20 ▂▁▂▁▂▄█▆▃▃
wandb:                       std_reward_21 ▂▁▂▂▃▆█▇▃▃
wandb:                       std_reward_22 ▂▁▁▁▃▅█▆▂▃
wandb:                       std_reward_23 ▁▁▂▂▂▅█▇▂▃
wandb:                       std_reward_24 ▂▁▂▁▂▅█▇▂▃
wandb:                       std_reward_25 ▂▁▂▁▃▅█▆▃▃
wandb:                       std_reward_26 ▂▁▂▁▃▅█▇▃▃
wandb:                       std_reward_27 ▂▁▂▁▃▅█▆▃▃
wandb:                       std_reward_28 ▂▁▂▁▃▅█▇▃▃
wandb:                       std_reward_29 ▂▁▁▂▂▅█▆▃▃
wandb:                        std_reward_3 ▂▁▁▁▃▆██▃▃
wandb:                       std_reward_30 ▂▁▂▁▃▅█▇▃▃
wandb:                       std_reward_31 ▂▁▂▁▂▅▇█▃▃
wandb:                       std_reward_32 ▂▁▂▁▃▅█▇▃▃
wandb:                       std_reward_33 ▂▁▂▁▃▅█▆▃▃
wandb:                       std_reward_34 ▂▁▂▁▃▆█▇▃▃
wandb:                       std_reward_35 ▂▁▂▂▃▆██▃▄
wandb:                        std_reward_4 ▂▁▂▁▂▄█▆▃▃
wandb:                        std_reward_5 ▂▁▁▁▃▆▇█▃▄
wandb:                        std_reward_6 ▂▁▂▁▃▅█▇▃▃
wandb:                        std_reward_7 ▂▁▁▂▂▅██▃▃
wandb:                        std_reward_8 ▂▁▂▁▃▆▇█▃▃
wandb:                        std_reward_9 ▂▁▂▁▂▅█▆▂▂
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▂▃█▁▂▂▄▃▄▆▆
wandb:                 train/clip_fraction ▃▃▃▄▄▁▃▅▅▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▃▆▇▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▃▂▂▁▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▆▆▅█▆▅▃▄▂▁▂
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1122/global_step 212992
wandb:        PPO_1122/rollout/ep_len_mean 200.0
wandb:        PPO_1122/rollout/ep_rew_mean -765.4762
wandb:                   PPO_1122/time/fps 1164.0
wandb:            PPO_1122/train/approx_kl 0.0109
wandb:        PPO_1122/train/clip_fraction 0.13574
wandb:           PPO_1122/train/clip_range 0.2
wandb:         PPO_1122/train/entropy_loss -7.48617
wandb:   PPO_1122/train/explained_variance 0.9617
wandb:        PPO_1122/train/learning_rate 0.0003
wandb:                 PPO_1122/train/loss 54.31795
wandb: PPO_1122/train/policy_gradient_loss -0.00502
wandb:                  PPO_1122/train/std 0.70378
wandb:           PPO_1122/train/value_loss 85.49967
wandb:                PPO_1132/global_step 212992
wandb:        PPO_1132/rollout/ep_len_mean 200.0
wandb:        PPO_1132/rollout/ep_rew_mean -609.03107
wandb:                   PPO_1132/time/fps 1165.0
wandb:            PPO_1132/train/approx_kl 0.01428
wandb:        PPO_1132/train/clip_fraction 0.17495
wandb:           PPO_1132/train/clip_range 0.2
wandb:         PPO_1132/train/entropy_loss -6.60693
wandb:   PPO_1132/train/explained_variance 0.98015
wandb:        PPO_1132/train/learning_rate 0.0003
wandb:                 PPO_1132/train/loss 19.59736
wandb: PPO_1132/train/policy_gradient_loss -0.00558
wandb:                  PPO_1132/train/std 0.622
wandb:           PPO_1132/train/value_loss 59.69187
wandb:                PPO_1142/global_step 212992
wandb:        PPO_1142/rollout/ep_len_mean 200.0
wandb:        PPO_1142/rollout/ep_rew_mean -557.48566
wandb:                   PPO_1142/time/fps 1159.0
wandb:            PPO_1142/train/approx_kl 0.01786
wandb:        PPO_1142/train/clip_fraction 0.19352
wandb:           PPO_1142/train/clip_range 0.2
wandb:         PPO_1142/train/entropy_loss -5.76783
wandb:   PPO_1142/train/explained_variance 0.96782
wandb:        PPO_1142/train/learning_rate 0.0003
wandb:                 PPO_1142/train/loss 10.53841
wandb: PPO_1142/train/policy_gradient_loss -0.00336
wandb:                  PPO_1142/train/std 0.55095
wandb:           PPO_1142/train/value_loss 42.45692
wandb:                PPO_1152/global_step 212992
wandb:        PPO_1152/rollout/ep_len_mean 200.0
wandb:        PPO_1152/rollout/ep_rew_mean -568.17651
wandb:                   PPO_1152/time/fps 1157.0
wandb:            PPO_1152/train/approx_kl 0.01418
wandb:        PPO_1152/train/clip_fraction 0.17045
wandb:           PPO_1152/train/clip_range 0.2
wandb:         PPO_1152/train/entropy_loss -5.26857
wandb:   PPO_1152/train/explained_variance 0.98839
wandb:        PPO_1152/train/learning_rate 0.0003
wandb:                 PPO_1152/train/loss 12.1888
wandb: PPO_1152/train/policy_gradient_loss -0.00495
wandb:                  PPO_1152/train/std 0.5135
wandb:           PPO_1152/train/value_loss 33.66475
wandb:                PPO_1162/global_step 212992
wandb:        PPO_1162/rollout/ep_len_mean 200.0
wandb:        PPO_1162/rollout/ep_rew_mean -521.18518
wandb:                   PPO_1162/time/fps 1163.0
wandb:            PPO_1162/train/approx_kl 0.01731
wandb:        PPO_1162/train/clip_fraction 0.20936
wandb:           PPO_1162/train/clip_range 0.2
wandb:         PPO_1162/train/entropy_loss -4.73434
wandb:   PPO_1162/train/explained_variance 0.99581
wandb:        PPO_1162/train/learning_rate 0.0003
wandb:                 PPO_1162/train/loss 24.35085
wandb: PPO_1162/train/policy_gradient_loss -0.00296
wandb:                  PPO_1162/train/std 0.47589
wandb:           PPO_1162/train/value_loss 30.86611
wandb:                PPO_1172/global_step 212992
wandb:        PPO_1172/rollout/ep_len_mean 200.0
wandb:        PPO_1172/rollout/ep_rew_mean -553.91174
wandb:                   PPO_1172/time/fps 1147.0
wandb:            PPO_1172/train/approx_kl 0.01804
wandb:        PPO_1172/train/clip_fraction 0.23018
wandb:           PPO_1172/train/clip_range 0.2
wandb:         PPO_1172/train/entropy_loss -4.33654
wandb:   PPO_1172/train/explained_variance 0.9984
wandb:        PPO_1172/train/learning_rate 0.0003
wandb:                 PPO_1172/train/loss 11.16779
wandb: PPO_1172/train/policy_gradient_loss -0.0032
wandb:                  PPO_1172/train/std 0.44925
wandb:           PPO_1172/train/value_loss 26.65805
wandb:                PPO_1182/global_step 212992
wandb:        PPO_1182/rollout/ep_len_mean 200.0
wandb:        PPO_1182/rollout/ep_rew_mean -482.63354
wandb:                   PPO_1182/time/fps 1156.0
wandb:            PPO_1182/train/approx_kl 0.01961
wandb:        PPO_1182/train/clip_fraction 0.23301
wandb:           PPO_1182/train/clip_range 0.2
wandb:         PPO_1182/train/entropy_loss -3.96466
wandb:   PPO_1182/train/explained_variance 0.99724
wandb:        PPO_1182/train/learning_rate 0.0003
wandb:                 PPO_1182/train/loss 24.03317
wandb: PPO_1182/train/policy_gradient_loss -0.00324
wandb:                  PPO_1182/train/std 0.42647
wandb:           PPO_1182/train/value_loss 54.08805
wandb:                PPO_1192/global_step 212992
wandb:        PPO_1192/rollout/ep_len_mean 200.0
wandb:        PPO_1192/rollout/ep_rew_mean -439.90277
wandb:                   PPO_1192/time/fps 1155.0
wandb:            PPO_1192/train/approx_kl 0.01683
wandb:        PPO_1192/train/clip_fraction 0.21965
wandb:           PPO_1192/train/clip_range 0.2
wandb:         PPO_1192/train/entropy_loss -3.49153
wandb:   PPO_1192/train/explained_variance 0.99646
wandb:        PPO_1192/train/learning_rate 0.0003
wandb:                 PPO_1192/train/loss 30.70124
wandb: PPO_1192/train/policy_gradient_loss -0.00139
wandb:                  PPO_1192/train/std 0.39883
wandb:           PPO_1192/train/value_loss 69.04636
wandb:                PPO_1202/global_step 212992
wandb:        PPO_1202/rollout/ep_len_mean 200.0
wandb:        PPO_1202/rollout/ep_rew_mean -423.01669
wandb:                   PPO_1202/time/fps 1141.0
wandb:            PPO_1202/train/approx_kl 0.02192
wandb:        PPO_1202/train/clip_fraction 0.26978
wandb:           PPO_1202/train/clip_range 0.2
wandb:         PPO_1202/train/entropy_loss -3.10655
wandb:   PPO_1202/train/explained_variance 0.9967
wandb:        PPO_1202/train/learning_rate 0.0003
wandb:                 PPO_1202/train/loss 8.33119
wandb: PPO_1202/train/policy_gradient_loss -0.00166
wandb:                  PPO_1202/train/std 0.37717
wandb:           PPO_1202/train/value_loss 16.68729
wandb:                    global_mean_eval -359.50565
wandb:                         global_step 212992
wandb:                       mean_reward_0 -366.65689
wandb:                       mean_reward_1 -363.67762
wandb:                      mean_reward_10 -365.90022
wandb:                      mean_reward_11 -352.87199
wandb:                      mean_reward_12 -350.27905
wandb:                      mean_reward_13 -340.10972
wandb:                      mean_reward_14 -361.10595
wandb:                      mean_reward_15 -341.36968
wandb:                      mean_reward_16 -346.69381
wandb:                      mean_reward_17 -369.23699
wandb:                      mean_reward_18 -382.84609
wandb:                      mean_reward_19 -372.20828
wandb:                       mean_reward_2 -356.49624
wandb:                      mean_reward_20 -366.55385
wandb:                      mean_reward_21 -341.61389
wandb:                      mean_reward_22 -366.68838
wandb:                      mean_reward_23 -362.12759
wandb:                      mean_reward_24 -366.37933
wandb:                      mean_reward_25 -358.15034
wandb:                      mean_reward_26 -371.47504
wandb:                      mean_reward_27 -355.19137
wandb:                      mean_reward_28 -373.67665
wandb:                      mean_reward_29 -372.56528
wandb:                       mean_reward_3 -334.86244
wandb:                      mean_reward_30 -356.51653
wandb:                      mean_reward_31 -350.80372
wandb:                      mean_reward_32 -353.17293
wandb:                      mean_reward_33 -363.25271
wandb:                      mean_reward_34 -352.19961
wandb:                      mean_reward_35 -377.6418
wandb:                       mean_reward_4 -342.52911
wandb:                       mean_reward_5 -372.40568
wandb:                       mean_reward_6 -357.46801
wandb:                       mean_reward_7 -375.87417
wandb:                       mean_reward_8 -350.46951
wandb:                       mean_reward_9 -351.13286
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -931.94031
wandb:                        std_reward_0 115.89513
wandb:                        std_reward_1 109.63733
wandb:                       std_reward_10 117.09353
wandb:                       std_reward_11 116.24516
wandb:                       std_reward_12 113.0151
wandb:                       std_reward_13 112.4244
wandb:                       std_reward_14 123.49972
wandb:                       std_reward_15 100.8525
wandb:                       std_reward_16 106.69157
wandb:                       std_reward_17 126.57376
wandb:                       std_reward_18 127.29333
wandb:                       std_reward_19 119.15283
wandb:                        std_reward_2 116.13506
wandb:                       std_reward_20 113.11483
wandb:                       std_reward_21 101.19329
wandb:                       std_reward_22 122.05421
wandb:                       std_reward_23 114.46882
wandb:                       std_reward_24 110.02731
wandb:                       std_reward_25 115.82819
wandb:                       std_reward_26 123.80979
wandb:                       std_reward_27 110.37146
wandb:                       std_reward_28 108.06505
wandb:                       std_reward_29 110.12748
wandb:                        std_reward_3 94.02416
wandb:                       std_reward_30 112.56716
wandb:                       std_reward_31 106.45361
wandb:                       std_reward_32 112.39167
wandb:                       std_reward_33 115.97323
wandb:                       std_reward_34 116.98897
wandb:                       std_reward_35 118.70519
wandb:                        std_reward_4 111.52134
wandb:                        std_reward_5 119.7748
wandb:                        std_reward_6 118.10891
wandb:                        std_reward_7 123.06819
wandb:                        std_reward_8 110.85624
wandb:                        std_reward_9 105.51659
wandb:                            time/fps 1178.0
wandb:                     train/approx_kl 0.01244
wandb:                 train/clip_fraction 0.15652
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7411
wandb:            train/explained_variance 0.96718
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 11.08948
wandb:          train/policy_gradient_loss -0.01304
wandb:                           train/std 0.84138
wandb:                    train/value_loss 21.77692
wandb: 
wandb: Synced azure-valley-27: https://wandb.ai/tidiane/meta_rl_context/runs/1mez24fw
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_012622-1mez24fw/logs

real	121m11.352s
user	1175m47.719s
sys	3m30.264s
+ mpirun python dev/automl/meta_rl/scripts/orig_impl/striker_baselines.py --context none
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_032708-260njzkv
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_032708-25ymiicf
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_032708-27vz0el0
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_032708-3d0g5b5s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-capybara-36
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/260njzkv
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_032708-3s3rvojv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-night-38
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/25ymiicf
wandb: Syncing run splendid-gorge-35
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/27vz0el0
wandb: Syncing run pious-star-36
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/3d0g5b5s
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_032708-n0hoact8
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_032708-1uc7k9re
wandb: Run `wandb offline` to turn off syncing.
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_032708-2znu6ayn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-shadow-33
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: Tracking run with wandb version 0.13.5
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/3s3rvojv
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_032708-jt2eiuao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-totem-39
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/n0hoact8
wandb: Syncing run legendary-snowflake-39
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/1uc7k9re
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Syncing run curious-universe-39
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/2znu6ayn
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_032708-1ol3e8yx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-planet-38
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/jt2eiuao
wandb: Syncing run deft-snow-34
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/1ol3e8yx
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1205/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1205/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1205/rollout/ep_rew_mean ▁▁▂▂▃▅▄▅▆▇██
wandb:                   PPO_1205/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1205/train/approx_kl ▅▄▁▆▆▄▆▄█▇▇
wandb:        PPO_1205/train/clip_fraction ▄▄▁▇▇▆▇▄▇▇█
wandb:           PPO_1205/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1205/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1205/train/explained_variance ▆▇▁█▆█▅▆▇▂▅
wandb:        PPO_1205/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1205/train/loss ▁▁▃▅▂▃▆▁▂▂█
wandb: PPO_1205/train/policy_gradient_loss ▁▆█▁▁▅▆▅▂▃▂
wandb:                  PPO_1205/train/std █▇▇▆▅▅▄▄▃▂▁
wandb:           PPO_1205/train/value_loss ▁▄▇▃▆▄▆▇▇▆█
wandb:                PPO_1214/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1214/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1214/rollout/ep_rew_mean ▁▂▃▃▃▄▄▄▅▆▆█
wandb:                   PPO_1214/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1214/train/approx_kl ▁▁▁▂▂▂▄▅█▆▆
wandb:        PPO_1214/train/clip_fraction ▁▃▃▁▂▃▅▇█▇▆
wandb:           PPO_1214/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1214/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1214/train/explained_variance ▃▅▁▄▃▁▁▂▃▅█
wandb:        PPO_1214/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1214/train/loss ▄▄▄▄▁█▅▁▅▂▄
wandb: PPO_1214/train/policy_gradient_loss ▃▁▂▇▄█▃▅▄▂▇
wandb:                  PPO_1214/train/std █▇▆▆▅▅▄▄▃▂▁
wandb:           PPO_1214/train/value_loss █▇▅▅▆▅▄▃▁▁▃
wandb:                PPO_1224/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1224/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1224/rollout/ep_rew_mean ▁▁▃▃▃▄▆▆▇█▇█
wandb:                   PPO_1224/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1224/train/approx_kl ▁▁▄▄▄▄▃▃▂▂█
wandb:        PPO_1224/train/clip_fraction ▂▁▆▄▄▇▅▅▄▅█
wandb:           PPO_1224/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1224/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1224/train/explained_variance ▅▇▆▁▆▅▇█▄▆▅
wandb:        PPO_1224/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1224/train/loss ▂▃█▆▂▁▄▁▂▂▁
wandb: PPO_1224/train/policy_gradient_loss ▂▁▄▃▄▄▆▅▅▆█
wandb:                  PPO_1224/train/std ██▇▆▅▅▄▃▃▂▁
wandb:           PPO_1224/train/value_loss ▆█▂▆▅▄▃▂▃▄▁
wandb:                PPO_1234/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1234/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1234/rollout/ep_rew_mean ▁▄▂▂▃▅▆▅█▅▆▇
wandb:                   PPO_1234/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1234/train/approx_kl ▂▇▃▄▃▇█▁▃▆█
wandb:        PPO_1234/train/clip_fraction ▃▇▂▂▂██▁▄▄▅
wandb:           PPO_1234/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1234/train/entropy_loss ▁▁▂▃▃▄▅▅▆▇█
wandb:   PPO_1234/train/explained_variance ▁▄▃▅▂▆▅▇▇██
wandb:        PPO_1234/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1234/train/loss ▆▂▂█▂▁▄▂▃▂▃
wandb: PPO_1234/train/policy_gradient_loss ▂▆▁▂▁▄▄▅▂▃█
wandb:                  PPO_1234/train/std ██▇▆▆▅▄▄▃▂▁
wandb:           PPO_1234/train/value_loss █▁▄▆█▄▂▃▅▇▃
wandb:                PPO_1244/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1244/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1244/rollout/ep_rew_mean ▁▅▆▄███▆▅▂▄▂
wandb:                   PPO_1244/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1244/train/approx_kl ▃▂▁▃▃▄▄▃█▂▆
wandb:        PPO_1244/train/clip_fraction ▂▃▄▃▄▄▅▂█▁▇
wandb:           PPO_1244/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1244/train/entropy_loss ▁▁▂▃▄▄▅▅▇▇█
wandb:   PPO_1244/train/explained_variance ▄▁▅▆▆▅▆▆▇██
wandb:        PPO_1244/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1244/train/loss ▁▁▂▄▅▄▇▅▄▄█
wandb: PPO_1244/train/policy_gradient_loss ▄▂▆▃▁▅▃▅█▇▇
wandb:                  PPO_1244/train/std ██▇▆▅▅▄▄▂▂▁
wandb:           PPO_1244/train/value_loss ▂▇▁▃▅▁▁▇▂█▄
wandb:                PPO_1254/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1254/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1254/rollout/ep_rew_mean ▅▁▄▅▅▅▁▆▂█▅▂
wandb:                   PPO_1254/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1254/train/approx_kl ▂▁▄▄▂▁█▃▃▄▄
wandb:        PPO_1254/train/clip_fraction ▃▃▇▆▅▂█▁███
wandb:           PPO_1254/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1254/train/entropy_loss ▁▁▃▄▄▄▅▅▆▇█
wandb:   PPO_1254/train/explained_variance ▁▄▃▆▇▇▇▃█▅▆
wandb:        PPO_1254/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1254/train/loss ▂█▄▂▃▂▁▇▂▁▂
wandb: PPO_1254/train/policy_gradient_loss ▄▁▂▅▄▅▅▁▅█▅
wandb:                  PPO_1254/train/std ██▆▆▅▅▄▄▃▂▁
wandb:           PPO_1254/train/value_loss ▅▇▆▅▃▆▄█▃▁▆
wandb:                PPO_1264/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1264/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1264/rollout/ep_rew_mean ▁▅▆▄▃▇▇████▇
wandb:                   PPO_1264/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1264/train/approx_kl ▃▇▁▆▇▄▇█▃▅▇
wandb:        PPO_1264/train/clip_fraction ▆▇▁▆▄▆▅█▇▆▇
wandb:           PPO_1264/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1264/train/entropy_loss ▁▂▂▃▃▄▄▅▅▇█
wandb:   PPO_1264/train/explained_variance ▇▄▁▅▆▅▅▇▅▆█
wandb:        PPO_1264/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1264/train/loss █▃▇▇█▄▅▅▄▃▁
wandb: PPO_1264/train/policy_gradient_loss ▃█▁▅▃▆▁▅▆█▇
wandb:                  PPO_1264/train/std █▇▇▆▆▅▅▄▄▂▁
wandb:           PPO_1264/train/value_loss ▁▂█▂▅▂▃▁▂▃▁
wandb:                PPO_1274/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1274/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1274/rollout/ep_rew_mean ▅▇█▄▄▆▅▆▂▂▁▃
wandb:                   PPO_1274/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1274/train/approx_kl ▃▂█▁▁▆▇▆▁▁▅
wandb:        PPO_1274/train/clip_fraction ▆▇█▁▄▇▅▇▄▅▅
wandb:           PPO_1274/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1274/train/entropy_loss ▁▂▂▃▄▅▆▆▆▇█
wandb:   PPO_1274/train/explained_variance ▂▄▅▇▁▅█▄▄▆▅
wandb:        PPO_1274/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1274/train/loss ▅▄▁▅▂▂▂▃▂█▆
wandb: PPO_1274/train/policy_gradient_loss ▃▆█▁▁▄▄▇▄▄▄
wandb:                  PPO_1274/train/std ██▇▆▅▄▃▃▃▂▁
wandb:           PPO_1274/train/value_loss ▄▃▁▅▆▃▄▅█▇█
wandb:                PPO_1284/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1284/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1284/rollout/ep_rew_mean ▅▅▆▇█▁▆▃▄▅▆▄
wandb:                   PPO_1284/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1284/train/approx_kl ▅█▄▅▃▁▃▁█▃▃
wandb:        PPO_1284/train/clip_fraction ▃▇▄▄▄▁▇▄▆█▄
wandb:           PPO_1284/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1284/train/entropy_loss ▁▁▃▄▆▆▆▇▇▇█
wandb:   PPO_1284/train/explained_variance ▄▁█▄▃▆▃▇▄▄▅
wandb:        PPO_1284/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1284/train/loss ▁▂▂▁▂▄▂▁▆▃█
wandb: PPO_1284/train/policy_gradient_loss ▁▇▆▃▄▄█▁█▆▇
wandb:                  PPO_1284/train/std █▇▆▄▂▃▃▂▁▂▁
wandb:           PPO_1284/train/value_loss ▆▄▁▅▄▄▆▅█▄█
wandb:                    global_mean_eval ▁▃▆▇██████
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▂▂█▇▇▇▇▇▆
wandb:                       mean_reward_1 ▁▄▅▇██████
wandb:                      mean_reward_10 ▁▃▇▇▇█████
wandb:                      mean_reward_11 ▁▂▇▆██████
wandb:                      mean_reward_12 ▁▁▂█▇▆▆▇▇▅
wandb:                      mean_reward_13 ▁▄▅▆▇▇████
wandb:                      mean_reward_14 ▁▄▆▆▇▇█▇██
wandb:                      mean_reward_15 ▁▄▇▇▇▇████
wandb:                      mean_reward_16 ▁▃▇▇▇█████
wandb:                      mean_reward_17 ▁▂▇▆██████
wandb:                      mean_reward_18 ▁▂▂█▇▇▇▇▇▆
wandb:                      mean_reward_19 ▁▄▅▇██████
wandb:                       mean_reward_2 ▁▄▆▆▇▇█▇██
wandb:                      mean_reward_20 ▁▄▆▆▇▇█▇██
wandb:                      mean_reward_21 ▁▄▇▇▇▇████
wandb:                      mean_reward_22 ▁▃▇▇██████
wandb:                      mean_reward_23 ▁▂▇▆██████
wandb:                      mean_reward_24 ▁▁▂█▇▇▆▇▇▆
wandb:                      mean_reward_25 ▁▄▅▆██████
wandb:                      mean_reward_26 ▁▄▆▆▇██▇██
wandb:                      mean_reward_27 ▁▄▇▇▇█████
wandb:                      mean_reward_28 ▁▃▇▇▇█████
wandb:                      mean_reward_29 ▁▃▇▆██████
wandb:                       mean_reward_3 ▁▄▇▇▇▇████
wandb:                      mean_reward_30 ▁▁▂█▇▆▆▆▇▅
wandb:                      mean_reward_31 ▁▄▅▆██████
wandb:                      mean_reward_32 ▁▄▆▆▇██▇██
wandb:                      mean_reward_33 ▁▄▇▇▇▇████
wandb:                      mean_reward_34 ▁▃▇▇▇█████
wandb:                      mean_reward_35 ▁▂▇▆██████
wandb:                       mean_reward_4 ▁▃▇▇██████
wandb:                       mean_reward_5 ▁▂▇▆██████
wandb:                       mean_reward_6 ▁▁▂█▇▇▆▇▇▅
wandb:                       mean_reward_7 ▁▄▅▆██████
wandb:                       mean_reward_8 ▁▄▆▆▇▇█▇██
wandb:                       mean_reward_9 ▁▄▇▇▇█████
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▄▄▅▅▇▆█
wandb:                        std_reward_0 █▁▃▁▂▁▂▂▁▃
wandb:                        std_reward_1 ▄▁▁▁▁▁▁▂▃█
wandb:                       std_reward_10 ▂█▁▂▂▁▂▁▁▁
wandb:                       std_reward_11 ▂█▂▃▁▁▂▁▁▂
wandb:                       std_reward_12 █▁▃▁▂▁▁▂▂▃
wandb:                       std_reward_13 ▅▁▁▂▂▁▁▃▄█
wandb:                       std_reward_14 ▄█▁▂▂▁▁▃▃▅
wandb:                       std_reward_15 ▂█▁▃▂▁▁▂▂▂
wandb:                       std_reward_16 ▂█▁▃▂▁▁▁▁▁
wandb:                       std_reward_17 ▂█▂▂▁▁▂▁▁▂
wandb:                       std_reward_18 █▁▃▁▂▁▂▂▂▃
wandb:                       std_reward_19 ▄▂▁▁▂▁▁▂▃█
wandb:                        std_reward_2 ▅█▁▂▂▁▁▃▃▅
wandb:                       std_reward_20 ▄█▁▂▂▁▁▃▃▅
wandb:                       std_reward_21 ▃█▁▃▂▁▁▂▂▂
wandb:                       std_reward_22 ▂█▁▃▂▁▂▁▁▁
wandb:                       std_reward_23 ▂█▁▃▁▁▂▁▁▂
wandb:                       std_reward_24 █▁▃▁▂▁▁▂▁▃
wandb:                       std_reward_25 ▅▁▁▁▂▁▁▃▃█
wandb:                       std_reward_26 ▅█▁▂▂▁▁▃▄▆
wandb:                       std_reward_27 ▂█▁▃▂▁▁▂▁▂
wandb:                       std_reward_28 ▂█▁▃▂▁▁▁▁▁
wandb:                       std_reward_29 ▂█▁▂▁▁▂▁▁▂
wandb:                        std_reward_3 ▂█▁▃▂▁▁▂▂▂
wandb:                       std_reward_30 █▁▃▁▂▁▂▃▂▃
wandb:                       std_reward_31 ▄▁▁▁▁▁▁▃▃█
wandb:                       std_reward_32 ▄█▁▂▂▁▁▃▃▆
wandb:                       std_reward_33 ▃█▁▃▂▁▁▂▂▂
wandb:                       std_reward_34 ▂█▁▂▂▁▁▁▁▁
wandb:                       std_reward_35 ▂█▂▃▁▁▂▁▁▂
wandb:                        std_reward_4 ▂█▁▃▂▁▁▁▁▁
wandb:                        std_reward_5 ▂█▂▃▁▁▁▁▁▂
wandb:                        std_reward_6 █▁▃▁▂▁▂▂▂▄
wandb:                        std_reward_7 ▅▂▁▁▂▁▁▃▃█
wandb:                        std_reward_8 ▅█▁▂▂▁▁▃▄▄
wandb:                        std_reward_9 ▃█▁▃▂▁▁▂▂▂
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▅█▁▂▄▅▅▆▇▆
wandb:                 train/clip_fraction ▂▃▄▃▄▁▃▅▆▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▄▆▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▃▃▂▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▅▆█▆▄▃▂▂▁▂
wandb:                           train/std █▇▇▇▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1205/global_step 212992
wandb:        PPO_1205/rollout/ep_len_mean 200.0
wandb:        PPO_1205/rollout/ep_rew_mean -805.2749
wandb:                   PPO_1205/time/fps 1203.0
wandb:            PPO_1205/train/approx_kl 0.01247
wandb:        PPO_1205/train/clip_fraction 0.1606
wandb:           PPO_1205/train/clip_range 0.2
wandb:         PPO_1205/train/entropy_loss -7.77934
wandb:   PPO_1205/train/explained_variance 0.95789
wandb:        PPO_1205/train/learning_rate 0.0003
wandb:                 PPO_1205/train/loss 65.84785
wandb: PPO_1205/train/policy_gradient_loss -0.00929
wandb:                  PPO_1205/train/std 0.73392
wandb:           PPO_1205/train/value_loss 97.94604
wandb:                PPO_1214/global_step 212992
wandb:        PPO_1214/rollout/ep_len_mean 200.0
wandb:        PPO_1214/rollout/ep_rew_mean -664.48822
wandb:                   PPO_1214/time/fps 1203.0
wandb:            PPO_1214/train/approx_kl 0.0146
wandb:        PPO_1214/train/clip_fraction 0.18329
wandb:           PPO_1214/train/clip_range 0.2
wandb:         PPO_1214/train/entropy_loss -7.00572
wandb:   PPO_1214/train/explained_variance 0.97086
wandb:        PPO_1214/train/learning_rate 0.0003
wandb:                 PPO_1214/train/loss 38.31045
wandb: PPO_1214/train/policy_gradient_loss -0.00718
wandb:                  PPO_1214/train/std 0.65751
wandb:           PPO_1214/train/value_loss 70.15537
wandb:                PPO_1224/global_step 212992
wandb:        PPO_1224/rollout/ep_len_mean 200.0
wandb:        PPO_1224/rollout/ep_rew_mean -576.60651
wandb:                   PPO_1224/time/fps 1197.0
wandb:            PPO_1224/train/approx_kl 0.01755
wandb:        PPO_1224/train/clip_fraction 0.214
wandb:           PPO_1224/train/clip_range 0.2
wandb:         PPO_1224/train/entropy_loss -6.1475
wandb:   PPO_1224/train/explained_variance 0.96407
wandb:        PPO_1224/train/learning_rate 0.0003
wandb:                 PPO_1224/train/loss 12.80777
wandb: PPO_1224/train/policy_gradient_loss -0.0033
wandb:                  PPO_1224/train/std 0.58186
wandb:           PPO_1224/train/value_loss 41.48785
wandb:                PPO_1234/global_step 212992
wandb:        PPO_1234/rollout/ep_len_mean 200.0
wandb:        PPO_1234/rollout/ep_rew_mean -543.07617
wandb:                   PPO_1234/time/fps 1195.0
wandb:            PPO_1234/train/approx_kl 0.0176
wandb:        PPO_1234/train/clip_fraction 0.2142
wandb:           PPO_1234/train/clip_range 0.2
wandb:         PPO_1234/train/entropy_loss -5.4342
wandb:   PPO_1234/train/explained_variance 0.97669
wandb:        PPO_1234/train/learning_rate 0.0003
wandb:                 PPO_1234/train/loss 28.95366
wandb: PPO_1234/train/policy_gradient_loss -0.00253
wandb:                  PPO_1234/train/std 0.5253
wandb:           PPO_1234/train/value_loss 54.49433
wandb:                PPO_1244/global_step 212992
wandb:        PPO_1244/rollout/ep_len_mean 200.0
wandb:        PPO_1244/rollout/ep_rew_mean -572.27435
wandb:                   PPO_1244/time/fps 1196.0
wandb:            PPO_1244/train/approx_kl 0.01822
wandb:        PPO_1244/train/clip_fraction 0.24073
wandb:           PPO_1244/train/clip_range 0.2
wandb:         PPO_1244/train/entropy_loss -5.05347
wandb:   PPO_1244/train/explained_variance 0.98451
wandb:        PPO_1244/train/learning_rate 0.0003
wandb:                 PPO_1244/train/loss 75.51214
wandb: PPO_1244/train/policy_gradient_loss -0.00242
wandb:                  PPO_1244/train/std 0.49787
wandb:           PPO_1244/train/value_loss 105.54469
wandb:                PPO_1254/global_step 212992
wandb:        PPO_1254/rollout/ep_len_mean 200.0
wandb:        PPO_1254/rollout/ep_rew_mean -563.04944
wandb:                   PPO_1254/time/fps 1195.0
wandb:            PPO_1254/train/approx_kl 0.01787
wandb:        PPO_1254/train/clip_fraction 0.23976
wandb:           PPO_1254/train/clip_range 0.2
wandb:         PPO_1254/train/entropy_loss -4.66117
wandb:   PPO_1254/train/explained_variance 0.98512
wandb:        PPO_1254/train/learning_rate 0.0003
wandb:                 PPO_1254/train/loss 59.09639
wandb: PPO_1254/train/policy_gradient_loss -0.00101
wandb:                  PPO_1254/train/std 0.47101
wandb:           PPO_1254/train/value_loss 196.36703
wandb:                PPO_1264/global_step 212992
wandb:        PPO_1264/rollout/ep_len_mean 200.0
wandb:        PPO_1264/rollout/ep_rew_mean -535.07129
wandb:                   PPO_1264/time/fps 1195.0
wandb:            PPO_1264/train/approx_kl 0.02103
wandb:        PPO_1264/train/clip_fraction 0.248
wandb:           PPO_1264/train/clip_range 0.2
wandb:         PPO_1264/train/entropy_loss -4.26442
wandb:   PPO_1264/train/explained_variance 0.99033
wandb:        PPO_1264/train/learning_rate 0.0003
wandb:                 PPO_1264/train/loss 24.61095
wandb: PPO_1264/train/policy_gradient_loss 0.00166
wandb:                  PPO_1264/train/std 0.44471
wandb:           PPO_1264/train/value_loss 171.54788
wandb:                PPO_1274/global_step 212992
wandb:        PPO_1274/rollout/ep_len_mean 200.0
wandb:        PPO_1274/rollout/ep_rew_mean -545.6203
wandb:                   PPO_1274/time/fps 1193.0
wandb:            PPO_1274/train/approx_kl 0.02305
wandb:        PPO_1274/train/clip_fraction 0.2357
wandb:           PPO_1274/train/clip_range 0.2
wandb:         PPO_1274/train/entropy_loss -3.79929
wandb:   PPO_1274/train/explained_variance 0.98725
wandb:        PPO_1274/train/learning_rate 0.0003
wandb:                 PPO_1274/train/loss 141.28677
wandb: PPO_1274/train/policy_gradient_loss 0.00194
wandb:                  PPO_1274/train/std 0.41544
wandb:           PPO_1274/train/value_loss 290.38177
wandb:                PPO_1284/global_step 212992
wandb:        PPO_1284/rollout/ep_len_mean 200.0
wandb:        PPO_1284/rollout/ep_rew_mean -560.60712
wandb:                   PPO_1284/time/fps 1195.0
wandb:            PPO_1284/train/approx_kl 0.01818
wandb:        PPO_1284/train/clip_fraction 0.24084
wandb:           PPO_1284/train/clip_range 0.2
wandb:         PPO_1284/train/entropy_loss -3.54834
wandb:   PPO_1284/train/explained_variance 0.98748
wandb:        PPO_1284/train/learning_rate 0.0003
wandb:                 PPO_1284/train/loss 699.79517
wandb: PPO_1284/train/policy_gradient_loss 0.00511
wandb:                  PPO_1284/train/std 0.40326
wandb:           PPO_1284/train/value_loss 408.62311
wandb:                    global_mean_eval -437.80984
wandb:                         global_step 212992
wandb:                       mean_reward_0 -459.0984
wandb:                       mean_reward_1 -327.45453
wandb:                      mean_reward_10 -484.30669
wandb:                      mean_reward_11 -490.8868
wandb:                      mean_reward_12 -480.16064
wandb:                      mean_reward_13 -309.9498
wandb:                      mean_reward_14 -404.61578
wandb:                      mean_reward_15 -458.29863
wandb:                      mean_reward_16 -484.55229
wandb:                      mean_reward_17 -491.3382
wandb:                      mean_reward_18 -463.6253
wandb:                      mean_reward_19 -331.07386
wandb:                       mean_reward_2 -404.09115
wandb:                      mean_reward_20 -404.82557
wandb:                      mean_reward_21 -458.57979
wandb:                      mean_reward_22 -483.27696
wandb:                      mean_reward_23 -490.88107
wandb:                      mean_reward_24 -453.90458
wandb:                      mean_reward_25 -320.36624
wandb:                      mean_reward_26 -406.60537
wandb:                      mean_reward_27 -459.19153
wandb:                      mean_reward_28 -484.57351
wandb:                      mean_reward_29 -491.7365
wandb:                       mean_reward_3 -458.97567
wandb:                      mean_reward_30 -461.58253
wandb:                      mean_reward_31 -320.03082
wandb:                      mean_reward_32 -408.97098
wandb:                      mean_reward_33 -457.88257
wandb:                      mean_reward_34 -484.26527
wandb:                      mean_reward_35 -492.14056
wandb:                       mean_reward_4 -484.21562
wandb:                       mean_reward_5 -492.09585
wandb:                       mean_reward_6 -469.6561
wandb:                       mean_reward_7 -320.94063
wandb:                       mean_reward_8 -406.63007
wandb:                       mean_reward_9 -460.37455
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -903.27844
wandb:                        std_reward_0 62.41127
wandb:                        std_reward_1 74.26109
wandb:                       std_reward_10 4.37166
wandb:                       std_reward_11 7.26388
wandb:                       std_reward_12 57.97595
wandb:                       std_reward_13 45.15359
wandb:                       std_reward_14 27.0521
wandb:                       std_reward_15 11.58439
wandb:                       std_reward_16 4.33233
wandb:                       std_reward_17 7.34452
wandb:                       std_reward_18 63.6336
wandb:                       std_reward_19 67.10001
wandb:                        std_reward_2 21.75933
wandb:                       std_reward_20 25.31844
wandb:                       std_reward_21 11.70671
wandb:                       std_reward_22 4.80294
wandb:                       std_reward_23 7.62653
wandb:                       std_reward_24 62.38097
wandb:                       std_reward_25 57.85785
wandb:                       std_reward_26 27.84791
wandb:                       std_reward_27 11.95338
wandb:                       std_reward_28 4.21385
wandb:                       std_reward_29 6.78733
wandb:                        std_reward_3 12.86711
wandb:                       std_reward_30 58.60251
wandb:                       std_reward_31 54.18466
wandb:                       std_reward_32 31.33932
wandb:                       std_reward_33 11.6258
wandb:                       std_reward_34 4.64285
wandb:                       std_reward_35 8.04228
wandb:                        std_reward_4 4.48645
wandb:                        std_reward_5 7.19947
wandb:                        std_reward_6 64.43883
wandb:                        std_reward_7 56.64435
wandb:                        std_reward_8 15.27768
wandb:                        std_reward_9 14.60557
wandb:                            time/fps 1153.0
wandb:                     train/approx_kl 0.01145
wandb:                 train/clip_fraction 0.14658
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.87514
wandb:            train/explained_variance 0.94396
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.10234
wandb:          train/policy_gradient_loss -0.01207
wandb:                           train/std 0.85747
wandb:                    train/value_loss 31.23324
wandb: 
wandb: Synced upbeat-planet-38: https://wandb.ai/tidiane/meta_rl_context/runs/jt2eiuao
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_032708-jt2eiuao/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1204/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1204/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1204/rollout/ep_rew_mean ▂▁▁▃▃▃▄▅▄▅▅█
wandb:                   PPO_1204/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1204/train/approx_kl ▁▂▇▇▆▃█▆▅▇▇
wandb:        PPO_1204/train/clip_fraction ▁▅▆▇▇▄█▅▇██
wandb:           PPO_1204/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1204/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1204/train/explained_variance █▅▆█▄▁▅▄▅▅▂
wandb:        PPO_1204/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1204/train/loss ▁▃▁▃▃▄▂▃▃█▄
wandb: PPO_1204/train/policy_gradient_loss ▇█▃▂▄▇▁▅▄▃▃
wandb:                  PPO_1204/train/std ██▇▆▅▅▄▃▃▂▁
wandb:           PPO_1204/train/value_loss ▁▄▂▃▁▇▅█▆▇█
wandb:                PPO_1215/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1215/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1215/rollout/ep_rew_mean ▁▁▂▂▄▄▆▆▆▇▇█
wandb:                   PPO_1215/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1215/train/approx_kl ▃▂▁▃▄▇▆▇██▆
wandb:        PPO_1215/train/clip_fraction ▂▁▂▁▃▅▅▆█▆▅
wandb:           PPO_1215/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1215/train/entropy_loss ▁▂▂▃▃▄▅▆▆▇█
wandb:   PPO_1215/train/explained_variance ▄▃▄▆▁▂▇▇█▆▆
wandb:        PPO_1215/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1215/train/loss ▅█▃▇▆▂▃▃▂▃▁
wandb: PPO_1215/train/policy_gradient_loss ▂▁▄▃▄▂▄▆▆█▇
wandb:                  PPO_1215/train/std █▇▇▆▆▅▄▃▃▂▁
wandb:           PPO_1215/train/value_loss ▆█▆▆▄▃▂▁▁▁▁
wandb:                PPO_1225/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1225/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1225/rollout/ep_rew_mean ▃▃▃▃▁▂▄▄█▅▃▁
wandb:                   PPO_1225/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1225/train/approx_kl ▅▇▂██▄▇▅▆▅▁
wandb:        PPO_1225/train/clip_fraction ▆▇▅▇▃▃▆▇█▂▁
wandb:           PPO_1225/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1225/train/entropy_loss ▁▂▃▄▄▅▆▆▇██
wandb:   PPO_1225/train/explained_variance ▁▅▆▆▄▄▆▇▇▅█
wandb:        PPO_1225/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1225/train/loss ▄▁▁▂▂▇▅▄▁▅█
wandb: PPO_1225/train/policy_gradient_loss ▁█▆▃▆█▆█▅▇▃
wandb:                  PPO_1225/train/std █▇▆▅▅▄▃▂▁▁▁
wandb:           PPO_1225/train/value_loss ▂▂▁▁▃▃▂▂▂▆█
wandb:                PPO_1235/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1235/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1235/rollout/ep_rew_mean █▆▄▄▂▁▅▆▄▅▃▄
wandb:                   PPO_1235/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1235/train/approx_kl ▄▄▄▃▂▁██▄▅▃
wandb:        PPO_1235/train/clip_fraction ▇▆▃▆▁▅▇▇▄█▃
wandb:           PPO_1235/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1235/train/entropy_loss ▁▁▂▃▃▄▅▆▇▇█
wandb:   PPO_1235/train/explained_variance ▅▇▁█▄▂▆▄▇▅▆
wandb:        PPO_1235/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1235/train/loss ▁▂▄▄▆▄▂▅█▂▅
wandb: PPO_1235/train/policy_gradient_loss ▅▇▂▆▃▄▁▂▅█▅
wandb:                  PPO_1235/train/std █▇▇▇▆▅▄▃▂▂▁
wandb:           PPO_1235/train/value_loss ▂▁▆▂█▇▆▃█▇▆
wandb:                PPO_1245/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1245/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1245/rollout/ep_rew_mean ▆▃▃▃▃▃█▆▁▁█▅
wandb:                   PPO_1245/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1245/train/approx_kl ▄▁▅▆▄▅██▅▄▅
wandb:        PPO_1245/train/clip_fraction ▄▁▃▃▃▄█▆▃▃█
wandb:           PPO_1245/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1245/train/entropy_loss ▁▁▁▂▃▃▃▄▄▅█
wandb:   PPO_1245/train/explained_variance ██▆▅▁▇▇▁▆▆▆
wandb:        PPO_1245/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1245/train/loss ▅▄▇█▃▁▃▂▂▂▁
wandb: PPO_1245/train/policy_gradient_loss ▁▆▅▆▅▅▇▆▇▄█
wandb:                  PPO_1245/train/std ██▇▆▆▆▆▅▅▄▁
wandb:           PPO_1245/train/value_loss ▁▅▆▆█▆▁▆▇█▆
wandb:                PPO_1255/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1255/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1255/rollout/ep_rew_mean ▄▄▅▆▃▆▇▇▁▆█▇
wandb:                   PPO_1255/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1255/train/approx_kl ▁▁▄▁▇▂▅▆▁▅█
wandb:        PPO_1255/train/clip_fraction ▄▁▄▂▅▄▆▅▁██
wandb:           PPO_1255/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1255/train/entropy_loss ▁▁▂▃▄▄▄▅▆▆█
wandb:   PPO_1255/train/explained_variance ▅▃▁▃▆█▂▅▃▅▅
wandb:        PPO_1255/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1255/train/loss ▁▁█▆▃█▂▂▃▄█
wandb: PPO_1255/train/policy_gradient_loss ▇▅▁▁▆▅▅▅▁▇█
wandb:                  PPO_1255/train/std ██▇▅▅▅▅▄▃▃▁
wandb:           PPO_1255/train/value_loss ▃▅▅▆▄▁▁▂█▁▂
wandb:                PPO_1265/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1265/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1265/rollout/ep_rew_mean █▇█▂▅▃▃▁▂▄▃▆
wandb:                   PPO_1265/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1265/train/approx_kl █▁▁▄▂▂▁▄▅█▆
wandb:        PPO_1265/train/clip_fraction █▄▄▅▁▄▂▃▆▅█
wandb:           PPO_1265/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1265/train/entropy_loss ▁▂▃▃▄▃▄▄▅▅█
wandb:   PPO_1265/train/explained_variance █▅▆█▅▇▁▆▇▂▆
wandb:        PPO_1265/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1265/train/loss ▁▂▅▁▁█▂▃▃▅▃
wandb: PPO_1265/train/policy_gradient_loss ▆▁▄▆█▆▂▅▄▄▄
wandb:                  PPO_1265/train/std █▇▆▆▅▆▅▅▄▃▁
wandb:           PPO_1265/train/value_loss ▁▄▅▅▇▅█▇▆▇▃
wandb:                PPO_1275/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1275/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1275/rollout/ep_rew_mean ▇▁▃█▅▄▆▄▆▄▇▆
wandb:                   PPO_1275/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1275/train/approx_kl ▆▂▁▅▆█▃▃▄▁▇
wandb:        PPO_1275/train/clip_fraction █▁▆▇▄▇▃▄▅▃▅
wandb:           PPO_1275/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1275/train/entropy_loss ▁▁▂▂▃▄▄▅▅▆█
wandb:   PPO_1275/train/explained_variance ▁▄█▂▃▅▄▅▃▇▇
wandb:        PPO_1275/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1275/train/loss ▁▆▃▁▁▂▂▁█▂▃
wandb: PPO_1275/train/policy_gradient_loss █▁▃█▇▃▄▇▃▂▅
wandb:                  PPO_1275/train/std ██▇▇▆▅▅▅▄▃▁
wandb:           PPO_1275/train/value_loss ▃█▅▁▄▃▅█▆▄▂
wandb:                PPO_1285/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1285/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1285/rollout/ep_rew_mean ▅▇█▆▁▅▃▅▄▄▆▅
wandb:                   PPO_1285/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1285/train/approx_kl ▂▁▅▃▅▅▅█▂▂▃
wandb:        PPO_1285/train/clip_fraction ▃▆█▄▆▇▇█▃▁▁
wandb:           PPO_1285/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1285/train/entropy_loss ▁▂▃▄▅▅▆▆▇▇█
wandb:   PPO_1285/train/explained_variance ▁▄▄▆▆▆▄▅█▅▇
wandb:        PPO_1285/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1285/train/loss █▁▂▂▃▄▇▂▁▂▃
wandb: PPO_1285/train/policy_gradient_loss ▃▄█▄▂▄▃▆▅▁▁
wandb:                  PPO_1285/train/std █▇▆▅▄▄▃▃▂▂▁
wandb:           PPO_1285/train/value_loss █▁▅▆▅▄▇▆▁█▅
wandb:                    global_mean_eval ▁▅████████
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▄█▅▃▄▅▆▆▇
wandb:                       mean_reward_1 ▁▄▇███▇▇▇▇
wandb:                      mean_reward_10 ▁▅▇███████
wandb:                      mean_reward_11 ▁▅████████
wandb:                      mean_reward_12 ▁▄█▅▄▅▆▆▆▇
wandb:                      mean_reward_13 ▁▄▇██▇▇▇▇▇
wandb:                      mean_reward_14 ▁▅▇███▇▇▇▇
wandb:                      mean_reward_15 ▁▅▇███████
wandb:                      mean_reward_16 ▁▅▇███████
wandb:                      mean_reward_17 ▁▅████████
wandb:                      mean_reward_18 ▁▄█▅▄▄▅▅▆▇
wandb:                      mean_reward_19 ▁▄▇██▇▇▇▇▇
wandb:                       mean_reward_2 ▁▅▇███▇▇▇▇
wandb:                      mean_reward_20 ▁▅▇███▇▇▇▇
wandb:                      mean_reward_21 ▁▅▇███████
wandb:                      mean_reward_22 ▁▅▇███████
wandb:                      mean_reward_23 ▁▅████████
wandb:                      mean_reward_24 ▁▄█▅▃▄▅▆▇▇
wandb:                      mean_reward_25 ▁▄▇███▇▇▇▇
wandb:                      mean_reward_26 ▁▅▇███▇▇▇▇
wandb:                      mean_reward_27 ▁▅▇███████
wandb:                      mean_reward_28 ▁▅▇███████
wandb:                      mean_reward_29 ▁▅████████
wandb:                       mean_reward_3 ▁▅▇███████
wandb:                      mean_reward_30 ▁▄█▅▃▄▅▆▆▇
wandb:                      mean_reward_31 ▁▄▇███▇▇▇▇
wandb:                      mean_reward_32 ▁▅▇███▇▇▇▇
wandb:                      mean_reward_33 ▁▅▇███████
wandb:                      mean_reward_34 ▁▅▇███████
wandb:                      mean_reward_35 ▁▅████████
wandb:                       mean_reward_4 ▁▅▇███████
wandb:                       mean_reward_5 ▁▅████████
wandb:                       mean_reward_6 ▁▄█▅▃▄▅▅▆▇
wandb:                       mean_reward_7 ▁▄▇██▇▇▇▇▇
wandb:                       mean_reward_8 ▁▅▇███▇▇▇▇
wandb:                       mean_reward_9 ▁▅▇███████
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▄▄▅▅▇▆█
wandb:                        std_reward_0 █▂▁▂▁▂▂▁▂▂
wandb:                        std_reward_1 █▂▁▁▂▂▂▂▂▁
wandb:                       std_reward_10 █▃▁▁▂▃▁▁▂▂
wandb:                       std_reward_11 ▆█▁▁▁▄▁▁▂▁
wandb:                       std_reward_12 █▃▁▂▂▂▂▂▃▂
wandb:                       std_reward_13 █▁▁▁▁▅▂▂▃▁
wandb:                       std_reward_14 █▂▁▁▁▄▁▂▂▂
wandb:                       std_reward_15 █▃▁▁▂▄▁▂▂▂
wandb:                       std_reward_16 █▃▁▁▂▆▁▁▂▂
wandb:                       std_reward_17 ▅█▁▁▁▄▁▁▁▁
wandb:                       std_reward_18 █▃▁▂▂▃▂▂▃▂
wandb:                       std_reward_19 █▁▁▁▁▅▂▂▃▁
wandb:                        std_reward_2 █▂▁▁▂▃▁▂▂▂
wandb:                       std_reward_20 █▂▁▁▁▅▁▂▂▂
wandb:                       std_reward_21 █▂▁▁▂▄▁▁▂▂
wandb:                       std_reward_22 █▃▁▁▂▇▁▂▂▂
wandb:                       std_reward_23 ▄█▁▁▁▃▁▁▁▁
wandb:                       std_reward_24 █▃▁▂▂▂▂▂▂▃
wandb:                       std_reward_25 █▂▁▁▁▄▂▂▃▁
wandb:                       std_reward_26 █▂▁▁▁▃▁▂▂▂
wandb:                       std_reward_27 █▂▁▁▁▂▁▁▂▂
wandb:                       std_reward_28 █▃▁▁▂▄▁▂▂▂
wandb:                       std_reward_29 ▅█▁▁▁▄▂▁▂▂
wandb:                        std_reward_3 █▂▁▁▂▃▁▂▂▂
wandb:                       std_reward_30 █▂▁▂▁▁▁▂▂▂
wandb:                       std_reward_31 █▁▁▁▁▃▂▂▂▁
wandb:                       std_reward_32 █▂▁▁▂▂▁▂▂▂
wandb:                       std_reward_33 █▂▁▁▂▄▁▂▂▂
wandb:                       std_reward_34 █▃▁▁▂▃▁▂▂▂
wandb:                       std_reward_35 ▇█▁▁▁▂▁▂▂▂
wandb:                        std_reward_4 █▂▁▁▁▄▁▁▂▂
wandb:                        std_reward_5 ▅█▁▁▁▄▁▁▂▂
wandb:                        std_reward_6 █▂▁▂▂▃▂▂▂▃
wandb:                        std_reward_7 █▂▁▁▁▅▂▂▂▁
wandb:                        std_reward_8 █▂▁▁▁▂▁▂▂▂
wandb:                        std_reward_9 █▂▁▁▁▄▁▁▂▂
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▅█▁▂▄▅▅▆▇▆
wandb:                 train/clip_fraction ▂▃▄▃▄▁▃▅▆▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▄▆▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▃▃▂▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▅▆█▆▄▃▂▂▁▂
wandb:                           train/std █▇▇▇▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1204/global_step 212992
wandb:        PPO_1204/rollout/ep_len_mean 200.0
wandb:        PPO_1204/rollout/ep_rew_mean -766.41443
wandb:                   PPO_1204/time/fps 1201.0
wandb:            PPO_1204/train/approx_kl 0.01136
wandb:        PPO_1204/train/clip_fraction 0.14548
wandb:           PPO_1204/train/clip_range 0.2
wandb:         PPO_1204/train/entropy_loss -7.78763
wandb:   PPO_1204/train/explained_variance 0.95962
wandb:        PPO_1204/train/learning_rate 0.0003
wandb:                 PPO_1204/train/loss 65.06133
wandb: PPO_1204/train/policy_gradient_loss -0.00892
wandb:                  PPO_1204/train/std 0.73537
wandb:           PPO_1204/train/value_loss 177.50426
wandb:                PPO_1215/global_step 212992
wandb:        PPO_1215/rollout/ep_len_mean 200.0
wandb:        PPO_1215/rollout/ep_rew_mean -589.49054
wandb:                   PPO_1215/time/fps 1195.0
wandb:            PPO_1215/train/approx_kl 0.0135
wandb:        PPO_1215/train/clip_fraction 0.18517
wandb:           PPO_1215/train/clip_range 0.2
wandb:         PPO_1215/train/entropy_loss -6.88918
wandb:   PPO_1215/train/explained_variance 0.96359
wandb:        PPO_1215/train/learning_rate 0.0003
wandb:                 PPO_1215/train/loss 13.75859
wandb: PPO_1215/train/policy_gradient_loss -0.00593
wandb:                  PPO_1215/train/std 0.64799
wandb:           PPO_1215/train/value_loss 53.65878
wandb:                PPO_1225/global_step 212992
wandb:        PPO_1225/rollout/ep_len_mean 200.0
wandb:        PPO_1225/rollout/ep_rew_mean -586.01599
wandb:                   PPO_1225/time/fps 1197.0
wandb:            PPO_1225/train/approx_kl 0.01191
wandb:        PPO_1225/train/clip_fraction 0.15842
wandb:           PPO_1225/train/clip_range 0.2
wandb:         PPO_1225/train/entropy_loss -6.37916
wandb:   PPO_1225/train/explained_variance 0.97763
wandb:        PPO_1225/train/learning_rate 0.0003
wandb:                 PPO_1225/train/loss 85.02012
wandb: PPO_1225/train/policy_gradient_loss -0.00525
wandb:                  PPO_1225/train/std 0.60272
wandb:           PPO_1225/train/value_loss 224.20891
wandb:                PPO_1235/global_step 212992
wandb:        PPO_1235/rollout/ep_len_mean 200.0
wandb:        PPO_1235/rollout/ep_rew_mean -573.11615
wandb:                   PPO_1235/time/fps 1199.0
wandb:            PPO_1235/train/approx_kl 0.01354
wandb:        PPO_1235/train/clip_fraction 0.16572
wandb:           PPO_1235/train/clip_range 0.2
wandb:         PPO_1235/train/entropy_loss -5.92667
wandb:   PPO_1235/train/explained_variance 0.98066
wandb:        PPO_1235/train/learning_rate 0.0003
wandb:                 PPO_1235/train/loss 93.41003
wandb: PPO_1235/train/policy_gradient_loss -0.00257
wandb:                  PPO_1235/train/std 0.56465
wandb:           PPO_1235/train/value_loss 292.11072
wandb:                PPO_1245/global_step 212992
wandb:        PPO_1245/rollout/ep_len_mean 200.0
wandb:        PPO_1245/rollout/ep_rew_mean -587.41986
wandb:                   PPO_1245/time/fps 1195.0
wandb:            PPO_1245/train/approx_kl 0.01302
wandb:        PPO_1245/train/clip_fraction 0.19999
wandb:           PPO_1245/train/clip_range 0.2
wandb:         PPO_1245/train/entropy_loss -5.58866
wandb:   PPO_1245/train/explained_variance 0.9806
wandb:        PPO_1245/train/learning_rate 0.0003
wandb:                 PPO_1245/train/loss 102.58804
wandb: PPO_1245/train/policy_gradient_loss -0.00052
wandb:                  PPO_1245/train/std 0.53807
wandb:           PPO_1245/train/value_loss 534.59552
wandb:                PPO_1255/global_step 212992
wandb:        PPO_1255/rollout/ep_len_mean 200.0
wandb:        PPO_1255/rollout/ep_rew_mean -566.57007
wandb:                   PPO_1255/time/fps 1194.0
wandb:            PPO_1255/train/approx_kl 0.01691
wandb:        PPO_1255/train/clip_fraction 0.21627
wandb:           PPO_1255/train/clip_range 0.2
wandb:         PPO_1255/train/entropy_loss -5.07658
wandb:   PPO_1255/train/explained_variance 0.98337
wandb:        PPO_1255/train/learning_rate 0.0003
wandb:                 PPO_1255/train/loss 369.5806
wandb: PPO_1255/train/policy_gradient_loss -0.00039
wandb:                  PPO_1255/train/std 0.49945
wandb:           PPO_1255/train/value_loss 398.09641
wandb:                PPO_1265/global_step 212992
wandb:        PPO_1265/rollout/ep_len_mean 200.0
wandb:        PPO_1265/rollout/ep_rew_mean -545.69714
wandb:                   PPO_1265/time/fps 1189.0
wandb:            PPO_1265/train/approx_kl 0.01682
wandb:        PPO_1265/train/clip_fraction 0.23752
wandb:           PPO_1265/train/clip_range 0.2
wandb:         PPO_1265/train/entropy_loss -4.67195
wandb:   PPO_1265/train/explained_variance 0.98133
wandb:        PPO_1265/train/learning_rate 0.0003
wandb:                 PPO_1265/train/loss 136.51187
wandb: PPO_1265/train/policy_gradient_loss -6e-05
wandb:                  PPO_1265/train/std 0.47252
wandb:           PPO_1265/train/value_loss 318.72162
wandb:                PPO_1275/global_step 212992
wandb:        PPO_1275/rollout/ep_len_mean 200.0
wandb:        PPO_1275/rollout/ep_rew_mean -530.84143
wandb:                   PPO_1275/time/fps 1192.0
wandb:            PPO_1275/train/approx_kl 0.01941
wandb:        PPO_1275/train/clip_fraction 0.23215
wandb:           PPO_1275/train/clip_range 0.2
wandb:         PPO_1275/train/entropy_loss -4.34057
wandb:   PPO_1275/train/explained_variance 0.98406
wandb:        PPO_1275/train/learning_rate 0.0003
wandb:                 PPO_1275/train/loss 132.52335
wandb: PPO_1275/train/policy_gradient_loss 0.00135
wandb:                  PPO_1275/train/std 0.44947
wandb:           PPO_1275/train/value_loss 176.00734
wandb:                PPO_1285/global_step 212992
wandb:        PPO_1285/rollout/ep_len_mean 200.0
wandb:        PPO_1285/rollout/ep_rew_mean -521.65717
wandb:                   PPO_1285/time/fps 1189.0
wandb:            PPO_1285/train/approx_kl 0.01871
wandb:        PPO_1285/train/clip_fraction 0.2222
wandb:           PPO_1285/train/clip_range 0.2
wandb:         PPO_1285/train/entropy_loss -3.96569
wandb:   PPO_1285/train/explained_variance 0.98722
wandb:        PPO_1285/train/learning_rate 0.0003
wandb:                 PPO_1285/train/loss 75.33188
wandb: PPO_1285/train/policy_gradient_loss 0.00022
wandb:                  PPO_1285/train/std 0.42712
wandb:           PPO_1285/train/value_loss 176.69337
wandb:                    global_mean_eval -457.19261
wandb:                         global_step 212992
wandb:                       mean_reward_0 -402.26806
wandb:                       mean_reward_1 -401.2081
wandb:                      mean_reward_10 -497.64192
wandb:                      mean_reward_11 -506.5726
wandb:                      mean_reward_12 -405.42843
wandb:                      mean_reward_13 -401.44989
wandb:                      mean_reward_14 -450.83252
wandb:                      mean_reward_15 -481.57192
wandb:                      mean_reward_16 -497.90762
wandb:                      mean_reward_17 -506.58164
wandb:                      mean_reward_18 -402.54593
wandb:                      mean_reward_19 -401.19902
wandb:                       mean_reward_2 -450.5175
wandb:                      mean_reward_20 -450.76696
wandb:                      mean_reward_21 -481.86438
wandb:                      mean_reward_22 -497.48536
wandb:                      mean_reward_23 -506.42191
wandb:                      mean_reward_24 -409.92687
wandb:                      mean_reward_25 -401.20762
wandb:                      mean_reward_26 -450.81294
wandb:                      mean_reward_27 -481.63637
wandb:                      mean_reward_28 -497.93473
wandb:                      mean_reward_29 -507.04155
wandb:                       mean_reward_3 -481.7625
wandb:                      mean_reward_30 -403.89078
wandb:                      mean_reward_31 -400.87704
wandb:                      mean_reward_32 -450.42453
wandb:                      mean_reward_33 -481.31828
wandb:                      mean_reward_34 -497.3866
wandb:                      mean_reward_35 -506.74366
wandb:                       mean_reward_4 -497.4769
wandb:                       mean_reward_5 -507.10342
wandb:                       mean_reward_6 -407.50753
wandb:                       mean_reward_7 -401.21901
wandb:                       mean_reward_8 -450.73775
wandb:                       mean_reward_9 -481.66211
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -903.27844
wandb:                        std_reward_0 42.52398
wandb:                        std_reward_1 2.14159
wandb:                       std_reward_10 2.47662
wandb:                       std_reward_11 2.41686
wandb:                       std_reward_12 46.44858
wandb:                       std_reward_13 2.4424
wandb:                       std_reward_14 2.87985
wandb:                       std_reward_15 2.66923
wandb:                       std_reward_16 2.4102
wandb:                       std_reward_17 2.46581
wandb:                       std_reward_18 43.24126
wandb:                       std_reward_19 2.82917
wandb:                        std_reward_2 2.91597
wandb:                       std_reward_20 2.79294
wandb:                       std_reward_21 2.10695
wandb:                       std_reward_22 2.50803
wandb:                       std_reward_23 2.27387
wandb:                       std_reward_24 52.47012
wandb:                       std_reward_25 2.43814
wandb:                       std_reward_26 2.62061
wandb:                       std_reward_27 2.21787
wandb:                       std_reward_28 2.44042
wandb:                       std_reward_29 2.47788
wandb:                        std_reward_3 2.29548
wandb:                       std_reward_30 46.00493
wandb:                       std_reward_31 2.94185
wandb:                       std_reward_32 2.87932
wandb:                       std_reward_33 2.24897
wandb:                       std_reward_34 2.42452
wandb:                       std_reward_35 2.39107
wandb:                        std_reward_4 2.49362
wandb:                        std_reward_5 2.71916
wandb:                        std_reward_6 49.01591
wandb:                        std_reward_7 2.64564
wandb:                        std_reward_8 2.87704
wandb:                        std_reward_9 2.0105
wandb:                            time/fps 1153.0
wandb:                     train/approx_kl 0.01145
wandb:                 train/clip_fraction 0.14658
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.87514
wandb:            train/explained_variance 0.94396
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.10234
wandb:          train/policy_gradient_loss -0.01207
wandb:                           train/std 0.85747
wandb:                    train/value_loss 31.23324
wandb: 
wandb: Synced flowing-totem-39: https://wandb.ai/tidiane/meta_rl_context/runs/n0hoact8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_032708-n0hoact8/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1207/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1207/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1207/rollout/ep_rew_mean ▁▂▂▂▄▃▄▇▇███
wandb:                   PPO_1207/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1207/train/approx_kl ▄▂▂▁▄▆▃▄███
wandb:        PPO_1207/train/clip_fraction ▁▁▂▂▅▆▅▅▇▇█
wandb:           PPO_1207/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1207/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1207/train/explained_variance ▁██▆▇▅▆▄▆▂▂
wandb:        PPO_1207/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1207/train/loss ▁█▁▃▃▇▇▅▄▁▂
wandb: PPO_1207/train/policy_gradient_loss ▁▅█▅▄▁▇▇▅▆█
wandb:                  PPO_1207/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1207/train/value_loss ▃▁▂▅▂▇▅█▅▂▂
wandb:                PPO_1216/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1216/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1216/rollout/ep_rew_mean ▂▁▁▁▄▄▅▆▅▇██
wandb:                   PPO_1216/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1216/train/approx_kl ▃▂▂▂▂▁▃█▅█▅
wandb:        PPO_1216/train/clip_fraction ▂▃▂▁▂▁▃█▅█▅
wandb:           PPO_1216/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1216/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1216/train/explained_variance ▄▄▃▅▁▄▆▃█▅▄
wandb:        PPO_1216/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1216/train/loss ▃▃█▅▆▆▄▁▂▂▂
wandb: PPO_1216/train/policy_gradient_loss ▃▁▄▄▇▇▅▄▅▆█
wandb:                  PPO_1216/train/std █▇▇▆▅▅▄▃▃▂▁
wandb:           PPO_1216/train/value_loss ▆▄█▅█▆▆▁▆▁▁
wandb:                PPO_1226/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1226/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1226/rollout/ep_rew_mean ▁▁▁▃▃▄▅▆▆▇▆█
wandb:                   PPO_1226/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1226/train/approx_kl ▁▄▄▇▂▇▃▇█▆▃
wandb:        PPO_1226/train/clip_fraction ▁▁▄▅▁▂▃▅▄█▄
wandb:           PPO_1226/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1226/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1226/train/explained_variance ▄▆▆▆▅▆█▇▂▄▁
wandb:        PPO_1226/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1226/train/loss ▁▃▄▁█▃▂▃▃▂▂
wandb: PPO_1226/train/policy_gradient_loss ▆▁▂▆▆▂▃▁▂▄█
wandb:                  PPO_1226/train/std █▇▆▆▅▄▄▃▂▂▁
wandb:           PPO_1226/train/value_loss ▃█▅▁▇▅▃▆▇▂▅
wandb:                PPO_1236/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1236/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1236/rollout/ep_rew_mean ▁▄▆▄▁▄▅▆▇▇█▅
wandb:                   PPO_1236/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1236/train/approx_kl ▁▂▅▂█▇▆▁▅▂▄
wandb:        PPO_1236/train/clip_fraction ▅▅▆▄▇█▇▁█▇▃
wandb:           PPO_1236/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1236/train/entropy_loss ▁▂▂▃▄▅▅▆▆▇█
wandb:   PPO_1236/train/explained_variance ▁▇▂▇█▇▆▅▆▁▅
wandb:        PPO_1236/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1236/train/loss ▄▁▂▇▆▂▁▄▃█▄
wandb: PPO_1236/train/policy_gradient_loss ▄▁▅▅▃▅▃▃█▅▁
wandb:                  PPO_1236/train/std █▇▇▆▅▄▃▃▃▂▁
wandb:           PPO_1236/train/value_loss ▂▃▂▃▇▃▁█▆▂▂
wandb:                PPO_1246/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1246/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1246/rollout/ep_rew_mean ▁▁▁▂▃▆▆▆▃▅▅█
wandb:                   PPO_1246/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1246/train/approx_kl ▁▅▄▄▄▅▄▃▂▇█
wandb:        PPO_1246/train/clip_fraction ▁▂▂▆▃▅▆▁▂▆█
wandb:           PPO_1246/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1246/train/entropy_loss ▁▂▂▃▄▅▅▆▇▇█
wandb:   PPO_1246/train/explained_variance ▁▃▄▅▄▅█▇▆▆█
wandb:        PPO_1246/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1246/train/loss ▇▄▅▁▃█▃▅▅▄▃
wandb: PPO_1246/train/policy_gradient_loss ▇▂▁▄▂▆▅▃▄▆█
wandb:                  PPO_1246/train/std █▇▇▆▅▄▃▃▃▂▁
wandb:           PPO_1246/train/value_loss ▆▅█▃▅▅▁▄█▅▁
wandb:                PPO_1256/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1256/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1256/rollout/ep_rew_mean ▁▃▅▆▅▆█▆▇▆▇▄
wandb:                   PPO_1256/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1256/train/approx_kl ▁▁▅▃▃▄█▄▄▇▃
wandb:        PPO_1256/train/clip_fraction ▄▄▆▆▁▅▆█▆▄▄
wandb:           PPO_1256/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1256/train/entropy_loss ▁▁▂▃▃▄▅▆▆▇█
wandb:   PPO_1256/train/explained_variance ▆▆▇▆▇▇▂▁█▆▇
wandb:        PPO_1256/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1256/train/loss ▁█▃▁▃▂▂▆▂▄▇
wandb: PPO_1256/train/policy_gradient_loss ▄▁▃▆▅▂▆█▂▅▄
wandb:                  PPO_1256/train/std ██▇▆▅▄▄▃▃▂▁
wandb:           PPO_1256/train/value_loss ▅▇▅▁▅▅█▃▃▅▇
wandb:                PPO_1266/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1266/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1266/rollout/ep_rew_mean ▂▄▃▄▄▅▆█▁▄▅▄
wandb:                   PPO_1266/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1266/train/approx_kl ▃▇▄█▂▇▆▁▁▄▄
wandb:        PPO_1266/train/clip_fraction ▅█▅▇▄█▆▆▁▅▄
wandb:           PPO_1266/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1266/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1266/train/explained_variance ▅▅█▆▆▇▄▄▅▁▄
wandb:        PPO_1266/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1266/train/loss ▁▃▁▁▂▁▁▂▃▂█
wandb: PPO_1266/train/policy_gradient_loss ▄▁▂▅▆▇▆▇▃█▁
wandb:                  PPO_1266/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1266/train/value_loss ▃▂▂▁▃▁▂▂▆▆█
wandb:                PPO_1276/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1276/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1276/rollout/ep_rew_mean ▄▂▁▃█▆▇▆▃▄▇▄
wandb:                   PPO_1276/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1276/train/approx_kl ▂▃▃▂▅▅▁█▂▄▄
wandb:        PPO_1276/train/clip_fraction ▂▆▁▄▆▄▃█▄▆▆
wandb:           PPO_1276/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1276/train/entropy_loss ▁▁▂▂▃▄▅▅▆▇█
wandb:   PPO_1276/train/explained_variance ▃█▁▁▇▅▅▆▂▄▁
wandb:        PPO_1276/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1276/train/loss ▃▂▂▇▁▂▃▁▃▁█
wandb: PPO_1276/train/policy_gradient_loss ▁▅▂▄▃▂▄▇▄█▄
wandb:                  PPO_1276/train/std ██▇▇▆▅▄▃▃▂▁
wandb:           PPO_1276/train/value_loss ▆▃█▆▁▃▅▂▅▆▇
wandb:                PPO_1286/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1286/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1286/rollout/ep_rew_mean ▄▅▃▃▁▃▃▂█▄▄▇
wandb:                   PPO_1286/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1286/train/approx_kl ▄▃▂▃▁▃▃▄▄█▂
wandb:        PPO_1286/train/clip_fraction ▂█▁▇▃▅▅█▆█▂
wandb:           PPO_1286/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1286/train/entropy_loss ▁▁▂▂▃▄▄▅▅▆█
wandb:   PPO_1286/train/explained_variance ▄█▇▄▇▆▅▄▁▄▂
wandb:        PPO_1286/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1286/train/loss ▃▂▁▂▃▁▁█▂▂▂
wandb: PPO_1286/train/policy_gradient_loss ▂▃▆▄▅▁▆▄█▆▇
wandb:                  PPO_1286/train/std ███▇▆▅▅▄▃▃▁
wandb:           PPO_1286/train/value_loss █▂▇▆▆▄▄▅▁▁▅
wandb:                    global_mean_eval ▁▃▅▆▇▇████
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▁▂▆████▇▇
wandb:                       mean_reward_1 ▂▁▄▅▆▆▇███
wandb:                      mean_reward_10 ▁▃▆▇▇▇████
wandb:                      mean_reward_11 ▁▃▆▆▇▇████
wandb:                      mean_reward_12 ▁▁▂▆████▇▇
wandb:                      mean_reward_13 ▂▁▄▅▆▆▇███
wandb:                      mean_reward_14 ▁▄▅▆▆▇▇███
wandb:                      mean_reward_15 ▁▄▆▇▇▇████
wandb:                      mean_reward_16 ▁▃▆▇▇▇████
wandb:                      mean_reward_17 ▁▃▆▆▇▇████
wandb:                      mean_reward_18 ▁▁▂▆████▇▇
wandb:                      mean_reward_19 ▁▁▄▅▆▆▇███
wandb:                       mean_reward_2 ▁▄▅▆▆▇▇███
wandb:                      mean_reward_20 ▁▄▅▆▆▇▇███
wandb:                      mean_reward_21 ▁▄▆▇▇▇▇███
wandb:                      mean_reward_22 ▁▃▆▇▇▇████
wandb:                      mean_reward_23 ▁▃▆▆▇▇███▇
wandb:                      mean_reward_24 ▁▁▂▆████▆▇
wandb:                      mean_reward_25 ▁▁▄▅▆▆▇███
wandb:                      mean_reward_26 ▁▄▅▆▆▇▇███
wandb:                      mean_reward_27 ▁▄▆▇▇▇████
wandb:                      mean_reward_28 ▁▃▆▇▇▇████
wandb:                      mean_reward_29 ▁▃▆▆▇▇████
wandb:                       mean_reward_3 ▁▄▆▇▇▇████
wandb:                      mean_reward_30 ▁▁▂▆████▆▇
wandb:                      mean_reward_31 ▂▁▄▅▆▆▇███
wandb:                      mean_reward_32 ▁▄▅▆▆▇▇███
wandb:                      mean_reward_33 ▁▄▆▇▇▇████
wandb:                      mean_reward_34 ▁▃▆▇▇▇████
wandb:                      mean_reward_35 ▁▃▆▆▇▇████
wandb:                       mean_reward_4 ▁▃▆▇▇▇███▇
wandb:                       mean_reward_5 ▁▃▆▆▇▇████
wandb:                       mean_reward_6 ▁▁▂▆████▆▇
wandb:                       mean_reward_7 ▂▁▄▅▆▆▇███
wandb:                       mean_reward_8 ▁▄▅▆▆▇▇███
wandb:                       mean_reward_9 ▁▄▆▇▇▇▇███
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▄▄▅▅▇▆█
wandb:                        std_reward_0 ▇▃▃▂▁▁▃▂▆█
wandb:                        std_reward_1 █▅▁▂▁▂▁▁▁▇
wandb:                       std_reward_10 ▄▃▁▂▁▂▂▁▁█
wandb:                       std_reward_11 ▄▄▁▁▂▂▂▁▁█
wandb:                       std_reward_12 ▇▂▃▂▁▁▂▂▆█
wandb:                       std_reward_13 █▄▁▂▁▂▁▁▁▆
wandb:                       std_reward_14 █▆▁▁▄▂▂▁▁▅
wandb:                       std_reward_15 ▄▂▁▁▃▁▂▁▁█
wandb:                       std_reward_16 ▄▃▁▂▂▂▂▁▁█
wandb:                       std_reward_17 ▄▃▁▁▂▂▂▁▁█
wandb:                       std_reward_18 █▃▃▂▁▁▃▂▆▆
wandb:                       std_reward_19 █▄▁▂▁▂▁▁▁▄
wandb:                        std_reward_2 █▅▁▁▃▁▂▁▁▆
wandb:                       std_reward_20 █▆▁▁▄▂▂▁▁▆
wandb:                       std_reward_21 ▇▃▁▁▆▂▂▁▁█
wandb:                       std_reward_22 ▃▂▁▂▁▂▁▁▁█
wandb:                       std_reward_23 ▃▃▁▁▁▂▂▁▁█
wandb:                       std_reward_24 █▃▃▂▂▁▃▂█▇
wandb:                       std_reward_25 █▄▁▂▁▂▁▁▁▃
wandb:                       std_reward_26 █▆▁▁▄▂▂▁▁█
wandb:                       std_reward_27 ▅▃▁▁▅▂▂▁▁█
wandb:                       std_reward_28 ▄▄▁▂▂▂▂▁▁█
wandb:                       std_reward_29 ▄▄▁▁▂▂▂▁▁█
wandb:                        std_reward_3 ▄▂▁▁▄▁▂▁▁█
wandb:                       std_reward_30 █▃▃▂▂▁▃▂▇▆
wandb:                       std_reward_31 █▅▁▂▁▂▁▁▁▂
wandb:                       std_reward_32 █▆▁▁▄▂▂▁▁▁
wandb:                       std_reward_33 █▃▁▂▆▂▂▁▁▁
wandb:                       std_reward_34 ▃▃▁▂▂▂▂▁▁█
wandb:                       std_reward_35 ▄▃▁▁▁▂▂▁▁█
wandb:                        std_reward_4 ▃▂▁▂▁▂▁▁▁█
wandb:                        std_reward_5 ▃▃▁▁▂▂▂▅▁█
wandb:                        std_reward_6 █▃▃▂▂▁▃▂▇█
wandb:                        std_reward_7 █▄▁▂▁▂▁▂▁▃
wandb:                        std_reward_8 █▅▁▁▃▂▂▁▁▂
wandb:                        std_reward_9 ▅▃▁▁▄▂▂▁▁█
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▅█▁▂▄▅▅▆▇▆
wandb:                 train/clip_fraction ▂▃▄▃▄▁▃▅▆▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▄▆▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▃▃▂▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▅▆█▆▄▃▂▂▁▂
wandb:                           train/std █▇▇▇▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1207/global_step 212992
wandb:        PPO_1207/rollout/ep_len_mean 200.0
wandb:        PPO_1207/rollout/ep_rew_mean -801.86603
wandb:                   PPO_1207/time/fps 1198.0
wandb:            PPO_1207/train/approx_kl 0.0114
wandb:        PPO_1207/train/clip_fraction 0.14545
wandb:           PPO_1207/train/clip_range 0.2
wandb:         PPO_1207/train/entropy_loss -7.9137
wandb:   PPO_1207/train/explained_variance 0.9505
wandb:        PPO_1207/train/learning_rate 0.0003
wandb:                 PPO_1207/train/loss 35.40438
wandb: PPO_1207/train/policy_gradient_loss -0.00701
wandb:                  PPO_1207/train/std 0.74801
wandb:           PPO_1207/train/value_loss 93.91075
wandb:                PPO_1216/global_step 212992
wandb:        PPO_1216/rollout/ep_len_mean 200.0
wandb:        PPO_1216/rollout/ep_rew_mean -693.40479
wandb:                   PPO_1216/time/fps 1190.0
wandb:            PPO_1216/train/approx_kl 0.01378
wandb:        PPO_1216/train/clip_fraction 0.17845
wandb:           PPO_1216/train/clip_range 0.2
wandb:         PPO_1216/train/entropy_loss -7.0129
wandb:   PPO_1216/train/explained_variance 0.96496
wandb:        PPO_1216/train/learning_rate 0.0003
wandb:                 PPO_1216/train/loss 17.15711
wandb: PPO_1216/train/policy_gradient_loss -0.00465
wandb:                  PPO_1216/train/std 0.65719
wandb:           PPO_1216/train/value_loss 64.93651
wandb:                PPO_1226/global_step 212992
wandb:        PPO_1226/rollout/ep_len_mean 200.0
wandb:        PPO_1226/rollout/ep_rew_mean -580.71161
wandb:                   PPO_1226/time/fps 1186.0
wandb:            PPO_1226/train/approx_kl 0.01445
wandb:        PPO_1226/train/clip_fraction 0.20764
wandb:           PPO_1226/train/clip_range 0.2
wandb:         PPO_1226/train/entropy_loss -6.18124
wandb:   PPO_1226/train/explained_variance 0.96164
wandb:        PPO_1226/train/learning_rate 0.0003
wandb:                 PPO_1226/train/loss 18.89764
wandb: PPO_1226/train/policy_gradient_loss -0.00255
wandb:                  PPO_1226/train/std 0.58586
wandb:           PPO_1226/train/value_loss 59.14081
wandb:                PPO_1236/global_step 212992
wandb:        PPO_1236/rollout/ep_len_mean 200.0
wandb:        PPO_1236/rollout/ep_rew_mean -570.59174
wandb:                   PPO_1236/time/fps 1187.0
wandb:            PPO_1236/train/approx_kl 0.01559
wandb:        PPO_1236/train/clip_fraction 0.20027
wandb:           PPO_1236/train/clip_range 0.2
wandb:         PPO_1236/train/entropy_loss -5.47799
wandb:   PPO_1236/train/explained_variance 0.95709
wandb:        PPO_1236/train/learning_rate 0.0003
wandb:                 PPO_1236/train/loss 25.40697
wandb: PPO_1236/train/policy_gradient_loss -0.00381
wandb:                  PPO_1236/train/std 0.53076
wandb:           PPO_1236/train/value_loss 59.93459
wandb:                PPO_1246/global_step 212992
wandb:        PPO_1246/rollout/ep_len_mean 200.0
wandb:        PPO_1246/rollout/ep_rew_mean -489.90472
wandb:                   PPO_1246/time/fps 1185.0
wandb:            PPO_1246/train/approx_kl 0.01847
wandb:        PPO_1246/train/clip_fraction 0.24537
wandb:           PPO_1246/train/clip_range 0.2
wandb:         PPO_1246/train/entropy_loss -4.88699
wandb:   PPO_1246/train/explained_variance 0.98646
wandb:        PPO_1246/train/learning_rate 0.0003
wandb:                 PPO_1246/train/loss 17.36936
wandb: PPO_1246/train/policy_gradient_loss -0.00064
wandb:                  PPO_1246/train/std 0.48676
wandb:           PPO_1246/train/value_loss 39.52348
wandb:                PPO_1256/global_step 212992
wandb:        PPO_1256/rollout/ep_len_mean 200.0
wandb:        PPO_1256/rollout/ep_rew_mean -504.86896
wandb:                   PPO_1256/time/fps 1183.0
wandb:            PPO_1256/train/approx_kl 0.017
wandb:        PPO_1256/train/clip_fraction 0.22128
wandb:           PPO_1256/train/clip_range 0.2
wandb:         PPO_1256/train/entropy_loss -4.33558
wandb:   PPO_1256/train/explained_variance 0.9884
wandb:        PPO_1256/train/learning_rate 0.0003
wandb:                 PPO_1256/train/loss 34.15127
wandb: PPO_1256/train/policy_gradient_loss -0.00164
wandb:                  PPO_1256/train/std 0.44928
wandb:           PPO_1256/train/value_loss 50.13847
wandb:                PPO_1266/global_step 212992
wandb:        PPO_1266/rollout/ep_len_mean 200.0
wandb:        PPO_1266/rollout/ep_rew_mean -473.42331
wandb:                   PPO_1266/time/fps 1183.0
wandb:            PPO_1266/train/approx_kl 0.01724
wandb:        PPO_1266/train/clip_fraction 0.21964
wandb:           PPO_1266/train/clip_range 0.2
wandb:         PPO_1266/train/entropy_loss -3.8388
wandb:   PPO_1266/train/explained_variance 0.98193
wandb:        PPO_1266/train/learning_rate 0.0003
wandb:                 PPO_1266/train/loss 133.59636
wandb: PPO_1266/train/policy_gradient_loss -0.00372
wandb:                  PPO_1266/train/std 0.41892
wandb:           PPO_1266/train/value_loss 146.39839
wandb:                PPO_1276/global_step 212992
wandb:        PPO_1276/rollout/ep_len_mean 200.0
wandb:        PPO_1276/rollout/ep_rew_mean -469.83853
wandb:                   PPO_1276/time/fps 1179.0
wandb:            PPO_1276/train/approx_kl 0.01765
wandb:        PPO_1276/train/clip_fraction 0.24415
wandb:           PPO_1276/train/clip_range 0.2
wandb:         PPO_1276/train/entropy_loss -3.46681
wandb:   PPO_1276/train/explained_variance 0.98333
wandb:        PPO_1276/train/learning_rate 0.0003
wandb:                 PPO_1276/train/loss 215.29788
wandb: PPO_1276/train/policy_gradient_loss -0.00069
wandb:                  PPO_1276/train/std 0.39764
wandb:           PPO_1276/train/value_loss 144.79266
wandb:                PPO_1286/global_step 212992
wandb:        PPO_1286/rollout/ep_len_mean 200.0
wandb:        PPO_1286/rollout/ep_rew_mean -432.93704
wandb:                   PPO_1286/time/fps 1177.0
wandb:            PPO_1286/train/approx_kl 0.0175
wandb:        PPO_1286/train/clip_fraction 0.22968
wandb:           PPO_1286/train/clip_range 0.2
wandb:         PPO_1286/train/entropy_loss -3.16468
wandb:   PPO_1286/train/explained_variance 0.98013
wandb:        PPO_1286/train/learning_rate 0.0003
wandb:                 PPO_1286/train/loss 50.78264
wandb: PPO_1286/train/policy_gradient_loss 0.00103
wandb:                  PPO_1286/train/std 0.38098
wandb:           PPO_1286/train/value_loss 133.14349
wandb:                    global_mean_eval -404.48348
wandb:                         global_step 212992
wandb:                       mean_reward_0 -381.19205
wandb:                       mean_reward_1 -321.18979
wandb:                      mean_reward_10 -454.91588
wandb:                      mean_reward_11 -475.00098
wandb:                      mean_reward_12 -377.13024
wandb:                      mean_reward_13 -316.35704
wandb:                      mean_reward_14 -375.20476
wandb:                      mean_reward_15 -430.49641
wandb:                      mean_reward_16 -453.60198
wandb:                      mean_reward_17 -472.91671
wandb:                      mean_reward_18 -376.90953
wandb:                      mean_reward_19 -314.18462
wandb:                       mean_reward_2 -378.29182
wandb:                      mean_reward_20 -377.35045
wandb:                      mean_reward_21 -418.60828
wandb:                      mean_reward_22 -461.29113
wandb:                      mean_reward_23 -482.46449
wandb:                      mean_reward_24 -374.19964
wandb:                      mean_reward_25 -313.57611
wandb:                      mean_reward_26 -381.02692
wandb:                      mean_reward_27 -424.92517
wandb:                      mean_reward_28 -451.83873
wandb:                      mean_reward_29 -473.76661
wandb:                       mean_reward_3 -430.81635
wandb:                      mean_reward_30 -373.96533
wandb:                      mean_reward_31 -309.55131
wandb:                      mean_reward_32 -371.04325
wandb:                      mean_reward_33 -414.92492
wandb:                      mean_reward_34 -457.66058
wandb:                      mean_reward_35 -474.32786
wandb:                       mean_reward_4 -467.7891
wandb:                       mean_reward_5 -479.11876
wandb:                       mean_reward_6 -382.94939
wandb:                       mean_reward_7 -314.14608
wandb:                       mean_reward_8 -371.69426
wandb:                       mean_reward_9 -426.97873
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -903.27844
wandb:                        std_reward_0 103.2776
wandb:                        std_reward_1 74.47256
wandb:                       std_reward_10 54.46055
wandb:                       std_reward_11 53.98617
wandb:                       std_reward_12 109.1418
wandb:                       std_reward_13 69.30443
wandb:                       std_reward_14 32.80403
wandb:                       std_reward_15 70.24595
wandb:                       std_reward_16 50.31322
wandb:                       std_reward_17 49.82934
wandb:                       std_reward_18 67.17812
wandb:                       std_reward_19 49.47941
wandb:                        std_reward_2 49.29085
wandb:                       std_reward_20 46.57595
wandb:                       std_reward_21 38.81507
wandb:                       std_reward_22 72.96314
wandb:                       std_reward_23 72.4153
wandb:                       std_reward_24 71.25402
wandb:                       std_reward_25 42.81334
wandb:                       std_reward_26 63.07029
wandb:                       std_reward_27 54.60805
wandb:                       std_reward_28 41.7165
wandb:                       std_reward_29 45.62225
wandb:                        std_reward_3 72.04532
wandb:                       std_reward_30 72.7008
wandb:                       std_reward_31 12.18461
wandb:                       std_reward_32 4.78775
wandb:                       std_reward_33 1.99163
wandb:                       std_reward_34 54.55365
wandb:                       std_reward_35 52.80401
wandb:                        std_reward_4 83.73364
wandb:                        std_reward_5 63.00008
wandb:                        std_reward_6 93.62813
wandb:                        std_reward_7 35.83106
wandb:                        std_reward_8 10.74872
wandb:                        std_reward_9 58.13689
wandb:                            time/fps 1153.0
wandb:                     train/approx_kl 0.01145
wandb:                 train/clip_fraction 0.14658
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.87514
wandb:            train/explained_variance 0.94396
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.10234
wandb:          train/policy_gradient_loss -0.01207
wandb:                           train/std 0.85747
wandb:                    train/value_loss 31.23324
wandb: 
wandb: Synced efficient-shadow-33: https://wandb.ai/tidiane/meta_rl_context/runs/3s3rvojv
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_032708-3s3rvojv/logs
wandb: Waiting for W&B process to finish... (success).
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1208/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1208/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1208/rollout/ep_rew_mean ▂▁▁▁▂▄▃▄▆▇▆█
wandb:                   PPO_1208/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1208/train/approx_kl ▁▅▆▅▂▄█▇▇▅▇
wandb:        PPO_1208/train/clip_fraction ▁█▅▅▅▅▇▇▄▇█
wandb:           PPO_1208/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1208/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1208/train/explained_variance ▁▆▇▇▅▇█▇▄▅▇
wandb:        PPO_1208/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1208/train/loss ▃▄▁▂▁▂▁▁▃█▂
wandb: PPO_1208/train/policy_gradient_loss ▅▁▅▂▆▂▃▃█▄▅
wandb:                  PPO_1208/train/std █▇▇▆▅▅▄▃▂▂▁
wandb:           PPO_1208/train/value_loss ▇▁▃▂▄▆▂▂█▆▅
wandb:                PPO_1218/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1218/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1218/rollout/ep_rew_mean ▁▂▄▄▅▆▆▆▇▇▇█
wandb:                   PPO_1218/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1218/train/approx_kl ▂▁▅▄▅▅▆▃▂▅█
wandb:        PPO_1218/train/clip_fraction ▁▃▅▅▆█▆▄▄▄█
wandb:           PPO_1218/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1218/train/entropy_loss ▁▂▂▃▄▄▅▆▆▆█
wandb:   PPO_1218/train/explained_variance ▁▄▄▆▄▃█▇▇█▇
wandb:        PPO_1218/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1218/train/loss ▂▃▅▂▃▁▃█▁▂▁
wandb: PPO_1218/train/policy_gradient_loss ▄▃▁▃▃▆▄▆█▆▅
wandb:                  PPO_1218/train/std █▇▇▆▅▅▄▃▃▂▁
wandb:           PPO_1218/train/value_loss █▇▆▄▄▂▃▂▂▃▁
wandb:                PPO_1228/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1228/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1228/rollout/ep_rew_mean ▁▃▃▅▅▄▅▇▆▅▅█
wandb:                   PPO_1228/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1228/train/approx_kl ▂▅▂▁▅▂▁█▄▆▂
wandb:        PPO_1228/train/clip_fraction ▂▅▄▂▃▁▅█▁▄▃
wandb:           PPO_1228/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1228/train/entropy_loss ▁▂▂▃▃▄▄▅▆▇█
wandb:   PPO_1228/train/explained_variance ▁▇▅█▆▇▇█▇▂▅
wandb:        PPO_1228/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1228/train/loss ▄▄█▁▁▂▂▂▃▅▂
wandb: PPO_1228/train/policy_gradient_loss ▇▂▄▄▅▁█▇▄▆▇
wandb:                  PPO_1228/train/std █▇▇▆▆▅▅▄▂▂▁
wandb:           PPO_1228/train/value_loss ▆▃▂▁▂▅▆▁▄█▃
wandb:                PPO_1238/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1238/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1238/rollout/ep_rew_mean ▂▁▁▅▂▄▅▃█▄▅█
wandb:                   PPO_1238/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1238/train/approx_kl ▅▂▂▁▄▃█▄▆▁▃
wandb:        PPO_1238/train/clip_fraction ▆▃▆▅▅▃█▃▅▁▇
wandb:           PPO_1238/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1238/train/entropy_loss ▁▂▂▃▃▄▅▅▆▇█
wandb:   PPO_1238/train/explained_variance ▆▁▅▃▄▄▃▆▆▅█
wandb:        PPO_1238/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1238/train/loss ▁█▄▂▂▂▁▂▄▂▃
wandb: PPO_1238/train/policy_gradient_loss █▆▆▇█▁▆▄▅▄▅
wandb:                  PPO_1238/train/std ██▇▆▆▅▄▄▃▂▁
wandb:           PPO_1238/train/value_loss ▃▆▁▅▇▃▂▃▂█▃
wandb:                PPO_1248/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1248/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1248/rollout/ep_rew_mean ▃▄▅▃▁▃▄█▆▄█▆
wandb:                   PPO_1248/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1248/train/approx_kl ▃▅▁▂▃▅▅▅▄▄█
wandb:        PPO_1248/train/clip_fraction ▅█▅▁▄▆▇▇▆▅▇
wandb:           PPO_1248/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1248/train/entropy_loss ▁▂▂▂▃▃▄▅▅▆█
wandb:   PPO_1248/train/explained_variance ▃▁▃▃▅▆▇▆▃█▇
wandb:        PPO_1248/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1248/train/loss ▁▆█▆▃▆▁▃▄▂▄
wandb: PPO_1248/train/policy_gradient_loss ▅▅▆▅▁▅▄▁█▅▂
wandb:                  PPO_1248/train/std ███▇▆▆▅▄▄▃▁
wandb:           PPO_1248/train/value_loss ▃▃▃█▄▃▃▁▆▃▂
wandb:                PPO_1258/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1258/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1258/rollout/ep_rew_mean ▃▃▁▅▆▆▁▂▃▃▅█
wandb:                   PPO_1258/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1258/train/approx_kl █▁▂▅▇▃▁▇▂▄▄
wandb:        PPO_1258/train/clip_fraction ▆▂▆▆▄▁▃█▁▇▃
wandb:           PPO_1258/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1258/train/entropy_loss ▁▂▂▃▄▅▅▆▆▇█
wandb:   PPO_1258/train/explained_variance ▁▇█▆▅▇█▆▇▆█
wandb:        PPO_1258/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1258/train/loss ▇▁▁▁▆█▁█▃▇▁
wandb: PPO_1258/train/policy_gradient_loss ▁▄█▂▃▁▄▄▄▃▅
wandb:                  PPO_1258/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1258/train/value_loss ▆▅▂▁▂▇█▃██▅
wandb:                PPO_1268/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1268/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1268/rollout/ep_rew_mean ▇█▇▆▅▆▇▇▇▁▅▆
wandb:                   PPO_1268/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1268/train/approx_kl ▁▃▁▃▄▁▂▂▄▂█
wandb:        PPO_1268/train/clip_fraction ▇▇▇▃█▄▆▂▄▁█
wandb:           PPO_1268/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1268/train/entropy_loss ▁▂▂▄▄▄▅▅▅▆█
wandb:   PPO_1268/train/explained_variance ▆▅▅▅▇▁▅▆▆▇█
wandb:        PPO_1268/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1268/train/loss ▂█▁▄▃▃▇▂▇▁▂
wandb: PPO_1268/train/policy_gradient_loss ▄▅█▁▂▇█▄▅▃█
wandb:                  PPO_1268/train/std █▇▇▅▆▄▄▄▄▂▁
wandb:           PPO_1268/train/value_loss ▁▃▄▄▄▇▄▃▆█▅
wandb:                PPO_1278/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1278/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1278/rollout/ep_rew_mean ▁▂▄▅▆▄▃▂▃▅▅█
wandb:                   PPO_1278/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1278/train/approx_kl ▆▁▃▆▅▂▃█▄▄▃
wandb:        PPO_1278/train/clip_fraction ▂▁▅█▂▁▆▃▃▆▁
wandb:           PPO_1278/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1278/train/entropy_loss ▁▂▃▄▄▅▆▆▇▇█
wandb:   PPO_1278/train/explained_variance ▆▆▅█▁▅▆▃▄▂▆
wandb:        PPO_1278/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1278/train/loss ▃▃▃▁█▅▂▂▄▂▂
wandb: PPO_1278/train/policy_gradient_loss ▄▁▃█▆▁▆█▅▆▅
wandb:                  PPO_1278/train/std █▇▆▅▅▄▃▃▂▂▁
wandb:           PPO_1278/train/value_loss ██▃▂▄▇▂▁▁▁▃
wandb:                PPO_1288/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1288/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1288/rollout/ep_rew_mean ▂▁▄█▄▂▃▃▃▅▃▄
wandb:                   PPO_1288/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1288/train/approx_kl ▂▃▅▁▃█▁▇▆▂▁
wandb:        PPO_1288/train/clip_fraction ▁▃▅▃▄█▅▃▆▄▄
wandb:           PPO_1288/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1288/train/entropy_loss ▁▂▂▃▃▄▅▅▆▇█
wandb:   PPO_1288/train/explained_variance ▂▆▃▅▂▅▆█▅▁█
wandb:        PPO_1288/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1288/train/loss ▁▂▄▃▃▂▁▁▁█▂
wandb: PPO_1288/train/policy_gradient_loss ▂▆▅▃▁▃▅▆▇▅█
wandb:                  PPO_1288/train/std █▇▇▆▇▅▅▄▃▁▁
wandb:           PPO_1288/train/value_loss ▃▄▂▃▃▁▃▄▄█▁
wandb:                    global_mean_eval ▁▄▅▆▇▇████
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▃▁▄█▇▇███▇
wandb:                       mean_reward_1 ▁▄▅▆▇▇▇▇██
wandb:                      mean_reward_10 ▁▅▆▆▇█████
wandb:                      mean_reward_11 ▁▅▆▆▇█████
wandb:                      mean_reward_12 ▄▁▄█▇▇███▇
wandb:                      mean_reward_13 ▁▄▆▆▇▇▇███
wandb:                      mean_reward_14 ▁▆▆▆▇▇████
wandb:                      mean_reward_15 ▁▅▆▆▇▇████
wandb:                      mean_reward_16 ▁▅▆▆▇█████
wandb:                      mean_reward_17 ▁▅▆▆▇█████
wandb:                      mean_reward_18 ▃▁▄█▇▇██▇█
wandb:                      mean_reward_19 ▁▄▅▅▇▇▇▇██
wandb:                       mean_reward_2 ▁▆▆▆▇▇████
wandb:                      mean_reward_20 ▁▆▆▆▇▇████
wandb:                      mean_reward_21 ▁▅▆▆▇▇████
wandb:                      mean_reward_22 ▁▅▆▆▇█████
wandb:                      mean_reward_23 ▁▅▆▆▇█████
wandb:                      mean_reward_24 ▄▁▄█▇▇████
wandb:                      mean_reward_25 ▁▃▅▅▇▇▇▇██
wandb:                      mean_reward_26 ▁▆▆▆▇▇████
wandb:                      mean_reward_27 ▁▅▆▆▇▇████
wandb:                      mean_reward_28 ▁▅▆▆▇█████
wandb:                      mean_reward_29 ▁▅▆▆▇█████
wandb:                       mean_reward_3 ▁▅▆▆▇▇████
wandb:                      mean_reward_30 ▄▁▄█▇▇███▇
wandb:                      mean_reward_31 ▁▄▅▆▇▇▇▇██
wandb:                      mean_reward_32 ▁▆▆▆▇▇████
wandb:                      mean_reward_33 ▁▅▆▆▇▇████
wandb:                      mean_reward_34 ▁▅▆▆▇█████
wandb:                      mean_reward_35 ▁▅▆▆▇█████
wandb:                       mean_reward_4 ▁▅▆▆▇█████
wandb:                       mean_reward_5 ▁▅▆▆▇█████
wandb:                       mean_reward_6 ▃▁▄█▇▇██▇█
wandb:                       mean_reward_7 ▁▃▅▅▇▇▇▇██
wandb:                       mean_reward_8 ▁▆▆▆▇▇████
wandb:                       mean_reward_9 ▁▅▆▆▇▇████
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▄▄▅▅▇▆█
wandb:                        std_reward_0 █▁▃▁▂▁▂▁▁▂
wandb:                        std_reward_1 █▅▁▂▂▁▁▁▁▁
wandb:                       std_reward_10 ▇▅▄▆█▅▁▆▃▁
wandb:                       std_reward_11 ▃█▂▃▃▂▁▆▂▄
wandb:                       std_reward_12 █▁▃▁▁▁▂▁▁▂
wandb:                       std_reward_13 █▆▁▂▂▁▂▁▂▁
wandb:                       std_reward_14 ▇█▆▄▅▁▁▁▂▁
wandb:                       std_reward_15 █▆▄█▂▃▁▁▃▁
wandb:                       std_reward_16 █▅▄▅█▅▁▂▃▂
wandb:                       std_reward_17 ▄█▂▃▄▂▁▇▂▄
wandb:                       std_reward_18 █▁▃▁▂▁▂▁▁▂
wandb:                       std_reward_19 █▅▁▂▂▁▁▁▁▁
wandb:                        std_reward_2 ██▆▄▆▁▁▁▂▁
wandb:                       std_reward_20 ▇█▅▅▆▁▁▁▂▁
wandb:                       std_reward_21 █▅▃▇▁▃▁▁▂▁
wandb:                       std_reward_22 █▄▃▅▇▄▁▄▂▂
wandb:                       std_reward_23 ▃█▂▃▃▂▁▆▂▅
wandb:                       std_reward_24 █▁▃▁▂▁▂▁▁▂
wandb:                       std_reward_25 █▆▁▂▂▁▂▁▁▁
wandb:                       std_reward_26 ▇█▆▄▅▁▁▁▂▁
wandb:                       std_reward_27 █▅▃▇▁▃▁▁▂▁
wandb:                       std_reward_28 █▄▄▆▆▄▁▅▃▂
wandb:                       std_reward_29 ▃█▂▃▄▂▁▇▂▅
wandb:                        std_reward_3 █▅▄▇▁▃▁▁▂▁
wandb:                       std_reward_30 █▁▃▁▂▁▂▁▁▂
wandb:                       std_reward_31 █▆▁▂▂▁▁▁▁▁
wandb:                       std_reward_32 █▇▅▄▆▁▁▁▂▁
wandb:                       std_reward_33 █▅▃▇▁▃▁▁▂▁
wandb:                       std_reward_34 █▅▄▆█▅▁▄▃▂
wandb:                       std_reward_35 ▃█▂▃▄▂▁▇▂▄
wandb:                        std_reward_4 █▅▄▆█▄▁▂▃▂
wandb:                        std_reward_5 ▄█▂▃▄▂▁▆▂▅
wandb:                        std_reward_6 █▁▃▁▁▁▂▁▁▂
wandb:                        std_reward_7 █▄▁▂▂▁▁▁▁▁
wandb:                        std_reward_8 ██▆▄▆▁▁▁▂▁
wandb:                        std_reward_9 █▅▃▆▁▃▁▁▂▁
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▅█▁▂▄▅▅▆▇▆
wandb:                 train/clip_fraction ▂▃▄▃▄▁▃▅▆▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▄▆▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▃▃▂▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▅▆█▆▄▃▂▂▁▂
wandb:                           train/std █▇▇▇▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1208/global_step 212992
wandb:        PPO_1208/rollout/ep_len_mean 200.0
wandb:        PPO_1208/rollout/ep_rew_mean -778.50555
wandb:                   PPO_1208/time/fps 1195.0
wandb:            PPO_1208/train/approx_kl 0.01084
wandb:        PPO_1208/train/clip_fraction 0.13502
wandb:           PPO_1208/train/clip_range 0.2
wandb:         PPO_1208/train/entropy_loss -7.77335
wandb:   PPO_1208/train/explained_variance 0.96973
wandb:        PPO_1208/train/learning_rate 0.0003
wandb:                 PPO_1208/train/loss 42.79044
wandb: PPO_1208/train/policy_gradient_loss -0.00627
wandb:                  PPO_1208/train/std 0.73327
wandb:           PPO_1208/train/value_loss 127.00729
wandb:                PPO_1218/global_step 212992
wandb:        PPO_1218/rollout/ep_len_mean 200.0
wandb:        PPO_1218/rollout/ep_rew_mean -611.0177
wandb:                   PPO_1218/time/fps 1191.0
wandb:            PPO_1218/train/approx_kl 0.01472
wandb:        PPO_1218/train/clip_fraction 0.19527
wandb:           PPO_1218/train/clip_range 0.2
wandb:         PPO_1218/train/entropy_loss -6.69648
wandb:   PPO_1218/train/explained_variance 0.97413
wandb:        PPO_1218/train/learning_rate 0.0003
wandb:                 PPO_1218/train/loss 24.24056
wandb: PPO_1218/train/policy_gradient_loss -0.00524
wandb:                  PPO_1218/train/std 0.62888
wandb:           PPO_1218/train/value_loss 67.75895
wandb:                PPO_1228/global_step 212992
wandb:        PPO_1228/rollout/ep_len_mean 200.0
wandb:        PPO_1228/rollout/ep_rew_mean -542.18219
wandb:                   PPO_1228/time/fps 1188.0
wandb:            PPO_1228/train/approx_kl 0.01345
wandb:        PPO_1228/train/clip_fraction 0.18564
wandb:           PPO_1228/train/clip_range 0.2
wandb:         PPO_1228/train/entropy_loss -5.81915
wandb:   PPO_1228/train/explained_variance 0.95968
wandb:        PPO_1228/train/learning_rate 0.0003
wandb:                 PPO_1228/train/loss 22.85525
wandb: PPO_1228/train/policy_gradient_loss -0.00143
wandb:                  PPO_1228/train/std 0.55572
wandb:           PPO_1228/train/value_loss 61.23231
wandb:                PPO_1238/global_step 212992
wandb:        PPO_1238/rollout/ep_len_mean 200.0
wandb:        PPO_1238/rollout/ep_rew_mean -533.55927
wandb:                   PPO_1238/time/fps 1196.0
wandb:            PPO_1238/train/approx_kl 0.01497
wandb:        PPO_1238/train/clip_fraction 0.20372
wandb:           PPO_1238/train/clip_range 0.2
wandb:         PPO_1238/train/entropy_loss -5.22251
wandb:   PPO_1238/train/explained_variance 0.97534
wandb:        PPO_1238/train/learning_rate 0.0003
wandb:                 PPO_1238/train/loss 25.88916
wandb: PPO_1238/train/policy_gradient_loss -0.00185
wandb:                  PPO_1238/train/std 0.50996
wandb:           PPO_1238/train/value_loss 63.76905
wandb:                PPO_1248/global_step 212992
wandb:        PPO_1248/rollout/ep_len_mean 200.0
wandb:        PPO_1248/rollout/ep_rew_mean -519.49084
wandb:                   PPO_1248/time/fps 1195.0
wandb:            PPO_1248/train/approx_kl 0.0174
wandb:        PPO_1248/train/clip_fraction 0.20724
wandb:           PPO_1248/train/clip_range 0.2
wandb:         PPO_1248/train/entropy_loss -4.76205
wandb:   PPO_1248/train/explained_variance 0.98255
wandb:        PPO_1248/train/learning_rate 0.0003
wandb:                 PPO_1248/train/loss 43.26511
wandb: PPO_1248/train/policy_gradient_loss -0.00332
wandb:                  PPO_1248/train/std 0.47742
wandb:           PPO_1248/train/value_loss 60.5948
wandb:                PPO_1258/global_step 212992
wandb:        PPO_1258/rollout/ep_len_mean 200.0
wandb:        PPO_1258/rollout/ep_rew_mean -471.76447
wandb:                   PPO_1258/time/fps 1194.0
wandb:            PPO_1258/train/approx_kl 0.01639
wandb:        PPO_1258/train/clip_fraction 0.21153
wandb:           PPO_1258/train/clip_range 0.2
wandb:         PPO_1258/train/entropy_loss -4.25482
wandb:   PPO_1258/train/explained_variance 0.98691
wandb:        PPO_1258/train/learning_rate 0.0003
wandb:                 PPO_1258/train/loss 18.6658
wandb: PPO_1258/train/policy_gradient_loss -0.00169
wandb:                  PPO_1258/train/std 0.44445
wandb:           PPO_1258/train/value_loss 75.69495
wandb:                PPO_1268/global_step 212992
wandb:        PPO_1268/rollout/ep_len_mean 200.0
wandb:        PPO_1268/rollout/ep_rew_mean -470.17627
wandb:                   PPO_1268/time/fps 1193.0
wandb:            PPO_1268/train/approx_kl 0.02141
wandb:        PPO_1268/train/clip_fraction 0.23713
wandb:           PPO_1268/train/clip_range 0.2
wandb:         PPO_1268/train/entropy_loss -3.77737
wandb:   PPO_1268/train/explained_variance 0.9901
wandb:        PPO_1268/train/learning_rate 0.0003
wandb:                 PPO_1268/train/loss 25.20687
wandb: PPO_1268/train/policy_gradient_loss 0.00082
wandb:                  PPO_1268/train/std 0.41482
wandb:           PPO_1268/train/value_loss 91.40865
wandb:                PPO_1278/global_step 212992
wandb:        PPO_1278/rollout/ep_len_mean 200.0
wandb:        PPO_1278/rollout/ep_rew_mean -432.05862
wandb:                   PPO_1278/time/fps 1191.0
wandb:            PPO_1278/train/approx_kl 0.01725
wandb:        PPO_1278/train/clip_fraction 0.22464
wandb:           PPO_1278/train/clip_range 0.2
wandb:         PPO_1278/train/entropy_loss -3.29807
wandb:   PPO_1278/train/explained_variance 0.98653
wandb:        PPO_1278/train/learning_rate 0.0003
wandb:                 PPO_1278/train/loss 28.46158
wandb: PPO_1278/train/policy_gradient_loss 0.00013
wandb:                  PPO_1278/train/std 0.38754
wandb:           PPO_1278/train/value_loss 87.82188
wandb:                PPO_1288/global_step 212992
wandb:        PPO_1288/rollout/ep_len_mean 200.0
wandb:        PPO_1288/rollout/ep_rew_mean -460.55188
wandb:                   PPO_1288/time/fps 1196.0
wandb:            PPO_1288/train/approx_kl 0.0177
wandb:        PPO_1288/train/clip_fraction 0.24814
wandb:           PPO_1288/train/clip_range 0.2
wandb:         PPO_1288/train/entropy_loss -2.99479
wandb:   PPO_1288/train/explained_variance 0.99197
wandb:        PPO_1288/train/learning_rate 0.0003
wandb:                 PPO_1288/train/loss 38.73233
wandb: PPO_1288/train/policy_gradient_loss 0.00304
wandb:                  PPO_1288/train/std 0.37195
wandb:           PPO_1288/train/value_loss 84.66266
wandb:                    global_mean_eval -384.86827
wandb:                         global_step 212992
wandb:                       mean_reward_0 -316.89638
wandb:                       mean_reward_1 -309.18996
wandb:                      mean_reward_10 -440.39025
wandb:                      mean_reward_11 -463.53438
wandb:                      mean_reward_12 -314.40597
wandb:                      mean_reward_13 -308.27968
wandb:                      mean_reward_14 -371.87801
wandb:                      mean_reward_15 -412.76201
wandb:                      mean_reward_16 -440.44301
wandb:                      mean_reward_17 -463.7444
wandb:                      mean_reward_18 -306.38877
wandb:                      mean_reward_19 -307.64841
wandb:                       mean_reward_2 -372.32525
wandb:                      mean_reward_20 -371.77433
wandb:                      mean_reward_21 -412.48594
wandb:                      mean_reward_22 -440.5689
wandb:                      mean_reward_23 -466.94097
wandb:                      mean_reward_24 -309.37572
wandb:                      mean_reward_25 -307.70074
wandb:                      mean_reward_26 -371.94281
wandb:                      mean_reward_27 -412.93732
wandb:                      mean_reward_28 -440.81694
wandb:                      mean_reward_29 -465.7084
wandb:                       mean_reward_3 -412.59062
wandb:                      mean_reward_30 -313.15508
wandb:                      mean_reward_31 -308.49357
wandb:                      mean_reward_32 -372.03364
wandb:                      mean_reward_33 -412.76441
wandb:                      mean_reward_34 -440.96672
wandb:                      mean_reward_35 -464.07619
wandb:                       mean_reward_4 -440.81437
wandb:                       mean_reward_5 -463.36908
wandb:                       mean_reward_6 -306.59129
wandb:                       mean_reward_7 -308.20443
wandb:                       mean_reward_8 -371.46795
wandb:                       mean_reward_9 -412.59188
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -903.27844
wandb:                        std_reward_0 43.69271
wandb:                        std_reward_1 6.99583
wandb:                       std_reward_10 1.85446
wandb:                       std_reward_11 11.09881
wandb:                       std_reward_12 42.56652
wandb:                       std_reward_13 7.04306
wandb:                       std_reward_14 2.94218
wandb:                       std_reward_15 1.38247
wandb:                       std_reward_16 2.42238
wandb:                       std_reward_17 10.81432
wandb:                       std_reward_18 35.25481
wandb:                       std_reward_19 6.52758
wandb:                        std_reward_2 3.22621
wandb:                       std_reward_20 2.89639
wandb:                       std_reward_21 1.49613
wandb:                       std_reward_22 2.38196
wandb:                       std_reward_23 18.67978
wandb:                       std_reward_24 35.28675
wandb:                       std_reward_25 7.16124
wandb:                       std_reward_26 2.92866
wandb:                       std_reward_27 1.88703
wandb:                       std_reward_28 2.39117
wandb:                       std_reward_29 16.59062
wandb:                        std_reward_3 1.61079
wandb:                       std_reward_30 43.27086
wandb:                       std_reward_31 7.23594
wandb:                       std_reward_32 2.58955
wandb:                       std_reward_33 1.65426
wandb:                       std_reward_34 2.1252
wandb:                       std_reward_35 13.45628
wandb:                        std_reward_4 2.7638
wandb:                        std_reward_5 14.60388
wandb:                        std_reward_6 36.54284
wandb:                        std_reward_7 7.37245
wandb:                        std_reward_8 2.64494
wandb:                        std_reward_9 1.37633
wandb:                            time/fps 1153.0
wandb:                     train/approx_kl 0.01145
wandb:                 train/clip_fraction 0.14658
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.87514
wandb:            train/explained_variance 0.94396
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.10234
wandb:          train/policy_gradient_loss -0.01207
wandb:                           train/std 0.85747
wandb:                    train/value_loss 31.23324
wandb: 
wandb: Synced eager-capybara-36: https://wandb.ai/tidiane/meta_rl_context/runs/260njzkv
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_032708-260njzkv/logs
wandb: 
wandb: Run history:
wandb:                PPO_1206/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1206/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1206/rollout/ep_rew_mean ▁▁▆▅▇▇█▇▄▄█▆
wandb:                   PPO_1206/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1206/train/approx_kl ▆▂▁▂▄▃█▅█▃▇
wandb:        PPO_1206/train/clip_fraction ▄▁▂▃▅▇▇▇█▇▇
wandb:           PPO_1206/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1206/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1206/train/explained_variance ▁█▆▆▃▂▂▁▃▄█
wandb:        PPO_1206/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1206/train/loss ▁▅▄▅▁▂█▄▄▁▂
wandb: PPO_1206/train/policy_gradient_loss ▃▆▃▄▆▃▃▁▁▇█
wandb:                  PPO_1206/train/std █▇▆▆▅▄▄▃▂▂▁
wandb:           PPO_1206/train/value_loss ▂▃▆▂█▅▅▂▃▁▁
wandb:                PPO_1217/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1217/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1217/rollout/ep_rew_mean ▁▂▃▃▄▅▆▅▆▇▇█
wandb:                   PPO_1217/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1217/train/approx_kl ▂▃▁▄▅▆▄▅▄▄█
wandb:        PPO_1217/train/clip_fraction ▁▂▁▄▆▆▂▇▆▄█
wandb:           PPO_1217/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1217/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1217/train/explained_variance ▄▆█▆▅▆▁▆▂▆▃
wandb:        PPO_1217/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1217/train/loss ▁▄█▅▃▁▆▄▅▇▆
wandb: PPO_1217/train/policy_gradient_loss ▃▆▆▃▂▆█▄▂▄▁
wandb:                  PPO_1217/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1217/train/value_loss ▃▂▃▂▂▁▆▄▄██
wandb:                PPO_1227/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1227/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1227/rollout/ep_rew_mean ▁▂▂▂▄▄▆▅▆▇▇█
wandb:                   PPO_1227/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1227/train/approx_kl ▂▅▆▆▃▆▄▁█▄▆
wandb:        PPO_1227/train/clip_fraction ▅▇▇▄▆▆▇▁▇▇█
wandb:           PPO_1227/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1227/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1227/train/explained_variance ▆█▇█▇▆▅█▆▁▆
wandb:        PPO_1227/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1227/train/loss ▄█▅▄▂▂▇▆▃▄▁
wandb: PPO_1227/train/policy_gradient_loss ▅▁▅█▅▇▆█▄▅▇
wandb:                  PPO_1227/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1227/train/value_loss ▆▇▆▄▅▅▆█▇▅▁
wandb:                PPO_1237/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1237/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1237/rollout/ep_rew_mean ▄▄▁▃▃▅▇▇▅▄█▄
wandb:                   PPO_1237/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1237/train/approx_kl ▄▄▅▁▃▆▆▅▅█▂
wandb:        PPO_1237/train/clip_fraction ▅▅▂▂▁█▅▆▇▇▁
wandb:           PPO_1237/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1237/train/entropy_loss ▁▂▃▄▄▅▆▆▇▇█
wandb:   PPO_1237/train/explained_variance ▄▄▃▄▁▂▆▁▄█▇
wandb:        PPO_1237/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1237/train/loss ▂▃▇▄█▄▇▆▄▁▆
wandb: PPO_1237/train/policy_gradient_loss ▃▁▃█▅█▆▅▅▃▃
wandb:                  PPO_1237/train/std █▇▆▅▄▄▃▃▂▂▁
wandb:           PPO_1237/train/value_loss ▁▂▄▄▄▂▂▄▃▃█
wandb:                PPO_1247/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1247/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1247/rollout/ep_rew_mean ▄▂▄▂▂▁█▅▁▇▅▅
wandb:                   PPO_1247/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1247/train/approx_kl ▄▄▅▁▁█▆▃▄▅▅
wandb:        PPO_1247/train/clip_fraction ▅▆▅▃▂▅█▄▁█▄
wandb:           PPO_1247/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1247/train/entropy_loss ▁▂▂▃▃▄▅▅▆▇█
wandb:   PPO_1247/train/explained_variance ▁▆▄▆▇▆▅▇▆█▇
wandb:        PPO_1247/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1247/train/loss ▁▁▃▄▂▆█▃▆▁▂
wandb: PPO_1247/train/policy_gradient_loss ▄▃▁▄██▇█▅█▆
wandb:                  PPO_1247/train/std █▇▇▆▆▅▄▄▃▂▁
wandb:           PPO_1247/train/value_loss ▂▁▁▄▃▆▄▄█▄▆
wandb:                PPO_1257/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1257/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1257/rollout/ep_rew_mean ▆█▄▂▄▂█▁▂▅▁▄
wandb:                   PPO_1257/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1257/train/approx_kl ▆▇▄▅█▂▆▁▅▆▆
wandb:        PPO_1257/train/clip_fraction ▆▆▅▂▅▁█▁▄▃▃
wandb:           PPO_1257/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1257/train/entropy_loss ▁▂▃▃▄▅▆▆▇▇█
wandb:   PPO_1257/train/explained_variance ▁▅▇▇▅▃▅▅▆▅█
wandb:        PPO_1257/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1257/train/loss ▁▂▁▂▁▂▃▃▃▃█
wandb: PPO_1257/train/policy_gradient_loss ▃▃▅▁▄▇█▇▅▃▁
wandb:                  PPO_1257/train/std █▇▆▅▄▄▃▃▂▂▁
wandb:           PPO_1257/train/value_loss ▂▁▁▃▄█▂███▆
wandb:                PPO_1267/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1267/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1267/rollout/ep_rew_mean ▁▅▂▆▅▆▆█▆▁▁▂
wandb:                   PPO_1267/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1267/train/approx_kl ▅▁▄█▂▆▅▅▃▇▁
wandb:        PPO_1267/train/clip_fraction ▅▃▆█▁██▄▃▇▂
wandb:           PPO_1267/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1267/train/entropy_loss ▁▂▂▃▃▄▅▇███
wandb:   PPO_1267/train/explained_variance ▄▄█▁▃█▆▇▇▇▁
wandb:        PPO_1267/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1267/train/loss ▂▁▂▃▅█▁▁▅▆▁
wandb: PPO_1267/train/policy_gradient_loss █▂▆▇▄▅█▄▃█▁
wandb:                  PPO_1267/train/std █▇▇▆▅▄▃▁▁▁▁
wandb:           PPO_1267/train/value_loss ▃▂▂▁▄▂▁▁▃▄█
wandb:                PPO_1277/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1277/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1277/rollout/ep_rew_mean ▅▂▄█▅▂▇▂▆▄▁▆
wandb:                   PPO_1277/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1277/train/approx_kl ▂▄▅▃▄▃█▁▅▁▇
wandb:        PPO_1277/train/clip_fraction ▄▅▇▅▅▅█▁▆▄▆
wandb:           PPO_1277/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1277/train/entropy_loss ▁▁▂▂▃▃▄▃▄▅█
wandb:   PPO_1277/train/explained_variance ▄▇▄▄▄▅▅▄█▁▂
wandb:        PPO_1277/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1277/train/loss █▁▄▃▃▄▁▅▃▇▆
wandb: PPO_1277/train/policy_gradient_loss ▂▄█▃▄▄▆▁▆▃▅
wandb:                  PPO_1277/train/std ███▇▆▆▆▆▅▄▁
wandb:           PPO_1277/train/value_loss ▅▃▄▃▄▄▁█▄█▆
wandb:                PPO_1287/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1287/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1287/rollout/ep_rew_mean ▁▂▅▄▆█▃▆▅▇▆▃
wandb:                   PPO_1287/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1287/train/approx_kl ▄▁▄▃█▇▃▅▃▃▁
wandb:        PPO_1287/train/clip_fraction ▂▅▅▂▆█▃▆▃▅▁
wandb:           PPO_1287/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1287/train/entropy_loss ▁▁▁▃▄▅▅▆▇██
wandb:   PPO_1287/train/explained_variance ▃█▆▆▁▅▇▇▃▄█
wandb:        PPO_1287/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1287/train/loss ▃▁▁█▄▃▅▃▄▃▁
wandb: PPO_1287/train/policy_gradient_loss ▂▄▇▃▇█▄▅▂▁▂
wandb:                  PPO_1287/train/std ██▇▅▅▄▄▃▂▁▂
wandb:           PPO_1287/train/value_loss █▄▆▆▄▄▁▁▄▆▆
wandb:                    global_mean_eval ▂▁▆▆▇▇▇▇▇█
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▁▆█▇▇▆▆▆▇
wandb:                       mean_reward_1 ▂▁▅▆▇█████
wandb:                      mean_reward_10 ▂▁▅▅▆▆▇▇▇█
wandb:                      mean_reward_11 ▁▁▅▅▇▆▇▇▇█
wandb:                      mean_reward_12 ▁▁▅█▇▇▇▆▆▇
wandb:                      mean_reward_13 ▂▁▅▆▇█████
wandb:                      mean_reward_14 ▃▁▅▆▇▇▇▇██
wandb:                      mean_reward_15 ▃▁▅▆▆▆▇▇▇█
wandb:                      mean_reward_16 ▂▁▅▅▆▆▇▇▇█
wandb:                      mean_reward_17 ▁▁▅▅▇▆▇▇▇█
wandb:                      mean_reward_18 ▁▁▆█▇▇▇▅▆▇
wandb:                      mean_reward_19 ▂▁▅▆▇█████
wandb:                       mean_reward_2 ▂▁▅▆▇▇▇▇██
wandb:                      mean_reward_20 ▃▁▅▆▇▇▇▇██
wandb:                      mean_reward_21 ▂▁▅▆▆▆▇▇▇█
wandb:                      mean_reward_22 ▂▁▅▅▆▆▇▇▇█
wandb:                      mean_reward_23 ▁▁▅▅▇▆▇▇▇█
wandb:                      mean_reward_24 ▁▁▆█▇▇▇▆▆▇
wandb:                      mean_reward_25 ▂▁▅▆▇█████
wandb:                      mean_reward_26 ▂▁▅▆▇▇▇▇██
wandb:                      mean_reward_27 ▃▁▅▆▆▆▇▇▇█
wandb:                      mean_reward_28 ▂▁▅▅▆▆▇▇▇█
wandb:                      mean_reward_29 ▁▁▅▅▆▆▇▇▇█
wandb:                       mean_reward_3 ▃▁▅▆▆▆▇▇▇█
wandb:                      mean_reward_30 ▁▁▆█▇▇▇▆▆▇
wandb:                      mean_reward_31 ▂▁▅▆▇█████
wandb:                      mean_reward_32 ▂▁▅▆▇▇▇▇██
wandb:                      mean_reward_33 ▃▁▅▆▆▆▇▇▇█
wandb:                      mean_reward_34 ▂▁▅▅▆▆▇▇▇█
wandb:                      mean_reward_35 ▁▁▅▅▇▆▇▇▇█
wandb:                       mean_reward_4 ▂▁▅▅▆▆▇▇▇█
wandb:                       mean_reward_5 ▁▁▅▅▇▆▇▇▇█
wandb:                       mean_reward_6 ▁▁▆█▇▇▆▅▆▇
wandb:                       mean_reward_7 ▂▁▅▆▇█████
wandb:                       mean_reward_8 ▂▁▅▆▇▇▇▇██
wandb:                       mean_reward_9 ▃▁▅▆▆▆▇▇▇█
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▄▄▅▅▇▆█
wandb:                        std_reward_0 ▅▁█▁▂▂▂▃▂▂
wandb:                        std_reward_1 █▂▁▁▁▁▁▁▁▂
wandb:                       std_reward_10 █▂▁▂▁▁▁▁▁▁
wandb:                       std_reward_11 █▂▂▂▁▇▁▂▂▃
wandb:                       std_reward_12 ▄▁█▁▂▂▂▂▂▂
wandb:                       std_reward_13 █▂▁▁▁▁▁▁▁▂
wandb:                       std_reward_14 █▂▂▁▁▁▁▂▁▁
wandb:                       std_reward_15 █▂▂▂▁▁▁▁▁▁
wandb:                       std_reward_16 █▂▁▂▁▁▁▁▁▁
wandb:                       std_reward_17 █▁▂▂▂▅▂▂▂▂
wandb:                       std_reward_18 ▅▁█▁▂▂▂▃▃▁
wandb:                       std_reward_19 █▂▁▁▁▁▁▁▁▂
wandb:                        std_reward_2 █▂▁▁▁▁▁▂▁▁
wandb:                       std_reward_20 █▂▂▁▁▁▁▂▁▁
wandb:                       std_reward_21 █▂▂▂▁▁▁▂▁▁
wandb:                       std_reward_22 █▁▁▁▁▁▁▁▁▁
wandb:                       std_reward_23 █▁▂▂▁▆▂▂▂▂
wandb:                       std_reward_24 ▅▁█▁▂▂▂▃▃▂
wandb:                       std_reward_25 █▂▁▁▁▁▁▁▁▂
wandb:                       std_reward_26 █▂▂▁▁▁▁▂▁▂
wandb:                       std_reward_27 █▁▂▂▁▁▁▁▁▁
wandb:                       std_reward_28 █▂▁▂▁▁▁▁▁▁
wandb:                       std_reward_29 █▃▂▂▁▅▂▁▁▂
wandb:                        std_reward_3 █▂▂▂▁▁▁▁▁▁
wandb:                       std_reward_30 ▅▁█▁▂▂▂▃▃▂
wandb:                       std_reward_31 █▂▁▁▁▁▁▁▁▂
wandb:                       std_reward_32 █▁▂▁▁▁▁▂▁▁
wandb:                       std_reward_33 █▂▂▂▁▁▁▁▁▁
wandb:                       std_reward_34 █▂▁▂▁▁▁▁▁▁
wandb:                       std_reward_35 █▂▁▁▁▅▁▁▁▁
wandb:                        std_reward_4 █▁▁▁▁▁▁▁▁▁
wandb:                        std_reward_5 █▁▁▁▁▇▁▁▁▂
wandb:                        std_reward_6 ▅▁█▁▂▂▂▃▃▂
wandb:                        std_reward_7 █▂▁▁▁▁▁▁▁▂
wandb:                        std_reward_8 █▂▂▁▁▁▁▂▁▁
wandb:                        std_reward_9 █▂▂▂▁▁▁▁▁▁
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▅█▁▂▄▅▅▆▇▆
wandb:                 train/clip_fraction ▂▃▄▃▄▁▃▅▆▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▄▆▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▃▃▂▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▅▆█▆▄▃▂▂▁▂
wandb:                           train/std █▇▇▇▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1206/global_step 212992
wandb:        PPO_1206/rollout/ep_len_mean 200.0
wandb:        PPO_1206/rollout/ep_rew_mean -868.6546
wandb:                   PPO_1206/time/fps 1195.0
wandb:            PPO_1206/train/approx_kl 0.01207
wandb:        PPO_1206/train/clip_fraction 0.15638
wandb:           PPO_1206/train/clip_range 0.2
wandb:         PPO_1206/train/entropy_loss -7.96414
wandb:   PPO_1206/train/explained_variance 0.97329
wandb:        PPO_1206/train/learning_rate 0.0003
wandb:                 PPO_1206/train/loss 32.56976
wandb: PPO_1206/train/policy_gradient_loss -0.00637
wandb:                  PPO_1206/train/std 0.75544
wandb:           PPO_1206/train/value_loss 72.5939
wandb:                PPO_1217/global_step 212992
wandb:        PPO_1217/rollout/ep_len_mean 200.0
wandb:        PPO_1217/rollout/ep_rew_mean -721.81812
wandb:                   PPO_1217/time/fps 1197.0
wandb:            PPO_1217/train/approx_kl 0.01442
wandb:        PPO_1217/train/clip_fraction 0.17774
wandb:           PPO_1217/train/clip_range 0.2
wandb:         PPO_1217/train/entropy_loss -7.17421
wandb:   PPO_1217/train/explained_variance 0.9544
wandb:        PPO_1217/train/learning_rate 0.0003
wandb:                 PPO_1217/train/loss 43.79922
wandb: PPO_1217/train/policy_gradient_loss -0.00875
wandb:                  PPO_1217/train/std 0.67386
wandb:           PPO_1217/train/value_loss 116.3063
wandb:                PPO_1227/global_step 212992
wandb:        PPO_1227/rollout/ep_len_mean 200.0
wandb:        PPO_1227/rollout/ep_rew_mean -605.68347
wandb:                   PPO_1227/time/fps 1188.0
wandb:            PPO_1227/train/approx_kl 0.01408
wandb:        PPO_1227/train/clip_fraction 0.19233
wandb:           PPO_1227/train/clip_range 0.2
wandb:         PPO_1227/train/entropy_loss -6.48165
wandb:   PPO_1227/train/explained_variance 0.9535
wandb:        PPO_1227/train/learning_rate 0.0003
wandb:                 PPO_1227/train/loss 17.36514
wandb: PPO_1227/train/policy_gradient_loss -0.00535
wandb:                  PPO_1227/train/std 0.60985
wandb:           PPO_1227/train/value_loss 60.70319
wandb:                PPO_1237/global_step 212992
wandb:        PPO_1237/rollout/ep_len_mean 200.0
wandb:        PPO_1237/rollout/ep_rew_mean -593.50977
wandb:                   PPO_1237/time/fps 1184.0
wandb:            PPO_1237/train/approx_kl 0.01355
wandb:        PPO_1237/train/clip_fraction 0.17427
wandb:           PPO_1237/train/clip_range 0.2
wandb:         PPO_1237/train/entropy_loss -5.88306
wandb:   PPO_1237/train/explained_variance 0.96394
wandb:        PPO_1237/train/learning_rate 0.0003
wandb:                 PPO_1237/train/loss 59.46148
wandb: PPO_1237/train/policy_gradient_loss -0.00552
wandb:                  PPO_1237/train/std 0.56052
wandb:           PPO_1237/train/value_loss 149.77466
wandb:                PPO_1247/global_step 212992
wandb:        PPO_1247/rollout/ep_len_mean 200.0
wandb:        PPO_1247/rollout/ep_rew_mean -558.67432
wandb:                   PPO_1247/time/fps 1182.0
wandb:            PPO_1247/train/approx_kl 0.0156
wandb:        PPO_1247/train/clip_fraction 0.19015
wandb:           PPO_1247/train/clip_range 0.2
wandb:         PPO_1247/train/entropy_loss -5.35736
wandb:   PPO_1247/train/explained_variance 0.97714
wandb:        PPO_1247/train/learning_rate 0.0003
wandb:                 PPO_1247/train/loss 42.9418
wandb: PPO_1247/train/policy_gradient_loss -0.00346
wandb:                  PPO_1247/train/std 0.52103
wandb:           PPO_1247/train/value_loss 221.35358
wandb:                PPO_1257/global_step 212992
wandb:        PPO_1257/rollout/ep_len_mean 200.0
wandb:        PPO_1257/rollout/ep_rew_mean -556.59009
wandb:                   PPO_1257/time/fps 1179.0
wandb:            PPO_1257/train/approx_kl 0.01583
wandb:        PPO_1257/train/clip_fraction 0.19363
wandb:           PPO_1257/train/clip_range 0.2
wandb:         PPO_1257/train/entropy_loss -4.937
wandb:   PPO_1257/train/explained_variance 0.98671
wandb:        PPO_1257/train/learning_rate 0.0003
wandb:                 PPO_1257/train/loss 400.46542
wandb: PPO_1257/train/policy_gradient_loss -0.0033
wandb:                  PPO_1257/train/std 0.49053
wandb:           PPO_1257/train/value_loss 278.72729
wandb:                PPO_1267/global_step 212992
wandb:        PPO_1267/rollout/ep_len_mean 200.0
wandb:        PPO_1267/rollout/ep_rew_mean -575.88336
wandb:                   PPO_1267/time/fps 1176.0
wandb:            PPO_1267/train/approx_kl 0.01197
wandb:        PPO_1267/train/clip_fraction 0.16451
wandb:           PPO_1267/train/clip_range 0.2
wandb:         PPO_1267/train/entropy_loss -4.69031
wandb:   PPO_1267/train/explained_variance 0.98172
wandb:        PPO_1267/train/learning_rate 0.0003
wandb:                 PPO_1267/train/loss 48.12065
wandb: PPO_1267/train/policy_gradient_loss -0.00256
wandb:                  PPO_1267/train/std 0.47501
wandb:           PPO_1267/train/value_loss 542.57458
wandb:                PPO_1277/global_step 212992
wandb:        PPO_1277/rollout/ep_len_mean 200.0
wandb:        PPO_1277/rollout/ep_rew_mean -526.19519
wandb:                   PPO_1277/time/fps 1175.0
wandb:            PPO_1277/train/approx_kl 0.0208
wandb:        PPO_1277/train/clip_fraction 0.23954
wandb:           PPO_1277/train/clip_range 0.2
wandb:         PPO_1277/train/entropy_loss -4.34669
wandb:   PPO_1277/train/explained_variance 0.98164
wandb:        PPO_1277/train/learning_rate 0.0003
wandb:                 PPO_1277/train/loss 371.70401
wandb: PPO_1277/train/policy_gradient_loss 0.0021
wandb:                  PPO_1277/train/std 0.45235
wandb:           PPO_1277/train/value_loss 403.4292
wandb:                PPO_1287/global_step 212992
wandb:        PPO_1287/rollout/ep_len_mean 200.0
wandb:        PPO_1287/rollout/ep_rew_mean -536.22052
wandb:                   PPO_1287/time/fps 1179.0
wandb:            PPO_1287/train/approx_kl 0.01362
wandb:        PPO_1287/train/clip_fraction 0.18479
wandb:           PPO_1287/train/clip_range 0.2
wandb:         PPO_1287/train/entropy_loss -4.11643
wandb:   PPO_1287/train/explained_variance 0.98741
wandb:        PPO_1287/train/learning_rate 0.0003
wandb:                 PPO_1287/train/loss 29.31653
wandb: PPO_1287/train/policy_gradient_loss -5e-05
wandb:                  PPO_1287/train/std 0.43828
wandb:           PPO_1287/train/value_loss 390.00974
wandb:                    global_mean_eval -403.63309
wandb:                         global_step 212992
wandb:                       mean_reward_0 -394.90058
wandb:                       mean_reward_1 -321.39395
wandb:                      mean_reward_10 -443.20955
wandb:                      mean_reward_11 -456.03522
wandb:                      mean_reward_12 -391.63282
wandb:                      mean_reward_13 -322.65814
wandb:                      mean_reward_14 -387.44707
wandb:                      mean_reward_15 -420.99487
wandb:                      mean_reward_16 -442.88293
wandb:                      mean_reward_17 -455.01265
wandb:                      mean_reward_18 -389.65871
wandb:                      mean_reward_19 -321.79691
wandb:                       mean_reward_2 -388.25027
wandb:                      mean_reward_20 -387.61502
wandb:                      mean_reward_21 -420.81567
wandb:                      mean_reward_22 -442.77092
wandb:                      mean_reward_23 -454.76234
wandb:                      mean_reward_24 -389.4337
wandb:                      mean_reward_25 -323.33623
wandb:                      mean_reward_26 -388.5173
wandb:                      mean_reward_27 -420.72591
wandb:                      mean_reward_28 -442.83738
wandb:                      mean_reward_29 -453.8749
wandb:                       mean_reward_3 -421.59803
wandb:                      mean_reward_30 -392.85853
wandb:                      mean_reward_31 -323.0767
wandb:                      mean_reward_32 -388.02924
wandb:                      mean_reward_33 -421.46792
wandb:                      mean_reward_34 -442.91709
wandb:                      mean_reward_35 -454.22424
wandb:                       mean_reward_4 -442.80677
wandb:                       mean_reward_5 -455.06083
wandb:                       mean_reward_6 -394.81327
wandb:                       mean_reward_7 -323.3135
wandb:                       mean_reward_8 -388.74371
wandb:                       mean_reward_9 -421.31829
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -903.27844
wandb:                        std_reward_0 23.65129
wandb:                        std_reward_1 10.68682
wandb:                       std_reward_10 5.47903
wandb:                       std_reward_11 17.28773
wandb:                       std_reward_12 21.18191
wandb:                       std_reward_13 12.02708
wandb:                       std_reward_14 9.11288
wandb:                       std_reward_15 2.66467
wandb:                       std_reward_16 5.58635
wandb:                       std_reward_17 11.73097
wandb:                       std_reward_18 20.94794
wandb:                       std_reward_19 10.73318
wandb:                        std_reward_2 9.20092
wandb:                       std_reward_20 8.25657
wandb:                       std_reward_21 3.12496
wandb:                       std_reward_22 4.92311
wandb:                       std_reward_23 10.43526
wandb:                       std_reward_24 23.03025
wandb:                       std_reward_25 11.06284
wandb:                       std_reward_26 10.1857
wandb:                       std_reward_27 2.69076
wandb:                       std_reward_28 4.92467
wandb:                       std_reward_29 12.07996
wandb:                        std_reward_3 2.98572
wandb:                       std_reward_30 22.18528
wandb:                       std_reward_31 10.32778
wandb:                       std_reward_32 9.11105
wandb:                       std_reward_33 2.60549
wandb:                       std_reward_34 4.48311
wandb:                       std_reward_35 9.06036
wandb:                        std_reward_4 4.9066
wandb:                        std_reward_5 13.10728
wandb:                        std_reward_6 21.79344
wandb:                        std_reward_7 11.61655
wandb:                        std_reward_8 9.09269
wandb:                        std_reward_9 2.94453
wandb:                            time/fps 1153.0
wandb:                     train/approx_kl 0.01145
wandb:                 train/clip_fraction 0.14658
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.87514
wandb:            train/explained_variance 0.94396
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.10234
wandb:          train/policy_gradient_loss -0.01207
wandb:                           train/std 0.85747
wandb:                    train/value_loss 31.23324
wandb: 
wandb: Synced deft-snow-34: https://wandb.ai/tidiane/meta_rl_context/runs/1ol3e8yx
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_032708-1ol3e8yx/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1209/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1209/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1209/rollout/ep_rew_mean ▂▁▂▁▂▃▃▄▅▅▆█
wandb:                   PPO_1209/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1209/train/approx_kl ▆▃▃▅▃▁▃▆▁▅█
wandb:        PPO_1209/train/clip_fraction ▅▄▃▄▃▁▃▆▁▇█
wandb:           PPO_1209/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1209/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1209/train/explained_variance ▆▃█▁▅▆▆█▄▅▆
wandb:        PPO_1209/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1209/train/loss ▃▁▁▃▃▃█▃▃▂█
wandb: PPO_1209/train/policy_gradient_loss ▂▁▅▄▇▆█▂█▅▄
wandb:                  PPO_1209/train/std █▇▇▆▅▅▄▃▃▂▁
wandb:           PPO_1209/train/value_loss ▁▁▂▅▃▅▅▃▇▆█
wandb:                PPO_1219/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1219/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1219/rollout/ep_rew_mean ▁▃▄▄▅▆▆▆▆▇▆█
wandb:                   PPO_1219/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1219/train/approx_kl ▃█▄▃▃▄▂▆▃▁▅
wandb:        PPO_1219/train/clip_fraction ▄█▅▅▃▅▅▇▁▃▄
wandb:           PPO_1219/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1219/train/entropy_loss ▁▁▂▃▃▄▅▅▆▆█
wandb:   PPO_1219/train/explained_variance ▄▄▆▆▅▆▇█▃▁▅
wandb:        PPO_1219/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1219/train/loss ▆█▃▃▄▁▂▂▄▂▄
wandb: PPO_1219/train/policy_gradient_loss ▃▁▃▄▇█▆▆▇█▅
wandb:                  PPO_1219/train/std ██▇▆▅▅▄▃▃▃▁
wandb:           PPO_1219/train/value_loss ██▅▄▃▂▁▁▃▁▂
wandb:                PPO_1229/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1229/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1229/rollout/ep_rew_mean ▃▃▂▁▃▂▃▄▄▇█▃
wandb:                   PPO_1229/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1229/train/approx_kl ▆▆▄▁▃▇▃▄▆█▃
wandb:        PPO_1229/train/clip_fraction ▆▇▄▃▅▅▅▂▆█▁
wandb:           PPO_1229/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1229/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1229/train/explained_variance ▇█▁▃▂▆▆▇▇▅▆
wandb:        PPO_1229/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1229/train/loss ▁▁▁▂▄▁▁▃▃█▅
wandb: PPO_1229/train/policy_gradient_loss ▂▁▄▅▄▄▄▁█▇▅
wandb:                  PPO_1229/train/std █▇▇▆▅▅▄▃▃▂▁
wandb:           PPO_1229/train/value_loss ▂▁▃▃▅▅▄▄▄▆█
wandb:                PPO_1239/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1239/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1239/rollout/ep_rew_mean █▄▂▃▁▅▃▁▄█▃▂
wandb:                   PPO_1239/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1239/train/approx_kl ▃▃▁▂▃▆▄▂▃█▄
wandb:        PPO_1239/train/clip_fraction ▄▃▂▁▃▆▃▂▂█▃
wandb:           PPO_1239/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1239/train/entropy_loss ▁▂▂▂▂▂▃▄▅▆█
wandb:   PPO_1239/train/explained_variance ▁▃▅▂▃▆▆▇▆██
wandb:        PPO_1239/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1239/train/loss ▃▃▂▅▄▁▂▃▂▄█
wandb: PPO_1239/train/policy_gradient_loss ▁▅▄▄▅█▆█▄▆▅
wandb:                  PPO_1239/train/std █▇▇█▇▇▆▅▄▃▁
wandb:           PPO_1239/train/value_loss ▃▅▅█▇▃▃▅▅▁▅
wandb:                PPO_1249/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1249/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1249/rollout/ep_rew_mean ▂▁▅▃█▆▄▅▅▅▆▄
wandb:                   PPO_1249/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1249/train/approx_kl ▁▃▅▄█▆▆▇▇█▆
wandb:        PPO_1249/train/clip_fraction ▁▃▅▄█▇▆▆▇▆▅
wandb:           PPO_1249/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1249/train/entropy_loss ▁▁▁▂▂▃▄▅▆▇█
wandb:   PPO_1249/train/explained_variance ▅▄▆▇▁▃█▇▇▆█
wandb:        PPO_1249/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1249/train/loss ▂▂▄▃▃▃█▁▂▂▄
wandb: PPO_1249/train/policy_gradient_loss ▄▁▂▄▆▂█▂▄▆▂
wandb:                  PPO_1249/train/std ███▇▇▅▅▃▃▂▁
wandb:           PPO_1249/train/value_loss ▆█▁▃▁▂▂▂▂▄▄
wandb:                PPO_1260/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1260/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1260/rollout/ep_rew_mean █▃▂▂▁▅▃▅▁▂▂▂
wandb:                   PPO_1260/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1260/train/approx_kl ▄▃▄▆▁▄▄▇▃█▅
wandb:        PPO_1260/train/clip_fraction ▅▁▆▄▃▆▄▇▃█▄
wandb:           PPO_1260/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1260/train/entropy_loss ▁▂▃▃▄▅▆▆▇▇█
wandb:   PPO_1260/train/explained_variance ▁▄█▆▆▇▇▄▆█▅
wandb:        PPO_1260/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1260/train/loss ▄▁▃▂▇▃▆▃▃▁█
wandb: PPO_1260/train/policy_gradient_loss ▆▄▆▄▆▆▄█▄▂▁
wandb:                  PPO_1260/train/std █▇▆▅▅▄▃▃▃▂▁
wandb:           PPO_1260/train/value_loss ▁▇▃▄▇▁▂▁▆▂█
wandb:                PPO_1269/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1269/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1269/rollout/ep_rew_mean ▆▂▄▅▁▅█▄▃▅▆▇
wandb:                   PPO_1269/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1269/train/approx_kl ▄▇▇▂▁█▄▅▂▃▃
wandb:        PPO_1269/train/clip_fraction ▄▇▆▁▂█▅▅▃▅▄
wandb:           PPO_1269/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1269/train/entropy_loss ▁▂▃▄▄▅▅▆▆▇█
wandb:   PPO_1269/train/explained_variance ▁█▃▅▆▄▇▇▇▄▆
wandb:        PPO_1269/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1269/train/loss ▅▁▁█▂▁▁▁▄▃▃
wandb: PPO_1269/train/policy_gradient_loss ▄▅▄▄▂█▅▆▁▄▅
wandb:                  PPO_1269/train/std █▇▆▅▆▅▄▃▃▂▁
wandb:           PPO_1269/train/value_loss ▆▁▁█▇▄▃▂▅▂▂
wandb:                PPO_1279/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1279/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1279/rollout/ep_rew_mean █▆▃▆▇▅▄█▇█▅▁
wandb:                   PPO_1279/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1279/train/approx_kl ▅▇▄▇▆▁▂█▂▄▃
wandb:        PPO_1279/train/clip_fraction ▃▇▅▆▃▁▂█▁▂▃
wandb:           PPO_1279/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1279/train/entropy_loss ▁▁▂▂▃▄▄▅▅▆█
wandb:   PPO_1279/train/explained_variance ▁▄▄▃▅▅▇█▆▆▆
wandb:        PPO_1279/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1279/train/loss ▂▆▄▁▂▂▃▅▁█▂
wandb: PPO_1279/train/policy_gradient_loss ▁▅▆▂▆▄▁▇▆▃█
wandb:                  PPO_1279/train/std █▇▇▆▅▅▅▄▃▂▁
wandb:           PPO_1279/train/value_loss ▄▄▄▁▄▅▄▂▂▅█
wandb:                PPO_1289/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1289/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1289/rollout/ep_rew_mean ▆▁▁▁▆▇█▅▅█▇▇
wandb:                   PPO_1289/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1289/train/approx_kl ▅▂▁█▃▇▁▆▄▆▅
wandb:        PPO_1289/train/clip_fraction ▆▃▁█▆█▃▇▄▅▄
wandb:           PPO_1289/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1289/train/entropy_loss ▁▂▂▃▅▆▇██▇█
wandb:   PPO_1289/train/explained_variance ▅▆▇▇▁▇▇▇█▆▆
wandb:        PPO_1289/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1289/train/loss ▂▃█▂▄▁▂▂▁▃▃
wandb: PPO_1289/train/policy_gradient_loss ▂▃▃▁▂▆▂▃▆█▆
wandb:                  PPO_1289/train/std █▇▆▆▄▃▂▁▂▁▁
wandb:           PPO_1289/train/value_loss ▂▇█▄▁▁▃▁▁▂▂
wandb:                    global_mean_eval ▁▆▇███████
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▂▇▇▂▁▃▆█▇█
wandb:                       mean_reward_1 ▁▅▇████▇▇▇
wandb:                      mean_reward_10 ▁▆▇███████
wandb:                      mean_reward_11 ▁▄▇███████
wandb:                      mean_reward_12 ▂▇▇▂▁▃▆█▆█
wandb:                      mean_reward_13 ▁▅▇████▇▇▇
wandb:                      mean_reward_14 ▁▅▆███▇▇▇▇
wandb:                      mean_reward_15 ▁▆▇████▇█▇
wandb:                      mean_reward_16 ▁▆▇███████
wandb:                      mean_reward_17 ▁▅▇███████
wandb:                      mean_reward_18 ▂▇▇▂▁▃▆█▇█
wandb:                      mean_reward_19 ▁▅▇████▇▇▇
wandb:                       mean_reward_2 ▁▅▆███▇▇▇▇
wandb:                      mean_reward_20 ▁▅▆███▇▇▇▇
wandb:                      mean_reward_21 ▁▆▇████▇▇▇
wandb:                      mean_reward_22 ▁▅▇███████
wandb:                      mean_reward_23 ▁▅▇███████
wandb:                      mean_reward_24 ▂▇▇▂▁▃▆█▇█
wandb:                      mean_reward_25 ▁▅▇████▇▇▇
wandb:                      mean_reward_26 ▁▅▆███▇▇▇▇
wandb:                      mean_reward_27 ▁▆▇████▇██
wandb:                      mean_reward_28 ▁▆▇███████
wandb:                      mean_reward_29 ▁▄▇███████
wandb:                       mean_reward_3 ▁▆▇████▇█▇
wandb:                      mean_reward_30 ▂▇▇▂▁▃▆█▇█
wandb:                      mean_reward_31 ▁▅▇████▇▇▇
wandb:                      mean_reward_32 ▁▅▆███▇▇▇▇
wandb:                      mean_reward_33 ▁▆▇████▇█▇
wandb:                      mean_reward_34 ▁▅▇███████
wandb:                      mean_reward_35 ▁▅▇███████
wandb:                       mean_reward_4 ▁▆▇███████
wandb:                       mean_reward_5 ▁▄▇███████
wandb:                       mean_reward_6 ▂▇▇▂▁▃▆█▇█
wandb:                       mean_reward_7 ▁▅▇████▇▇▇
wandb:                       mean_reward_8 ▁▅▆███▇▇▇▇
wandb:                       mean_reward_9 ▁▆▇████▇█▇
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▄▄▅▅▇▆█
wandb:                        std_reward_0 █▁▅▃▂▂▄▄▅▃
wandb:                        std_reward_1 █▁▁▃▃▂▂▄▃▅
wandb:                       std_reward_10 ▂█▁▁▂▁▁▁▁▁
wandb:                       std_reward_11 ▂█▁▁▂▂▁▁▁▁
wandb:                       std_reward_12 █▃▃▃▁▁▃▂▅▄
wandb:                       std_reward_13 █▁▁▃▃▂▂▄▄▆
wandb:                       std_reward_14 █▁▂▂▄▂▂█▁▅
wandb:                       std_reward_15 ▄▁▁▂▂▁▃▁█▂
wandb:                       std_reward_16 ▂█▁▁▂▁▁▁▁▁
wandb:                       std_reward_17 ▂█▁▁▂▂▁▁▁▁
wandb:                       std_reward_18 █▁▅▃▁▁▄▂▄▂
wandb:                       std_reward_19 █▁▁▂▃▂▂▃▃▅
wandb:                        std_reward_2 █▁▂▃▄▂▂▃▁▄
wandb:                       std_reward_20 ▅▁▂▂▃▂▂█▁▃
wandb:                       std_reward_21 ▃▁▁▁▂▁▂▁█▁
wandb:                       std_reward_22 ▂█▁▁▂▁▁▁▃▁
wandb:                       std_reward_23 ▂█▁▁▂▂▁▁▁▁
wandb:                       std_reward_24 █▃▅▃▁▂▃▂▆▄
wandb:                       std_reward_25 ▇▁▁▂▃▂▂█▂▄
wandb:                       std_reward_26 █▁▂▃▄▂▃▃▂▅
wandb:                       std_reward_27 █▃▂▃▄▂▅▂▁▄
wandb:                       std_reward_28 ▂█▁▁▂▁▁▁▄▁
wandb:                       std_reward_29 ▂█▁▁▂▂▁▁▁▁
wandb:                        std_reward_3 ▄▁▁▂▂▁▃▁█▂
wandb:                       std_reward_30 █▂▆▃▁▁▂▂▅▄
wandb:                       std_reward_31 █▁▁▂▃▁▂▃█▅
wandb:                       std_reward_32 █▁▂▃▄▂▃▃▂▄
wandb:                       std_reward_33 █▃▂▂▄▂▄▁▁▄
wandb:                       std_reward_34 ▂█▁▁▂▁▁▃▁▁
wandb:                       std_reward_35 ▂█▁▁▂▂▁▃▁▁
wandb:                        std_reward_4 ▂█▁▁▂▁▁▁▁▁
wandb:                        std_reward_5 ▂█▁▁▂▁▁▁▃▁
wandb:                        std_reward_6 █▄▆▄▁▁▃▃▆▃
wandb:                        std_reward_7 █▁▁▃▃▁▂▃█▄
wandb:                        std_reward_8 █▁▂▂▄▂▃▃▂▅
wandb:                        std_reward_9 █▄▂▃▄▂▅▂▁▄
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▅█▁▂▄▅▅▆▇▆
wandb:                 train/clip_fraction ▂▃▄▃▄▁▃▅▆▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▄▆▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▃▃▂▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▅▆█▆▄▃▂▂▁▂
wandb:                           train/std █▇▇▇▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1209/global_step 212992
wandb:        PPO_1209/rollout/ep_len_mean 200.0
wandb:        PPO_1209/rollout/ep_rew_mean -734.20221
wandb:                   PPO_1209/time/fps 1191.0
wandb:            PPO_1209/train/approx_kl 0.01249
wandb:        PPO_1209/train/clip_fraction 0.16172
wandb:           PPO_1209/train/clip_range 0.2
wandb:         PPO_1209/train/entropy_loss -7.68336
wandb:   PPO_1209/train/explained_variance 0.96117
wandb:        PPO_1209/train/learning_rate 0.0003
wandb:                 PPO_1209/train/loss 145.90727
wandb: PPO_1209/train/policy_gradient_loss -0.00765
wandb:                  PPO_1209/train/std 0.72482
wandb:           PPO_1209/train/value_loss 181.48923
wandb:                PPO_1219/global_step 212992
wandb:        PPO_1219/rollout/ep_len_mean 200.0
wandb:        PPO_1219/rollout/ep_rew_mean -588.39246
wandb:                   PPO_1219/time/fps 1172.0
wandb:            PPO_1219/train/approx_kl 0.01329
wandb:        PPO_1219/train/clip_fraction 0.16618
wandb:           PPO_1219/train/clip_range 0.2
wandb:         PPO_1219/train/entropy_loss -6.85361
wandb:   PPO_1219/train/explained_variance 0.96744
wandb:        PPO_1219/train/learning_rate 0.0003
wandb:                 PPO_1219/train/loss 40.85143
wandb: PPO_1219/train/policy_gradient_loss -0.00604
wandb:                  PPO_1219/train/std 0.64342
wandb:           PPO_1219/train/value_loss 87.11662
wandb:                PPO_1229/global_step 212992
wandb:        PPO_1229/rollout/ep_len_mean 200.0
wandb:        PPO_1229/rollout/ep_rew_mean -564.36133
wandb:                   PPO_1229/time/fps 1170.0
wandb:            PPO_1229/train/approx_kl 0.01121
wandb:        PPO_1229/train/clip_fraction 0.13472
wandb:           PPO_1229/train/clip_range 0.2
wandb:         PPO_1229/train/entropy_loss -6.22256
wandb:   PPO_1229/train/explained_variance 0.96471
wandb:        PPO_1229/train/learning_rate 0.0003
wandb:                 PPO_1229/train/loss 119.77422
wandb: PPO_1229/train/policy_gradient_loss -0.00352
wandb:                  PPO_1229/train/std 0.58877
wandb:           PPO_1229/train/value_loss 266.60684
wandb:                PPO_1239/global_step 212992
wandb:        PPO_1239/rollout/ep_len_mean 200.0
wandb:        PPO_1239/rollout/ep_rew_mean -616.22003
wandb:                   PPO_1239/time/fps 1165.0
wandb:            PPO_1239/train/approx_kl 0.01086
wandb:        PPO_1239/train/clip_fraction 0.13522
wandb:           PPO_1239/train/clip_range 0.2
wandb:         PPO_1239/train/entropy_loss -5.9459
wandb:   PPO_1239/train/explained_variance 0.98091
wandb:        PPO_1239/train/learning_rate 0.0003
wandb:                 PPO_1239/train/loss 410.18015
wandb: PPO_1239/train/policy_gradient_loss -0.00357
wandb:                  PPO_1239/train/std 0.56518
wandb:           PPO_1239/train/value_loss 482.26196
wandb:                PPO_1249/global_step 212992
wandb:        PPO_1249/rollout/ep_len_mean 200.0
wandb:        PPO_1249/rollout/ep_rew_mean -587.94397
wandb:                   PPO_1249/time/fps 1168.0
wandb:            PPO_1249/train/approx_kl 0.01101
wandb:        PPO_1249/train/clip_fraction 0.14436
wandb:           PPO_1249/train/clip_range 0.2
wandb:         PPO_1249/train/entropy_loss -5.53362
wandb:   PPO_1249/train/explained_variance 0.98414
wandb:        PPO_1249/train/learning_rate 0.0003
wandb:                 PPO_1249/train/loss 343.84839
wandb: PPO_1249/train/policy_gradient_loss -0.00383
wandb:                  PPO_1249/train/std 0.53325
wandb:           PPO_1249/train/value_loss 602.17615
wandb:                PPO_1260/global_step 212992
wandb:        PPO_1260/rollout/ep_len_mean 200.0
wandb:        PPO_1260/rollout/ep_rew_mean -602.44482
wandb:                   PPO_1260/time/fps 1169.0
wandb:            PPO_1260/train/approx_kl 0.0114
wandb:        PPO_1260/train/clip_fraction 0.13958
wandb:           PPO_1260/train/clip_range 0.2
wandb:         PPO_1260/train/entropy_loss -5.07221
wandb:   PPO_1260/train/explained_variance 0.98084
wandb:        PPO_1260/train/learning_rate 0.0003
wandb:                 PPO_1260/train/loss 668.16058
wandb: PPO_1260/train/policy_gradient_loss -0.00402
wandb:                  PPO_1260/train/std 0.49879
wandb:           PPO_1260/train/value_loss 943.9187
wandb:                PPO_1269/global_step 212992
wandb:        PPO_1269/rollout/ep_len_mean 200.0
wandb:        PPO_1269/rollout/ep_rew_mean -528.8446
wandb:                   PPO_1269/time/fps 1171.0
wandb:            PPO_1269/train/approx_kl 0.0133
wandb:        PPO_1269/train/clip_fraction 0.18474
wandb:           PPO_1269/train/clip_range 0.2
wandb:         PPO_1269/train/entropy_loss -4.73247
wandb:   PPO_1269/train/explained_variance 0.98072
wandb:        PPO_1269/train/learning_rate 0.0003
wandb:                 PPO_1269/train/loss 111.49516
wandb: PPO_1269/train/policy_gradient_loss -0.00133
wandb:                  PPO_1269/train/std 0.47514
wandb:           PPO_1269/train/value_loss 316.42419
wandb:                PPO_1279/global_step 212992
wandb:        PPO_1279/rollout/ep_len_mean 200.0
wandb:        PPO_1279/rollout/ep_rew_mean -556.07129
wandb:                   PPO_1279/time/fps 1172.0
wandb:            PPO_1279/train/approx_kl 0.01437
wandb:        PPO_1279/train/clip_fraction 0.18607
wandb:           PPO_1279/train/clip_range 0.2
wandb:         PPO_1279/train/entropy_loss -4.33181
wandb:   PPO_1279/train/explained_variance 0.98206
wandb:        PPO_1279/train/learning_rate 0.0003
wandb:                 PPO_1279/train/loss 56.75792
wandb: PPO_1279/train/policy_gradient_loss 0.00056
wandb:                  PPO_1279/train/std 0.44992
wandb:           PPO_1279/train/value_loss 392.21689
wandb:                PPO_1289/global_step 212992
wandb:        PPO_1289/rollout/ep_len_mean 200.0
wandb:        PPO_1289/rollout/ep_rew_mean -500.08087
wandb:                   PPO_1289/time/fps 1171.0
wandb:            PPO_1289/train/approx_kl 0.01659
wandb:        PPO_1289/train/clip_fraction 0.20763
wandb:           PPO_1289/train/clip_range 0.2
wandb:         PPO_1289/train/entropy_loss -3.95361
wandb:   PPO_1289/train/explained_variance 0.98212
wandb:        PPO_1289/train/learning_rate 0.0003
wandb:                 PPO_1289/train/loss 115.8401
wandb: PPO_1289/train/policy_gradient_loss 0.00017
wandb:                  PPO_1289/train/std 0.42705
wandb:           PPO_1289/train/value_loss 185.62531
wandb:                    global_mean_eval -436.52241
wandb:                         global_step 212992
wandb:                       mean_reward_0 -392.48394
wandb:                       mean_reward_1 -382.63338
wandb:                      mean_reward_10 -474.43172
wandb:                      mean_reward_11 -490.97444
wandb:                      mean_reward_12 -393.81578
wandb:                      mean_reward_13 -384.78328
wandb:                      mean_reward_14 -423.75841
wandb:                      mean_reward_15 -454.5402
wandb:                      mean_reward_16 -474.08714
wandb:                      mean_reward_17 -491.22672
wandb:                      mean_reward_18 -394.91883
wandb:                      mean_reward_19 -380.77342
wandb:                       mean_reward_2 -425.17367
wandb:                      mean_reward_20 -426.02049
wandb:                      mean_reward_21 -454.44216
wandb:                      mean_reward_22 -474.65253
wandb:                      mean_reward_23 -490.97808
wandb:                      mean_reward_24 -385.61694
wandb:                      mean_reward_25 -382.34961
wandb:                      mean_reward_26 -425.38508
wandb:                      mean_reward_27 -454.35685
wandb:                      mean_reward_28 -474.43769
wandb:                      mean_reward_29 -490.69247
wandb:                       mean_reward_3 -454.13005
wandb:                      mean_reward_30 -389.0102
wandb:                      mean_reward_31 -383.03354
wandb:                      mean_reward_32 -425.27858
wandb:                      mean_reward_33 -455.30743
wandb:                      mean_reward_34 -473.77854
wandb:                      mean_reward_35 -491.11856
wandb:                       mean_reward_4 -474.21271
wandb:                       mean_reward_5 -491.13551
wandb:                       mean_reward_6 -394.44658
wandb:                       mean_reward_7 -380.53785
wandb:                       mean_reward_8 -425.29413
wandb:                       mean_reward_9 -454.99036
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -903.27844
wandb:                        std_reward_0 59.64401
wandb:                        std_reward_1 16.47906
wandb:                       std_reward_10 3.6401
wandb:                       std_reward_11 3.87331
wandb:                       std_reward_12 67.95983
wandb:                       std_reward_13 18.47231
wandb:                       std_reward_14 11.0649
wandb:                       std_reward_15 6.31926
wandb:                       std_reward_16 3.77093
wandb:                       std_reward_17 3.55913
wandb:                       std_reward_18 53.94225
wandb:                       std_reward_19 17.00318
wandb:                        std_reward_2 9.58867
wandb:                       std_reward_20 9.9163
wandb:                       std_reward_21 5.56982
wandb:                       std_reward_22 3.92779
wandb:                       std_reward_23 3.36898
wandb:                       std_reward_24 70.72141
wandb:                       std_reward_25 15.96955
wandb:                       std_reward_26 10.7713
wandb:                       std_reward_27 6.47745
wandb:                       std_reward_28 3.60955
wandb:                       std_reward_29 3.56451
wandb:                        std_reward_3 6.19037
wandb:                       std_reward_30 74.8581
wandb:                       std_reward_31 20.01321
wandb:                       std_reward_32 9.23059
wandb:                       std_reward_33 6.5749
wandb:                       std_reward_34 3.52764
wandb:                       std_reward_35 3.97423
wandb:                        std_reward_4 4.33623
wandb:                        std_reward_5 3.49643
wandb:                        std_reward_6 59.72084
wandb:                        std_reward_7 17.39887
wandb:                        std_reward_8 11.23818
wandb:                        std_reward_9 6.77094
wandb:                            time/fps 1153.0
wandb:                     train/approx_kl 0.01145
wandb:                 train/clip_fraction 0.14658
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.87514
wandb:            train/explained_variance 0.94396
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.10234
wandb:          train/policy_gradient_loss -0.01207
wandb:                           train/std 0.85747
wandb:                    train/value_loss 31.23324
wandb: 
wandb: Synced legendary-snowflake-39: https://wandb.ai/tidiane/meta_rl_context/runs/1uc7k9re
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_032708-1uc7k9re/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1211/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1211/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1211/rollout/ep_rew_mean ▁▁▃▂▄▅▅▆▅▇██
wandb:                   PPO_1211/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1211/train/approx_kl ▁▇▂▅▅▆▅▆██▇
wandb:        PPO_1211/train/clip_fraction ▁▂▃▄▃▇▆▅▇██
wandb:           PPO_1211/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1211/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1211/train/explained_variance ▄▆█▃▄▁▄▄▃▃▃
wandb:        PPO_1211/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1211/train/loss ▄▂▁▂▅▃▃█▄▃▂
wandb: PPO_1211/train/policy_gradient_loss ▆▅▁▄▅▄█▅▁▄▅
wandb:                  PPO_1211/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1211/train/value_loss ▁▁▁▂▃█▃▆▆▄▅
wandb:                PPO_1221/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1221/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1221/rollout/ep_rew_mean ▁▁▃▃▄▄▅▅▅▆▇█
wandb:                   PPO_1221/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1221/train/approx_kl ▂▄▁▄▃▂▅▅█▅█
wandb:        PPO_1221/train/clip_fraction ▁▄▂▂▃▃▄▆▇▅█
wandb:           PPO_1221/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1221/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1221/train/explained_variance ▄▁▆▄▂▂▇█▆▅█
wandb:        PPO_1221/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1221/train/loss ▂▃▃█▁▃▂▄▃▄▂
wandb: PPO_1221/train/policy_gradient_loss ▃▄█▁▅▇▇▄▅█▅
wandb:                  PPO_1221/train/std █▇▇▆▅▅▄▃▂▂▁
wandb:           PPO_1221/train/value_loss █▅▂▇▆▅▅▄▇▄▁
wandb:                PPO_1230/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1230/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1230/rollout/ep_rew_mean ▁▂▄▃▄▄▅▅▅▅█▆
wandb:                   PPO_1230/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1230/train/approx_kl ▂▁▆▂▄▄▅█▆▃▂
wandb:        PPO_1230/train/clip_fraction ▃▁█▃▆▄▆▅▇▅▃
wandb:           PPO_1230/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1230/train/entropy_loss ▁▂▂▃▄▅▆▆▆▇█
wandb:   PPO_1230/train/explained_variance ▇▇▅██▇▆▁▆▇▂
wandb:        PPO_1230/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1230/train/loss ▁▇▃▅▁▃▄▃▅▃█
wandb: PPO_1230/train/policy_gradient_loss ▄▃▁▅▇▄▅▃▆▅█
wandb:                  PPO_1230/train/std █▇▆▆▅▄▃▃▂▂▁
wandb:           PPO_1230/train/value_loss ▁▅▂▂▁▁▂▆▂▃█
wandb:                PPO_1240/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1240/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1240/rollout/ep_rew_mean ▁▆▆▅█▆▇▅▄▃▆█
wandb:                   PPO_1240/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1240/train/approx_kl ▂▁▃█▅▅▃▄▄▃▁
wandb:        PPO_1240/train/clip_fraction ▁▃▅█▃▄▃▄▄▃▄
wandb:           PPO_1240/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1240/train/entropy_loss ▁▂▂▃▄▅▆▇███
wandb:   PPO_1240/train/explained_variance ▁▄▇█▇▇█▃▇▄▆
wandb:        PPO_1240/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1240/train/loss ▁▂▁▃▄▃▂█▇▆▂
wandb: PPO_1240/train/policy_gradient_loss ▁▂▇▅▁▂▁▁█▄▄
wandb:                  PPO_1240/train/std ██▇▆▅▃▂▂▁▁▁
wandb:           PPO_1240/train/value_loss ▃▂▃▁▃▃▃▆▅█▆
wandb:                PPO_1250/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1250/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1250/rollout/ep_rew_mean ▄▁▃▃▃▆█▆▅▇▅▇
wandb:                   PPO_1250/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1250/train/approx_kl ▂▁█▃▄▇▅▇▆█▇
wandb:        PPO_1250/train/clip_fraction ▂▁▆▁▅▇▅▆▄█▆
wandb:           PPO_1250/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1250/train/entropy_loss ▁▁▂▃▃▄▅▅▆▇█
wandb:   PPO_1250/train/explained_variance ▁▁█▅█▇▇▄▇▅▇
wandb:        PPO_1250/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1250/train/loss ▄▆▃▄▁▁▁▄▂█▁
wandb: PPO_1250/train/policy_gradient_loss ▁▆▃▄▆███▄▆▆
wandb:                  PPO_1250/train/std ██▇▆▆▅▄▄▃▂▁
wandb:           PPO_1250/train/value_loss ▅█▁█▁▁▁▂▄▁▃
wandb:                PPO_1259/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1259/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1259/rollout/ep_rew_mean ▇▇▃▁▄▅█▄▂▃▆▅
wandb:                   PPO_1259/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1259/train/approx_kl ▅▄▂▄█▃█▁▂▅▃
wandb:        PPO_1259/train/clip_fraction ▆▄▁▄▄▅█▁▃▄▄
wandb:           PPO_1259/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1259/train/entropy_loss ▁▁▂▃▅▆▆▆▆▇█
wandb:   PPO_1259/train/explained_variance ▁▁▄▆▆█▇█▆▆▇
wandb:        PPO_1259/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1259/train/loss ▂▁█▄▁▂▁█▃█▅
wandb: PPO_1259/train/policy_gradient_loss ▄▄▁▃▃▄█▃▄▄▄
wandb:                  PPO_1259/train/std █▇▆▅▄▃▃▃▃▂▁
wandb:           PPO_1259/train/value_loss ▄▃▇▆▄▄▁▇█▆▆
wandb:                PPO_1270/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1270/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1270/rollout/ep_rew_mean ▄▃▇█▄▁▅▁▇▂▄█
wandb:                   PPO_1270/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1270/train/approx_kl ▄▄▃█▁▂▂▆▁▃▄
wandb:        PPO_1270/train/clip_fraction ▅▄▇█▁▃▃█▂▅▅
wandb:           PPO_1270/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1270/train/entropy_loss ▁▂▁▁▃▄▅▆▆▇█
wandb:   PPO_1270/train/explained_variance ▄▁▄▇▆▆▄▇▆█▆
wandb:        PPO_1270/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1270/train/loss ▁▄▃▂▆▂▂▁▂█▁
wandb: PPO_1270/train/policy_gradient_loss ▂▃▂▅▁▃▁█▃▃▃
wandb:                  PPO_1270/train/std ▇▇█▇▆▄▄▃▃▂▁
wandb:           PPO_1270/train/value_loss ▆█▃▁▅█▆▄▅▃▄
wandb:                PPO_1280/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1280/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1280/rollout/ep_rew_mean ▆▆██▁▃▄▁▅▇▆▅
wandb:                   PPO_1280/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1280/train/approx_kl ▅█▇▅▃▄▄▁▃█▃
wandb:        PPO_1280/train/clip_fraction ▆▄█▅▃▃▇▁▅█▃
wandb:           PPO_1280/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1280/train/entropy_loss ▁▁▁▂▃▄▅▅▇██
wandb:   PPO_1280/train/explained_variance ▅▄▇█▅▇▇▇▇▁▅
wandb:        PPO_1280/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1280/train/loss ▅▂█▄▂▄▆▅▆▁▇
wandb: PPO_1280/train/policy_gradient_loss ▂▂█▆▆▁▄▂▁▄▅
wandb:                  PPO_1280/train/std ██▇▇▆▄▅▃▂▂▁
wandb:           PPO_1280/train/value_loss ▄▄▁▃▄▄▃█▅▄▅
wandb:                PPO_1290/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1290/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1290/rollout/ep_rew_mean ▄▁▃▄▇▆▄▅▄█▇▆
wandb:                   PPO_1290/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1290/train/approx_kl ▁▁▁▄▃▆█▇▁▆▂
wandb:        PPO_1290/train/clip_fraction ▁▄▅▄▆▅▄▃▅█▄
wandb:           PPO_1290/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1290/train/entropy_loss ▁▂▂▃▃▄▅▆▆▇█
wandb:   PPO_1290/train/explained_variance ▅▅█▇▁▇▅▂▅▂▇
wandb:        PPO_1290/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1290/train/loss ▃▆▃▅▄▁█▃▃▁▃
wandb: PPO_1290/train/policy_gradient_loss ▁▄▄▃▆▅▃▃▅█▆
wandb:                  PPO_1290/train/std ███▇▆▅▃▃▃▂▁
wandb:           PPO_1290/train/value_loss ▇▆▃▄▃▃▆█▆▁▄
wandb:                    global_mean_eval ▁▅▆███████
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▃▅█▅▅▄▅▅▅
wandb:                       mean_reward_1 ▁▄▅▇██████
wandb:                      mean_reward_10 ▁▅▇▇█▇████
wandb:                      mean_reward_11 ▁▅▇▇▇▇████
wandb:                      mean_reward_12 ▁▃▅█▅▄▄▅▅▆
wandb:                      mean_reward_13 ▁▄▅▆██████
wandb:                      mean_reward_14 ▁▄▅▇██████
wandb:                      mean_reward_15 ▁▅▆▇██████
wandb:                      mean_reward_16 ▁▅▇▇█▇████
wandb:                      mean_reward_17 ▁▅▇▇▇▇████
wandb:                      mean_reward_18 ▁▃▅█▅▅▄▅▅▆
wandb:                      mean_reward_19 ▁▄▅▇██████
wandb:                       mean_reward_2 ▁▄▅▇██████
wandb:                      mean_reward_20 ▁▄▅▇██████
wandb:                      mean_reward_21 ▁▅▆▇██████
wandb:                      mean_reward_22 ▁▅▇▇█▇████
wandb:                      mean_reward_23 ▁▅▇█▇▇████
wandb:                      mean_reward_24 ▁▃▅█▅▅▄▅▅▅
wandb:                      mean_reward_25 ▁▄▅▇██████
wandb:                      mean_reward_26 ▁▄▅▇██████
wandb:                      mean_reward_27 ▁▅▆▇██████
wandb:                      mean_reward_28 ▁▅▇▇█▇████
wandb:                      mean_reward_29 ▁▅▇▇▇▇████
wandb:                       mean_reward_3 ▁▅▆▇██████
wandb:                      mean_reward_30 ▁▃▅█▅▅▄▅▅▆
wandb:                      mean_reward_31 ▁▄▅▇██████
wandb:                      mean_reward_32 ▁▄▅▇██████
wandb:                      mean_reward_33 ▁▅▆▇██████
wandb:                      mean_reward_34 ▁▅▇▇█▇████
wandb:                      mean_reward_35 ▁▅▇█▇▇████
wandb:                       mean_reward_4 ▁▅▇▇█▇████
wandb:                       mean_reward_5 ▁▅▇▇▇▇████
wandb:                       mean_reward_6 ▁▃▅█▅▄▄▅▅▆
wandb:                       mean_reward_7 ▁▄▅▇██████
wandb:                       mean_reward_8 ▁▄▅▇██████
wandb:                       mean_reward_9 ▁▅▆▇██████
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▄▄▅▅▇▆█
wandb:                        std_reward_0 ▃▂█▁▃▂▂▃▂▃
wandb:                        std_reward_1 █▂▁▂▂▁▁▂▂▂
wandb:                       std_reward_10 █▁▃▄▂▂▂▂▂▁
wandb:                       std_reward_11 ▇▁▂▅▅█▃▂█▂
wandb:                       std_reward_12 ▃▂█▁▃▂▃▂▃▃
wandb:                       std_reward_13 █▂▁▂▂▁▁▂▂▂
wandb:                       std_reward_14 █▁▁▁▂▁▁▂▂▂
wandb:                       std_reward_15 █▁▄▂▂▂▂▁▂▁
wandb:                       std_reward_16 ▃▁▂▂▁▁▁▁▁█
wandb:                       std_reward_17 ▆▁▂▄▅▇▂▂█▂
wandb:                       std_reward_18 ▄▂█▁▃▂▃▂▂▃
wandb:                       std_reward_19 █▂▁▂▂▁▁▂▂▂
wandb:                        std_reward_2 █▁▁▁▂▁▁▂▂▂
wandb:                       std_reward_20 █▁▁▁▂▁▁▂▂▂
wandb:                       std_reward_21 █▁▃▂▁▁▂▁▁▁
wandb:                       std_reward_22 █▁▃▃▂▂▂▁▂▁
wandb:                       std_reward_23 ▇▁▂▄▆▆▃▂█▂
wandb:                       std_reward_24 ▃▂█▁▃▁▂▂▂▂
wandb:                       std_reward_25 █▂▁▂▂▁▁▂▂▂
wandb:                       std_reward_26 █▁▁▁▁▁▁▁▂▂
wandb:                       std_reward_27 ▄▁▂▁▁▁▁▁▁█
wandb:                       std_reward_28 ▃▁▂▂▁▁▁█▁▁
wandb:                       std_reward_29 ▆▁▃▄▆█▂▂█▂
wandb:                        std_reward_3 █▁▃▁▁▂▂▁▂▁
wandb:                       std_reward_30 ▄▂█▁▂▁▂▂▂▂
wandb:                       std_reward_31 █▂▁▂▂▁▁▂▂▂
wandb:                       std_reward_32 █▁▁▁▂▁▁▁▂▂
wandb:                       std_reward_33 █▁▃▁▂▂▂▁▂▁
wandb:                       std_reward_34 █▁▃▃▂▂▂▂▂▁
wandb:                       std_reward_35 ▆▁▂▅▆█▃▃█▂
wandb:                        std_reward_4 █▁▂▃▂▂▂▁▂▁
wandb:                        std_reward_5 █▁▂▄▆▇▃▂█▂
wandb:                        std_reward_6 ▄▂█▁▃▂▂▂▂▂
wandb:                        std_reward_7 █▂▁▂▂▁▁▂▂▂
wandb:                        std_reward_8 █▁▁▁▁▁▁▂▂▂
wandb:                        std_reward_9 █▁▃▁▁▁▂▁▂▁
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▅█▁▂▄▅▅▆▇▆
wandb:                 train/clip_fraction ▂▃▄▃▄▁▃▅▆▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▄▆▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▃▃▂▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▅▆█▆▄▃▂▂▁▂
wandb:                           train/std █▇▇▇▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1211/global_step 212992
wandb:        PPO_1211/rollout/ep_len_mean 200.0
wandb:        PPO_1211/rollout/ep_rew_mean -787.46509
wandb:                   PPO_1211/time/fps 1145.0
wandb:            PPO_1211/train/approx_kl 0.01195
wandb:        PPO_1211/train/clip_fraction 0.16163
wandb:           PPO_1211/train/clip_range 0.2
wandb:         PPO_1211/train/entropy_loss -7.72936
wandb:   PPO_1211/train/explained_variance 0.9579
wandb:        PPO_1211/train/learning_rate 0.0003
wandb:                 PPO_1211/train/loss 18.39633
wandb: PPO_1211/train/policy_gradient_loss -0.00876
wandb:                  PPO_1211/train/std 0.72771
wandb:           PPO_1211/train/value_loss 81.86788
wandb:                PPO_1221/global_step 212992
wandb:        PPO_1221/rollout/ep_len_mean 200.0
wandb:        PPO_1221/rollout/ep_rew_mean -659.56287
wandb:                   PPO_1221/time/fps 1151.0
wandb:            PPO_1221/train/approx_kl 0.01569
wandb:        PPO_1221/train/clip_fraction 0.19553
wandb:           PPO_1221/train/clip_range 0.2
wandb:         PPO_1221/train/entropy_loss -6.86084
wandb:   PPO_1221/train/explained_variance 0.96582
wandb:        PPO_1221/train/learning_rate 0.0003
wandb:                 PPO_1221/train/loss 21.77687
wandb: PPO_1221/train/policy_gradient_loss -0.00778
wandb:                  PPO_1221/train/std 0.64442
wandb:           PPO_1221/train/value_loss 58.00319
wandb:                PPO_1230/global_step 212992
wandb:        PPO_1230/rollout/ep_len_mean 200.0
wandb:        PPO_1230/rollout/ep_rew_mean -587.70697
wandb:                   PPO_1230/time/fps 1147.0
wandb:            PPO_1230/train/approx_kl 0.01525
wandb:        PPO_1230/train/clip_fraction 0.19391
wandb:           PPO_1230/train/clip_range 0.2
wandb:         PPO_1230/train/entropy_loss -6.0641
wandb:   PPO_1230/train/explained_variance 0.94572
wandb:        PPO_1230/train/learning_rate 0.0003
wandb:                 PPO_1230/train/loss 67.96434
wandb: PPO_1230/train/policy_gradient_loss -0.00454
wandb:                  PPO_1230/train/std 0.5751
wandb:           PPO_1230/train/value_loss 104.22106
wandb:                PPO_1240/global_step 212992
wandb:        PPO_1240/rollout/ep_len_mean 200.0
wandb:        PPO_1240/rollout/ep_rew_mean -542.86255
wandb:                   PPO_1240/time/fps 1146.0
wandb:            PPO_1240/train/approx_kl 0.01453
wandb:        PPO_1240/train/clip_fraction 0.19754
wandb:           PPO_1240/train/clip_range 0.2
wandb:         PPO_1240/train/entropy_loss -5.6562
wandb:   PPO_1240/train/explained_variance 0.95979
wandb:        PPO_1240/train/learning_rate 0.0003
wandb:                 PPO_1240/train/loss 48.78587
wandb: PPO_1240/train/policy_gradient_loss -0.00533
wandb:                  PPO_1240/train/std 0.54352
wandb:           PPO_1240/train/value_loss 195.13499
wandb:                PPO_1250/global_step 212992
wandb:        PPO_1250/rollout/ep_len_mean 200.0
wandb:        PPO_1250/rollout/ep_rew_mean -546.62732
wandb:                   PPO_1250/time/fps 1143.0
wandb:            PPO_1250/train/approx_kl 0.01687
wandb:        PPO_1250/train/clip_fraction 0.20969
wandb:           PPO_1250/train/clip_range 0.2
wandb:         PPO_1250/train/entropy_loss -5.30308
wandb:   PPO_1250/train/explained_variance 0.97428
wandb:        PPO_1250/train/learning_rate 0.0003
wandb:                 PPO_1250/train/loss 55.35489
wandb: PPO_1250/train/policy_gradient_loss -0.00349
wandb:                  PPO_1250/train/std 0.51619
wandb:           PPO_1250/train/value_loss 252.42143
wandb:                PPO_1259/global_step 212992
wandb:        PPO_1259/rollout/ep_len_mean 200.0
wandb:        PPO_1259/rollout/ep_rew_mean -558.95416
wandb:                   PPO_1259/time/fps 1139.0
wandb:            PPO_1259/train/approx_kl 0.01479
wandb:        PPO_1259/train/clip_fraction 0.19655
wandb:           PPO_1259/train/clip_range 0.2
wandb:         PPO_1259/train/entropy_loss -4.92791
wandb:   PPO_1259/train/explained_variance 0.98141
wandb:        PPO_1259/train/learning_rate 0.0003
wandb:                 PPO_1259/train/loss 202.35579
wandb: PPO_1259/train/policy_gradient_loss -0.0024
wandb:                  PPO_1259/train/std 0.48895
wandb:           PPO_1259/train/value_loss 439.05566
wandb:                PPO_1270/global_step 212992
wandb:        PPO_1270/rollout/ep_len_mean 200.0
wandb:        PPO_1270/rollout/ep_rew_mean -539.10602
wandb:                   PPO_1270/time/fps 1140.0
wandb:            PPO_1270/train/approx_kl 0.01587
wandb:        PPO_1270/train/clip_fraction 0.19372
wandb:           PPO_1270/train/clip_range 0.2
wandb:         PPO_1270/train/entropy_loss -4.69978
wandb:   PPO_1270/train/explained_variance 0.98389
wandb:        PPO_1270/train/learning_rate 0.0003
wandb:                 PPO_1270/train/loss 68.00609
wandb: PPO_1270/train/policy_gradient_loss -0.00239
wandb:                  PPO_1270/train/std 0.47436
wandb:           PPO_1270/train/value_loss 434.90451
wandb:                PPO_1280/global_step 212992
wandb:        PPO_1280/rollout/ep_len_mean 200.0
wandb:        PPO_1280/rollout/ep_rew_mean -560.3999
wandb:                   PPO_1280/time/fps 1137.0
wandb:            PPO_1280/train/approx_kl 0.01374
wandb:        PPO_1280/train/clip_fraction 0.17939
wandb:           PPO_1280/train/clip_range 0.2
wandb:         PPO_1280/train/entropy_loss -4.47682
wandb:   PPO_1280/train/explained_variance 0.98327
wandb:        PPO_1280/train/learning_rate 0.0003
wandb:                 PPO_1280/train/loss 345.98773
wandb: PPO_1280/train/policy_gradient_loss -2e-05
wandb:                  PPO_1280/train/std 0.45847
wandb:           PPO_1280/train/value_loss 510.14438
wandb:                PPO_1290/global_step 212992
wandb:        PPO_1290/rollout/ep_len_mean 200.0
wandb:        PPO_1290/rollout/ep_rew_mean -531.86493
wandb:                   PPO_1290/time/fps 1136.0
wandb:            PPO_1290/train/approx_kl 0.01485
wandb:        PPO_1290/train/clip_fraction 0.2043
wandb:           PPO_1290/train/clip_range 0.2
wandb:         PPO_1290/train/entropy_loss -4.06698
wandb:   PPO_1290/train/explained_variance 0.98787
wandb:        PPO_1290/train/learning_rate 0.0003
wandb:                 PPO_1290/train/loss 218.7187
wandb: PPO_1290/train/policy_gradient_loss 0.00245
wandb:                  PPO_1290/train/std 0.43406
wandb:           PPO_1290/train/value_loss 413.67593
wandb:                    global_mean_eval -446.32667
wandb:                         global_step 212992
wandb:                       mean_reward_0 -518.94126
wandb:                       mean_reward_1 -302.19102
wandb:                      mean_reward_10 -492.35106
wandb:                      mean_reward_11 -515.90527
wandb:                      mean_reward_12 -509.76128
wandb:                      mean_reward_13 -302.17901
wandb:                      mean_reward_14 -396.06879
wandb:                      mean_reward_15 -454.73637
wandb:                      mean_reward_16 -495.7986
wandb:                      mean_reward_17 -516.03493
wandb:                      mean_reward_18 -509.78522
wandb:                      mean_reward_19 -303.62511
wandb:                       mean_reward_2 -396.7451
wandb:                      mean_reward_20 -396.76834
wandb:                      mean_reward_21 -454.5542
wandb:                      mean_reward_22 -492.457
wandb:                      mean_reward_23 -515.45706
wandb:                      mean_reward_24 -522.19984
wandb:                      mean_reward_25 -301.53438
wandb:                      mean_reward_26 -396.64993
wandb:                      mean_reward_27 -458.23435
wandb:                      mean_reward_28 -492.50375
wandb:                      mean_reward_29 -515.34321
wandb:                       mean_reward_3 -454.91491
wandb:                      mean_reward_30 -512.1561
wandb:                      mean_reward_31 -302.8448
wandb:                      mean_reward_32 -396.66378
wandb:                      mean_reward_33 -454.41592
wandb:                      mean_reward_34 -492.39617
wandb:                      mean_reward_35 -515.57934
wandb:                       mean_reward_4 -492.52596
wandb:                       mean_reward_5 -515.79525
wandb:                       mean_reward_6 -516.92858
wandb:                       mean_reward_7 -301.92787
wandb:                       mean_reward_8 -397.05764
wandb:                       mean_reward_9 -454.72885
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -903.27844
wandb:                        std_reward_0 46.61633
wandb:                        std_reward_1 8.85006
wandb:                       std_reward_10 1.15959
wandb:                       std_reward_11 2.03091
wandb:                       std_reward_12 46.14315
wandb:                       std_reward_13 8.63319
wandb:                       std_reward_14 4.30129
wandb:                       std_reward_15 1.94465
wandb:                       std_reward_16 32.5212
wandb:                       std_reward_17 2.554
wandb:                       std_reward_18 46.7723
wandb:                       std_reward_19 8.69108
wandb:                        std_reward_2 4.16572
wandb:                       std_reward_20 4.25072
wandb:                       std_reward_21 1.77389
wandb:                       std_reward_22 1.27333
wandb:                       std_reward_23 2.68374
wandb:                       std_reward_24 46.00697
wandb:                       std_reward_25 8.32086
wandb:                       std_reward_26 4.45913
wandb:                       std_reward_27 36.32484
wandb:                       std_reward_28 1.33386
wandb:                       std_reward_29 3.10278
wandb:                        std_reward_3 2.16091
wandb:                       std_reward_30 45.41987
wandb:                       std_reward_31 9.39927
wandb:                       std_reward_32 4.50236
wandb:                       std_reward_33 1.92786
wandb:                       std_reward_34 1.29477
wandb:                       std_reward_35 2.57301
wandb:                        std_reward_4 1.30989
wandb:                        std_reward_5 2.61089
wandb:                        std_reward_6 47.08549
wandb:                        std_reward_7 8.42247
wandb:                        std_reward_8 4.10876
wandb:                        std_reward_9 1.78956
wandb:                            time/fps 1153.0
wandb:                     train/approx_kl 0.01145
wandb:                 train/clip_fraction 0.14658
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.87514
wandb:            train/explained_variance 0.94396
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.10234
wandb:          train/policy_gradient_loss -0.01207
wandb:                           train/std 0.85747
wandb:                    train/value_loss 31.23324
wandb: 
wandb: Synced pious-star-36: https://wandb.ai/tidiane/meta_rl_context/runs/3d0g5b5s
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_032708-3d0g5b5s/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1210/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1210/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1210/rollout/ep_rew_mean ▁▁▁▄▄▄▅▅▆▆██
wandb:                   PPO_1210/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1210/train/approx_kl ▅▁▂▂▆▄▇▇▅█▆
wandb:        PPO_1210/train/clip_fraction ▅▁▄▁▆▄▆▇▅█▇
wandb:           PPO_1210/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1210/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1210/train/explained_variance ▇██▇▇▁▁▂▂▄▃
wandb:        PPO_1210/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1210/train/loss █▂▁▁▃█▇▃▅▄▅
wandb: PPO_1210/train/policy_gradient_loss ▁█▄▆▃▇▅▃▅▄▇
wandb:                  PPO_1210/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1210/train/value_loss ▁▂▃▅▄▇█▇▇▆█
wandb:                PPO_1220/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1220/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1220/rollout/ep_rew_mean ▂▁▃▅▅▅▅▆▆▇▇█
wandb:                   PPO_1220/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1220/train/approx_kl ▁▃▂▇▆▆▅▄▅█▄
wandb:        PPO_1220/train/clip_fraction ▁▃▄▇▇▆▅▅▆█▇
wandb:           PPO_1220/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1220/train/entropy_loss ▁▂▂▃▃▄▅▅▆▇█
wandb:   PPO_1220/train/explained_variance ▃▁▃▄▄▅▇▅▆▄█
wandb:        PPO_1220/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1220/train/loss ▇▅█▄▄▂▂▃▁▂▇
wandb: PPO_1220/train/policy_gradient_loss ▄▁▃▄▃▄▅▇▆▆█
wandb:                  PPO_1220/train/std █▇▇▆▆▅▄▄▃▂▁
wandb:           PPO_1220/train/value_loss ▆█▇▅▃▄▃▄▃▂▁
wandb:                PPO_1231/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1231/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1231/rollout/ep_rew_mean ▂▂▁▂▄▃▃▃▃▅▅█
wandb:                   PPO_1231/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1231/train/approx_kl ▃▁▂▅▄█▂▆▆▆▇
wandb:        PPO_1231/train/clip_fraction ▄▃▁▅▄▇▄▆█▆▆
wandb:           PPO_1231/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1231/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1231/train/explained_variance ▅█▇▅▁▃▇▄▂▄▇
wandb:        PPO_1231/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1231/train/loss ▃▄▃▁█▆▂▃▄▂▃
wandb: PPO_1231/train/policy_gradient_loss ▄▅▆▄▃▂█▁▅▃█
wandb:                  PPO_1231/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1231/train/value_loss ▃▁▅▁█▄▅▆▅█▆
wandb:                PPO_1241/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1241/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1241/rollout/ep_rew_mean ▃▄▆▅▄▅▅▂▁▇▇█
wandb:                   PPO_1241/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1241/train/approx_kl ▄▆▃▁▃▁▃▁▃▃█
wandb:        PPO_1241/train/clip_fraction ▅▅▄▁▄▆▃▃▅█▇
wandb:           PPO_1241/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1241/train/entropy_loss ▁▂▂▃▄▄▄▅▆▇█
wandb:   PPO_1241/train/explained_variance ▇█▇▇▅▇▄▆▆▁▃
wandb:        PPO_1241/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1241/train/loss ▃▂▂▄▂▂▄█▃▂▁
wandb: PPO_1241/train/policy_gradient_loss ▁▇▄▆▄█▅▅▃▄▇
wandb:                  PPO_1241/train/std █▇▇▆▅▅▄▄▃▂▁
wandb:           PPO_1241/train/value_loss ▃▁▂▃▄▄▆█▅▃▂
wandb:                PPO_1251/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1251/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1251/rollout/ep_rew_mean ▁▁▆▅▆▄▄▄▅▇█▇
wandb:                   PPO_1251/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1251/train/approx_kl ▃██▄█▁▄▃▄▆▄
wandb:        PPO_1251/train/clip_fraction ▃▆█▃▄▁▃▁▂▇▃
wandb:           PPO_1251/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1251/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1251/train/explained_variance ▆▆▁▄▅▅▅▆██▆
wandb:        PPO_1251/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1251/train/loss ▆▅▇▁▁▇▁█▂▃▃
wandb: PPO_1251/train/policy_gradient_loss ▁▁▄▂▂▇▄▅▄▄█
wandb:                  PPO_1251/train/std █▇▆▅▅▅▄▃▃▂▁
wandb:           PPO_1251/train/value_loss ▆█▁▄▄▅▆▆▇▂▃
wandb:                PPO_1261/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1261/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1261/rollout/ep_rew_mean ▅▅▆▂▅▄▁▅▁▄▄█
wandb:                   PPO_1261/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1261/train/approx_kl ▄▂▄▁▃▃▃█▅▄▅
wandb:        PPO_1261/train/clip_fraction ▇▇▅▁▃▂▄█▅▁█
wandb:           PPO_1261/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1261/train/entropy_loss ▁▂▂▃▄▄▄▅▆▇█
wandb:   PPO_1261/train/explained_variance ▁▅▃▁▇▅▆▅█▁▆
wandb:        PPO_1261/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1261/train/loss ▁█▃▃▁▁█▂▄▃▅
wandb: PPO_1261/train/policy_gradient_loss ▄▄▆█▁▁▁▄▅▄▅
wandb:                  PPO_1261/train/std █▇▇▆▅▅▅▄▃▂▁
wandb:           PPO_1261/train/value_loss ▂▁▂▃▃▅▇▆▅█▆
wandb:                PPO_1271/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1271/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1271/rollout/ep_rew_mean ▁▄▅▅▅▇▆▇█▆▆▁
wandb:                   PPO_1271/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1271/train/approx_kl ▂▃▁█▆▇▆▃▇▃▄
wandb:        PPO_1271/train/clip_fraction ▂▆▁███▄▆▆▃▃
wandb:           PPO_1271/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1271/train/entropy_loss ▁▂▃▂▃▄▅▆▆▆█
wandb:   PPO_1271/train/explained_variance ▁▃▂▆▂▇▇█▆▇▃
wandb:        PPO_1271/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1271/train/loss ▆▃▆▆█▁▃▃▃▄█
wandb: PPO_1271/train/policy_gradient_loss ▂▃▂▂██▅▄▄▆▁
wandb:                  PPO_1271/train/std █▇▆▇▆▅▄▃▄▃▁
wandb:           PPO_1271/train/value_loss ▇▄█▃▄▂▃▁▁▄▇
wandb:                PPO_1281/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1281/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1281/rollout/ep_rew_mean ▄▄▂▂▇▁▄▃▁█▆▄
wandb:                   PPO_1281/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1281/train/approx_kl ▆▁▄▅▆▃█▅▅█▆
wandb:        PPO_1281/train/clip_fraction ▆▁▅▆█▄▆▄▃▇▃
wandb:           PPO_1281/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1281/train/entropy_loss ▁▂▃▃▄▄▅▆▆▆█
wandb:   PPO_1281/train/explained_variance ▄▃▁▄▅▄▅█▇▃▅
wandb:        PPO_1281/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1281/train/loss ▁▄▃▅▂▁▂▃█▁▅
wandb: PPO_1281/train/policy_gradient_loss ▆▄▅▅▆▁▅▆▇█▅
wandb:                  PPO_1281/train/std █▆▆▆▅▅▄▄▃▃▁
wandb:           PPO_1281/train/value_loss ▃█▇▆▂▂▃▃▇▁▆
wandb:                PPO_1291/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1291/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1291/rollout/ep_rew_mean ▁▇▅▆▅██▅▅██▇
wandb:                   PPO_1291/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1291/train/approx_kl ▁▃▄▇▄▃▅▆▅█▅
wandb:        PPO_1291/train/clip_fraction ▁▄▅▃▆▄▆█▂▆▆
wandb:           PPO_1291/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1291/train/entropy_loss ▁▁▂▃▄▅▅▆▆▇█
wandb:   PPO_1291/train/explained_variance ▄▅▅▅▁▁▂█▅▂▆
wandb:        PPO_1291/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1291/train/loss ▇▁█▆█▂▅▁▃▁▁
wandb: PPO_1291/train/policy_gradient_loss ▅▅▃▁▆▄█▇▃▆█
wandb:                  PPO_1291/train/std ██▇▆▅▄▄▃▃▂▁
wandb:           PPO_1291/train/value_loss █▃▃▆▃▄▇▁█▆▇
wandb:                    global_mean_eval ▁▄▃▅▆█████
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▃▁▂▄▆█▇▇▆▆
wandb:                       mean_reward_1 ▁▃▂▃▃▅▇█▇█
wandb:                      mean_reward_10 ▁▅▄▅▇▇████
wandb:                      mean_reward_11 ▁▅▄▄▇▇████
wandb:                      mean_reward_12 ▃▁▂▄▆█▇▇▆▆
wandb:                      mean_reward_13 ▁▂▂▃▃▅▇█▇█
wandb:                      mean_reward_14 ▁▄▃▅▆▇▇███
wandb:                      mean_reward_15 ▁▅▄▅▇█▇███
wandb:                      mean_reward_16 ▁▅▃▅▇▇████
wandb:                      mean_reward_17 ▁▅▄▄▇▇████
wandb:                      mean_reward_18 ▃▁▂▄▆█▇▇▆▆
wandb:                      mean_reward_19 ▁▃▂▃▃▅▇█▇█
wandb:                       mean_reward_2 ▁▄▃▅▆▇▇███
wandb:                      mean_reward_20 ▁▄▃▅▆▇████
wandb:                      mean_reward_21 ▁▅▃▅▇█▇▇██
wandb:                      mean_reward_22 ▁▅▄▅▇▇████
wandb:                      mean_reward_23 ▁▅▄▄▇▇████
wandb:                      mean_reward_24 ▃▁▂▄▆█▇▇▆▆
wandb:                      mean_reward_25 ▁▂▂▃▃▅▇█▇█
wandb:                      mean_reward_26 ▁▅▄▅▆▇████
wandb:                      mean_reward_27 ▁▅▃▅▇█▇▇██
wandb:                      mean_reward_28 ▁▅▄▅▇▇████
wandb:                      mean_reward_29 ▁▅▄▄▇▇████
wandb:                       mean_reward_3 ▁▅▃▅▇█▇███
wandb:                      mean_reward_30 ▃▁▂▄▆█▇▇▆▆
wandb:                      mean_reward_31 ▁▂▂▃▃▅▇█▇█
wandb:                      mean_reward_32 ▁▄▃▅▆▇▇███
wandb:                      mean_reward_33 ▁▅▃▅▇█▇▇██
wandb:                      mean_reward_34 ▁▅▄▅▇▇████
wandb:                      mean_reward_35 ▁▅▄▄▇▇████
wandb:                       mean_reward_4 ▁▅▄▅▇▇████
wandb:                       mean_reward_5 ▁▅▄▄▇▇████
wandb:                       mean_reward_6 ▃▁▂▄▆█▇▇▆▆
wandb:                       mean_reward_7 ▁▃▂▃▃▅▇█▇█
wandb:                       mean_reward_8 ▁▅▄▅▆▇████
wandb:                       mean_reward_9 ▁▅▄▅▇█▇███
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▄▄▅▅▇▆█
wandb:                        std_reward_0 █▂▄▂▁▁▁▁▂▂
wandb:                        std_reward_1 █▁▁▂▁▁▁▁▁▁
wandb:                       std_reward_10 █▂▁▃▂▂▁▁▁▁
wandb:                       std_reward_11 █▂▂▂▂▂▁▁▁▁
wandb:                       std_reward_12 █▂▄▂▁▁▁▁▂▂
wandb:                       std_reward_13 █▁▁▂▁▁▁▁▁▁
wandb:                       std_reward_14 █▁▂▂▃▂▁▁▁▁
wandb:                       std_reward_15 █▁▂▂▁▁▂▂▁▁
wandb:                       std_reward_16 █▂▁▃▂▂▁▁▁▁
wandb:                       std_reward_17 █▂▂▂▂▂▁▁▁▁
wandb:                       std_reward_18 █▂▄▂▁▁▁▁▂▂
wandb:                       std_reward_19 █▁▁▂▁▁▁▁▁▁
wandb:                        std_reward_2 █▁▂▂▃▂▁▁▁▁
wandb:                       std_reward_20 █▁▂▂▂▂▁▁▁▁
wandb:                       std_reward_21 █▁▂▂▁▁▂▂▁▂
wandb:                       std_reward_22 █▂▁▂▂▂▁▁▁▁
wandb:                       std_reward_23 █▂▂▂▂▂▁▁▁▁
wandb:                       std_reward_24 █▃▄▂▁▁▁▁▂▂
wandb:                       std_reward_25 █▁▁▂▁▁▁▁▁▁
wandb:                       std_reward_26 █▁▁▁▂▂▁▁▁▁
wandb:                       std_reward_27 █▁▃▂▂▁▂▂▂▂
wandb:                       std_reward_28 █▂▁▂▂▂▁▁▁▁
wandb:                       std_reward_29 █▂▂▃▂▂▁▁▁▁
wandb:                        std_reward_3 █▁▃▂▁▁▂▂▁▁
wandb:                       std_reward_30 █▂▃▂▁▁▁▁▂▂
wandb:                       std_reward_31 █▁▁▂▁▁▁▁▁▁
wandb:                       std_reward_32 █▁▂▁▂▂▁▁▁▁
wandb:                       std_reward_33 █▁▂▂▂▁▂▂▁▂
wandb:                       std_reward_34 █▂▁▃▂▂▁▁▁▁
wandb:                       std_reward_35 █▂▂▂▂▂▁▁▁▁
wandb:                        std_reward_4 █▂▁▃▂▂▁▁▁▁
wandb:                        std_reward_5 █▂▂▃▂▂▁▁▁▁
wandb:                        std_reward_6 █▂▄▂▁▁▁▁▂▂
wandb:                        std_reward_7 █▁▁▁▁▁▁▁▁▁
wandb:                        std_reward_8 █▁▂▁▂▂▁▁▁▁
wandb:                        std_reward_9 █▁▂▁▁▁▂▂▁▁
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▅█▁▂▄▅▅▆▇▆
wandb:                 train/clip_fraction ▂▃▄▃▄▁▃▅▆▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▄▆▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▃▃▂▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▅▆█▆▄▃▂▂▁▂
wandb:                           train/std █▇▇▇▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1210/global_step 212992
wandb:        PPO_1210/rollout/ep_len_mean 200.0
wandb:        PPO_1210/rollout/ep_rew_mean -788.40765
wandb:                   PPO_1210/time/fps 1191.0
wandb:            PPO_1210/train/approx_kl 0.01135
wandb:        PPO_1210/train/clip_fraction 0.14095
wandb:           PPO_1210/train/clip_range 0.2
wandb:         PPO_1210/train/entropy_loss -7.80498
wandb:   PPO_1210/train/explained_variance 0.96208
wandb:        PPO_1210/train/learning_rate 0.0003
wandb:                 PPO_1210/train/loss 57.57802
wandb: PPO_1210/train/policy_gradient_loss -0.00691
wandb:                  PPO_1210/train/std 0.73685
wandb:           PPO_1210/train/value_loss 160.16556
wandb:                PPO_1220/global_step 212992
wandb:        PPO_1220/rollout/ep_len_mean 200.0
wandb:        PPO_1220/rollout/ep_rew_mean -651.41107
wandb:                   PPO_1220/time/fps 1182.0
wandb:            PPO_1220/train/approx_kl 0.01272
wandb:        PPO_1220/train/clip_fraction 0.18301
wandb:           PPO_1220/train/clip_range 0.2
wandb:         PPO_1220/train/entropy_loss -6.96896
wandb:   PPO_1220/train/explained_variance 0.97788
wandb:        PPO_1220/train/learning_rate 0.0003
wandb:                 PPO_1220/train/loss 74.41776
wandb: PPO_1220/train/policy_gradient_loss -0.00414
wandb:                  PPO_1220/train/std 0.65368
wandb:           PPO_1220/train/value_loss 49.99812
wandb:                PPO_1231/global_step 212992
wandb:        PPO_1231/rollout/ep_len_mean 200.0
wandb:        PPO_1231/rollout/ep_rew_mean -585.10406
wandb:                   PPO_1231/time/fps 1180.0
wandb:            PPO_1231/train/approx_kl 0.01641
wandb:        PPO_1231/train/clip_fraction 0.20732
wandb:           PPO_1231/train/clip_range 0.2
wandb:         PPO_1231/train/entropy_loss -6.1254
wandb:   PPO_1231/train/explained_variance 0.97751
wandb:        PPO_1231/train/learning_rate 0.0003
wandb:                 PPO_1231/train/loss 26.17278
wandb: PPO_1231/train/policy_gradient_loss -0.00252
wandb:                  PPO_1231/train/std 0.57978
wandb:           PPO_1231/train/value_loss 56.15229
wandb:                PPO_1241/global_step 212992
wandb:        PPO_1241/rollout/ep_len_mean 200.0
wandb:        PPO_1241/rollout/ep_rew_mean -580.87579
wandb:                   PPO_1241/time/fps 1186.0
wandb:            PPO_1241/train/approx_kl 0.01955
wandb:        PPO_1241/train/clip_fraction 0.23644
wandb:           PPO_1241/train/clip_range 0.2
wandb:         PPO_1241/train/entropy_loss -5.3988
wandb:   PPO_1241/train/explained_variance 0.96249
wandb:        PPO_1241/train/learning_rate 0.0003
wandb:                 PPO_1241/train/loss 7.68504
wandb: PPO_1241/train/policy_gradient_loss -0.00277
wandb:                  PPO_1241/train/std 0.52311
wandb:           PPO_1241/train/value_loss 36.56745
wandb:                PPO_1251/global_step 212992
wandb:        PPO_1251/rollout/ep_len_mean 200.0
wandb:        PPO_1251/rollout/ep_rew_mean -535.39185
wandb:                   PPO_1251/time/fps 1182.0
wandb:            PPO_1251/train/approx_kl 0.01691
wandb:        PPO_1251/train/clip_fraction 0.22239
wandb:           PPO_1251/train/clip_range 0.2
wandb:         PPO_1251/train/entropy_loss -4.77797
wandb:   PPO_1251/train/explained_variance 0.96929
wandb:        PPO_1251/train/learning_rate 0.0003
wandb:                 PPO_1251/train/loss 18.00958
wandb: PPO_1251/train/policy_gradient_loss 0.00022
wandb:                  PPO_1251/train/std 0.4787
wandb:           PPO_1251/train/value_loss 39.68171
wandb:                PPO_1261/global_step 212992
wandb:        PPO_1261/rollout/ep_len_mean 200.0
wandb:        PPO_1261/rollout/ep_rew_mean -512.62616
wandb:                   PPO_1261/time/fps 1184.0
wandb:            PPO_1261/train/approx_kl 0.01773
wandb:        PPO_1261/train/clip_fraction 0.23289
wandb:           PPO_1261/train/clip_range 0.2
wandb:         PPO_1261/train/entropy_loss -4.34815
wandb:   PPO_1261/train/explained_variance 0.98169
wandb:        PPO_1261/train/learning_rate 0.0003
wandb:                 PPO_1261/train/loss 37.42626
wandb: PPO_1261/train/policy_gradient_loss -0.0006
wandb:                  PPO_1261/train/std 0.45022
wandb:           PPO_1261/train/value_loss 90.8445
wandb:                PPO_1271/global_step 212992
wandb:        PPO_1271/rollout/ep_len_mean 200.0
wandb:        PPO_1271/rollout/ep_rew_mean -588.474
wandb:                   PPO_1271/time/fps 1182.0
wandb:            PPO_1271/train/approx_kl 0.01752
wandb:        PPO_1271/train/clip_fraction 0.20226
wandb:           PPO_1271/train/clip_range 0.2
wandb:         PPO_1271/train/entropy_loss -3.95459
wandb:   PPO_1271/train/explained_variance 0.98182
wandb:        PPO_1271/train/learning_rate 0.0003
wandb:                 PPO_1271/train/loss 76.62786
wandb: PPO_1271/train/policy_gradient_loss -0.00256
wandb:                  PPO_1271/train/std 0.42604
wandb:           PPO_1271/train/value_loss 174.8204
wandb:                PPO_1281/global_step 212992
wandb:        PPO_1281/rollout/ep_len_mean 200.0
wandb:        PPO_1281/rollout/ep_rew_mean -536.20502
wandb:                   PPO_1281/time/fps 1181.0
wandb:            PPO_1281/train/approx_kl 0.01875
wandb:        PPO_1281/train/clip_fraction 0.2168
wandb:           PPO_1281/train/clip_range 0.2
wandb:         PPO_1281/train/entropy_loss -3.68467
wandb:   PPO_1281/train/explained_variance 0.98688
wandb:        PPO_1281/train/learning_rate 0.0003
wandb:                 PPO_1281/train/loss 157.26497
wandb: PPO_1281/train/policy_gradient_loss -0.00046
wandb:                  PPO_1281/train/std 0.41022
wandb:           PPO_1281/train/value_loss 180.69862
wandb:                PPO_1291/global_step 212992
wandb:        PPO_1291/rollout/ep_len_mean 200.0
wandb:        PPO_1291/rollout/ep_rew_mean -521.91608
wandb:                   PPO_1291/time/fps 1180.0
wandb:            PPO_1291/train/approx_kl 0.02073
wandb:        PPO_1291/train/clip_fraction 0.26346
wandb:           PPO_1291/train/clip_range 0.2
wandb:         PPO_1291/train/entropy_loss -3.1808
wandb:   PPO_1291/train/explained_variance 0.98795
wandb:        PPO_1291/train/learning_rate 0.0003
wandb:                 PPO_1291/train/loss 45.61468
wandb: PPO_1291/train/policy_gradient_loss 0.00173
wandb:                  PPO_1291/train/std 0.38121
wandb:           PPO_1291/train/value_loss 204.13251
wandb:                    global_mean_eval -438.69847
wandb:                         global_step 212992
wandb:                       mean_reward_0 -408.86521
wandb:                       mean_reward_1 -347.15632
wandb:                      mean_reward_10 -470.13663
wandb:                      mean_reward_11 -480.80342
wandb:                      mean_reward_12 -409.12977
wandb:                      mean_reward_13 -346.20766
wandb:                      mean_reward_14 -450.36335
wandb:                      mean_reward_15 -475.85361
wandb:                      mean_reward_16 -469.8084
wandb:                      mean_reward_17 -480.70023
wandb:                      mean_reward_18 -411.72089
wandb:                      mean_reward_19 -346.92016
wandb:                       mean_reward_2 -450.75739
wandb:                      mean_reward_20 -450.87169
wandb:                      mean_reward_21 -475.79486
wandb:                      mean_reward_22 -470.01318
wandb:                      mean_reward_23 -480.9897
wandb:                      mean_reward_24 -409.33447
wandb:                      mean_reward_25 -347.33164
wandb:                      mean_reward_26 -450.74775
wandb:                      mean_reward_27 -474.99392
wandb:                      mean_reward_28 -469.8041
wandb:                      mean_reward_29 -480.85007
wandb:                       mean_reward_3 -474.06227
wandb:                      mean_reward_30 -410.97995
wandb:                      mean_reward_31 -346.59149
wandb:                      mean_reward_32 -449.96002
wandb:                      mean_reward_33 -474.79858
wandb:                      mean_reward_34 -469.60242
wandb:                      mean_reward_35 -481.23596
wandb:                       mean_reward_4 -469.87121
wandb:                       mean_reward_5 -480.91332
wandb:                       mean_reward_6 -403.96404
wandb:                       mean_reward_7 -347.01794
wandb:                       mean_reward_8 -450.09075
wandb:                       mean_reward_9 -474.90244
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -903.27844
wandb:                        std_reward_0 20.86713
wandb:                        std_reward_1 4.7896
wandb:                       std_reward_10 2.3074
wandb:                       std_reward_11 3.39347
wandb:                       std_reward_12 22.56524
wandb:                       std_reward_13 5.61155
wandb:                       std_reward_14 4.59227
wandb:                       std_reward_15 6.82469
wandb:                       std_reward_16 2.36107
wandb:                       std_reward_17 3.55325
wandb:                       std_reward_18 21.55566
wandb:                       std_reward_19 5.04644
wandb:                        std_reward_2 4.65654
wandb:                       std_reward_20 4.01631
wandb:                       std_reward_21 8.01014
wandb:                       std_reward_22 3.07561
wandb:                       std_reward_23 3.12424
wandb:                       std_reward_24 22.3956
wandb:                       std_reward_25 5.39846
wandb:                       std_reward_26 4.21486
wandb:                       std_reward_27 7.87624
wandb:                       std_reward_28 2.72356
wandb:                       std_reward_29 3.58779
wandb:                        std_reward_3 7.10566
wandb:                       std_reward_30 20.74194
wandb:                       std_reward_31 5.36139
wandb:                       std_reward_32 4.61993
wandb:                       std_reward_33 8.11146
wandb:                       std_reward_34 1.72246
wandb:                       std_reward_35 3.75006
wandb:                        std_reward_4 2.61517
wandb:                        std_reward_5 3.27491
wandb:                        std_reward_6 23.05076
wandb:                        std_reward_7 5.32022
wandb:                        std_reward_8 3.92851
wandb:                        std_reward_9 7.91886
wandb:                            time/fps 1153.0
wandb:                     train/approx_kl 0.01145
wandb:                 train/clip_fraction 0.14658
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.87514
wandb:            train/explained_variance 0.94396
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.10234
wandb:          train/policy_gradient_loss -0.01207
wandb:                           train/std 0.85747
wandb:                    train/value_loss 31.23324
wandb: 
wandb: Synced curious-universe-39: https://wandb.ai/tidiane/meta_rl_context/runs/2znu6ayn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_032708-2znu6ayn/logs
wandb: Waiting for W&B process to finish... (success).
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1212/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1212/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1212/rollout/ep_rew_mean ▁▁▃▃▃▄▅▄▅▆▇█
wandb:                   PPO_1212/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1212/train/approx_kl ▄▂▁▄▃▄█▄▃▄▅
wandb:        PPO_1212/train/clip_fraction ▃▂▁▃▄▃█▆▃▅▅
wandb:           PPO_1212/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1212/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1212/train/explained_variance █▄▆▅▄▅▂▇▁▅▃
wandb:        PPO_1212/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1212/train/loss ▁▁▂▃▃█▃▂▃▄▄
wandb: PPO_1212/train/policy_gradient_loss ▅▆█▆▆█▁▃▅█▇
wandb:                  PPO_1212/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1212/train/value_loss ▁▂▄▄▄▆▆▆█▆▆
wandb:                PPO_1222/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1222/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1222/rollout/ep_rew_mean ▁▁▂▃▄▄▆▆▆▆▇█
wandb:                   PPO_1222/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1222/train/approx_kl ▁▂▃▃▁▆█▄▄▁▆
wandb:        PPO_1222/train/clip_fraction ▁▃▅▄▄▅█▆▄▅▆
wandb:           PPO_1222/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1222/train/entropy_loss ▁▁▂▃▃▄▅▅▆▆█
wandb:   PPO_1222/train/explained_variance ▆▂▄█▇█▁▂▁▆█
wandb:        PPO_1222/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1222/train/loss ▄▃█▅▁▂▄▃▃▂▁
wandb: PPO_1222/train/policy_gradient_loss ▅▂▂▃▂▁▁▇▇▆█
wandb:                  PPO_1222/train/std ██▇▇▆▅▄▄▃▃▁
wandb:           PPO_1222/train/value_loss █▇▅▅▅▄▅▃▇▁▁
wandb:                PPO_1232/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1232/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1232/rollout/ep_rew_mean ▆█▂▅▅▇▇▃▂▃▁▇
wandb:                   PPO_1232/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1232/train/approx_kl ▅█▂▇▅▅▁▂▃▅▇
wandb:        PPO_1232/train/clip_fraction ▆▇▁▇▆█▂▃▄▄▄
wandb:           PPO_1232/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1232/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1232/train/explained_variance ▁▅▁█▅█▅▆▅▆█
wandb:        PPO_1232/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1232/train/loss ▁▁▄▃▆▆▇▅█▄▁
wandb: PPO_1232/train/policy_gradient_loss ▆▅▆▃▆██▆▁▆▃
wandb:                  PPO_1232/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1232/train/value_loss ▂▁▃▂▃▁▃▆▅█▇
wandb:                PPO_1242/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1242/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1242/rollout/ep_rew_mean ▆█▆▇▃▄▅▄▂▄▂▁
wandb:                   PPO_1242/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1242/train/approx_kl ▅█▅▇▅▁█▄▅▅▄
wandb:        PPO_1242/train/clip_fraction ▃█▃▅▂▁█▁▄▃▄
wandb:           PPO_1242/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1242/train/entropy_loss ▁▂▃▃▅▅▆▇▆▇█
wandb:   PPO_1242/train/explained_variance ▂▆▇▁▄▄█▆█▅▆
wandb:        PPO_1242/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1242/train/loss ▁▂▃▂▁▃▂▄█▅▃
wandb: PPO_1242/train/policy_gradient_loss ▅▄▄▄▄█▅▇▆▆▁
wandb:                  PPO_1242/train/std █▆▆▅▄▃▃▂▃▂▁
wandb:           PPO_1242/train/value_loss ▄▁▃▅▆▇▃▆▅▇█
wandb:                PPO_1252/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1252/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1252/rollout/ep_rew_mean ▁▂▃▄▆▁▅▃▅▅█▇
wandb:                   PPO_1252/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1252/train/approx_kl ▁█▁▃▅▅▅▂▄▆▃
wandb:        PPO_1252/train/clip_fraction ▁▆▁▇▄▃▆▁█▆▃
wandb:           PPO_1252/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1252/train/entropy_loss ▁▂▃▃▃▃▄▅▆▆█
wandb:   PPO_1252/train/explained_variance ▂▃▃█▅▆▁▆▄▃▅
wandb:        PPO_1252/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1252/train/loss █▇▂▃▂▄▃▂▇▇▁
wandb: PPO_1252/train/policy_gradient_loss ▁▄▆▆▅▅▇▃█▄▅
wandb:                  PPO_1252/train/std ██▆▆▆▆▅▄▄▃▁
wandb:           PPO_1252/train/value_loss █▇▇▁▆█▅▄▄▅▅
wandb:                PPO_1262/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1262/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1262/rollout/ep_rew_mean ▂▁▃▃▄▂▆█▄▄▁▃
wandb:                   PPO_1262/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1262/train/approx_kl ▆▃▁█▄▃▇▂▄▁▅
wandb:        PPO_1262/train/clip_fraction ▃▂▃▇▄▂█▁▁▁▆
wandb:           PPO_1262/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1262/train/entropy_loss ▁▂▃▄▄▄▅▅▆▇█
wandb:   PPO_1262/train/explained_variance ▁▆▂█▃▄▅▆▄▅█
wandb:        PPO_1262/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1262/train/loss ▁▂▁▇█▇▁▁▃▃▁
wandb: PPO_1262/train/policy_gradient_loss ▇▃▂▇▅▃▇▂▁▄█
wandb:                  PPO_1262/train/std █▇▆▅▅▅▄▃▃▂▁
wandb:           PPO_1262/train/value_loss ▆▄▇▄▄▇▃▄▇█▁
wandb:                PPO_1272/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1272/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1272/rollout/ep_rew_mean ▃▁▅▅▃▃▆█▇▇█▆
wandb:                   PPO_1272/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1272/train/approx_kl ▂▁▆▅▆▆██▅▅▃
wandb:        PPO_1272/train/clip_fraction ▃▁▆▆▆▆█▆▃▄▂
wandb:           PPO_1272/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1272/train/entropy_loss ▁▂▁▂▂▂▃▅▆▇█
wandb:   PPO_1272/train/explained_variance █▅▇▇▅▃▁▇██▁
wandb:        PPO_1272/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1272/train/loss ▅▅▃▂█▁▁▁▂▁▁
wandb: PPO_1272/train/policy_gradient_loss ▁▅▄▇▇▄▇█▇█▃
wandb:                  PPO_1272/train/std █▇█▇▇▆▅▄▃▂▁
wandb:           PPO_1272/train/value_loss ▇█▅▂▅▄▁▃▂▃▅
wandb:                PPO_1282/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1282/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1282/rollout/ep_rew_mean █▂▁▃▆▆▃▂▂▅▁▃
wandb:                   PPO_1282/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1282/train/approx_kl ▅▂▁▇▄█▄▇▁▆▇
wandb:        PPO_1282/train/clip_fraction ▄▂▂▆▇█▄▅▁▇█
wandb:           PPO_1282/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1282/train/entropy_loss ▁▁▂▃▄▄▅▆▇██
wandb:   PPO_1282/train/explained_variance ▅█▇▇▄█▆█▅▁█
wandb:        PPO_1282/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1282/train/loss ▃█▂▂▁▂▁▂▂▁▁
wandb: PPO_1282/train/policy_gradient_loss ▅▃▃▆▂▃▁▄▃█▆
wandb:                  PPO_1282/train/std ██▇▆▅▅▄▃▂▁▁
wandb:           PPO_1282/train/value_loss ▂▆▆▃▃▁▅▄█▄▃
wandb:                PPO_1292/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1292/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1292/rollout/ep_rew_mean ▅▁▆▅█▇▃▄▃▃█▃
wandb:                   PPO_1292/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1292/train/approx_kl ▅▃▅▂▁▃▃▄▇▁█
wandb:        PPO_1292/train/clip_fraction ▄▃▇▂▁▇▃▅▅▂█
wandb:           PPO_1292/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1292/train/entropy_loss ▁▂▃▄▄▄▅▆▆▇█
wandb:   PPO_1292/train/explained_variance ▃▁█▇▁▂▆██▅▅
wandb:        PPO_1292/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1292/train/loss ▂▁▅▂▂█▂▃▁▄▂
wandb: PPO_1292/train/policy_gradient_loss ▄▁▁▁▂▆▃▃▆▁█
wandb:                  PPO_1292/train/std █▇▆▅▅▅▄▃▃▂▁
wandb:           PPO_1292/train/value_loss ▁▇▃▅▇▂█▆▆▇▃
wandb:                    global_mean_eval ▁▄██▇█▇███
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▃▇▁▁▄▅██▇
wandb:                       mean_reward_1 ▁▆███▇▇▇▆▆
wandb:                      mean_reward_10 ▁▂▇███▇▇▇▇
wandb:                      mean_reward_11 ▁▂▇███████
wandb:                      mean_reward_12 ▂▃▇▁▁▄▅██▇
wandb:                      mean_reward_13 ▁▆███▇▇▇▆▆
wandb:                      mean_reward_14 ▁▅▇██▇▇▇▆▇
wandb:                      mean_reward_15 ▁▃▇██▇▇▇▇▇
wandb:                      mean_reward_16 ▁▂▇███▇▇▇▇
wandb:                      mean_reward_17 ▁▂▇███▇███
wandb:                      mean_reward_18 ▁▃▇▁▁▄▅██▇
wandb:                      mean_reward_19 ▁▆███▇▇▇▆▆
wandb:                       mean_reward_2 ▁▅▇██▇▇▇▆▇
wandb:                      mean_reward_20 ▁▅▇██▇▇▇▆▇
wandb:                      mean_reward_21 ▁▃▇██▇▇▇▇▇
wandb:                      mean_reward_22 ▁▂▇███▇▇▇▇
wandb:                      mean_reward_23 ▁▂▇███▇███
wandb:                      mean_reward_24 ▁▃▇▁▁▄▅██▇
wandb:                      mean_reward_25 ▁▆███▇▇▇▆▆
wandb:                      mean_reward_26 ▁▅▇██▇▇▇▆▆
wandb:                      mean_reward_27 ▁▃▇██▇▇▇▇▇
wandb:                      mean_reward_28 ▁▂▇███▇▇▇▇
wandb:                      mean_reward_29 ▁▂▇███████
wandb:                       mean_reward_3 ▁▃▇██▇▇▇▇▇
wandb:                      mean_reward_30 ▁▃▇▁▁▄▅██▇
wandb:                      mean_reward_31 ▁▅███▇▇▇▆▆
wandb:                      mean_reward_32 ▁▅▇██▇▇▇▆▆
wandb:                      mean_reward_33 ▁▃▇██▇▇▇▇▇
wandb:                      mean_reward_34 ▁▂▇███▇▇▇▇
wandb:                      mean_reward_35 ▁▂▇███▇███
wandb:                       mean_reward_4 ▁▂▇███▇▇▇▇
wandb:                       mean_reward_5 ▁▂▇███▇███
wandb:                       mean_reward_6 ▁▃▇▁▁▄▅███
wandb:                       mean_reward_7 ▁▆███▇▇▇▆▆
wandb:                       mean_reward_8 ▁▅▇██▇▇▇▆▇
wandb:                       mean_reward_9 ▁▃▇██▇▇▇▇▇
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▄▄▅▅▇▆█
wandb:                        std_reward_0 █▃▄▂▁▂▄▁▅▄
wandb:                        std_reward_1 █▆▁▃▂▃▃▁▃▄
wandb:                       std_reward_10 ▄█▂▂▂▆▁▁▁▂
wandb:                       std_reward_11 ▄▇▂▂▂█▁▁▁▁
wandb:                       std_reward_12 █▃▄▂▁▂▄▂▄▄
wandb:                       std_reward_13 █▅▁▃▂▃▃▁▂▄
wandb:                       std_reward_14 ▃█▁▁▁▂▂▁▁▁
wandb:                       std_reward_15 ▂█▁▁▁▂▁▁▁▁
wandb:                       std_reward_16 ▄█▂▂▂▁▁▁▁▂
wandb:                       std_reward_17 ▄▆▂▂▂██▁▁▁
wandb:                       std_reward_18 █▂▄▄▁▁▄▂▆▆
wandb:                       std_reward_19 █▅▁▂▂▃▃▁▃▅
wandb:                        std_reward_2 ▃█▁▁▁▁▁▁▁▂
wandb:                       std_reward_20 ▃█▁▁▁▂▁▁▁▁
wandb:                       std_reward_21 ▂█▁▁▁▂▂▁▁▁
wandb:                       std_reward_22 ▄█▂▂▂▂▁▁▁▂
wandb:                       std_reward_23 ▅█▂▂▃▂█▁▁▂
wandb:                       std_reward_24 █▃▅▃▁▃▄▂▄▅
wandb:                       std_reward_25 █▆▁▃▇▄▇▁▂▆
wandb:                       std_reward_26 ▃█▁▁▁▂▂▁▁▂
wandb:                       std_reward_27 ▃█▁▁▁▂▁▁▁▁
wandb:                       std_reward_28 ▄█▂▂▂▁▂▁▁▂
wandb:                       std_reward_29 ▄█▂▃▂▁▁▁▁▂
wandb:                        std_reward_3 ▂█▁▁▁▁▂▁▁▁
wandb:                       std_reward_30 █▂▄▃▁▂▃▂▄▄
wandb:                       std_reward_31 █▆▁▃▂▆▄▁▃▅
wandb:                       std_reward_32 ▃█▁▁▁▂▂▁▁▁
wandb:                       std_reward_33 ▃█▁▁▁▁▁▁▁▂
wandb:                       std_reward_34 ▄█▂▂▂▆▁▁▁▂
wandb:                       std_reward_35 ▅█▂▃▃▁█▁▁▂
wandb:                        std_reward_4 ▃█▂▂▂▃▇▁▁▂
wandb:                        std_reward_5 ▅█▂▂▂█▃▁▁▁
wandb:                        std_reward_6 █▄▅▄▁▂▄▂▅▄
wandb:                        std_reward_7 █▅▁▂▂▃▄▁▃▅
wandb:                        std_reward_8 ▃█▁▁▁▂▁▁▁▁
wandb:                        std_reward_9 ▂█▁▁▁▂▁▁▁▁
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▅█▁▂▄▅▅▆▇▆
wandb:                 train/clip_fraction ▂▃▄▃▄▁▃▅▆▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▄▆▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▃▃▂▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▅▆█▆▄▃▂▂▁▂
wandb:                           train/std █▇▇▇▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1212/global_step 212992
wandb:        PPO_1212/rollout/ep_len_mean 200.0
wandb:        PPO_1212/rollout/ep_rew_mean -753.11841
wandb:                   PPO_1212/time/fps 1179.0
wandb:            PPO_1212/train/approx_kl 0.0117
wandb:        PPO_1212/train/clip_fraction 0.14111
wandb:           PPO_1212/train/clip_range 0.2
wandb:         PPO_1212/train/entropy_loss -7.89186
wandb:   PPO_1212/train/explained_variance 0.95476
wandb:        PPO_1212/train/learning_rate 0.0003
wandb:                 PPO_1212/train/loss 135.07863
wandb: PPO_1212/train/policy_gradient_loss -0.00751
wandb:                  PPO_1212/train/std 0.74673
wandb:           PPO_1212/train/value_loss 191.58308
wandb:                PPO_1222/global_step 212992
wandb:        PPO_1222/rollout/ep_len_mean 200.0
wandb:        PPO_1222/rollout/ep_rew_mean -564.15497
wandb:                   PPO_1222/time/fps 1181.0
wandb:            PPO_1222/train/approx_kl 0.01317
wandb:        PPO_1222/train/clip_fraction 0.16954
wandb:           PPO_1222/train/clip_range 0.2
wandb:         PPO_1222/train/entropy_loss -7.05137
wandb:   PPO_1222/train/explained_variance 0.96623
wandb:        PPO_1222/train/learning_rate 0.0003
wandb:                 PPO_1222/train/loss 32.48145
wandb: PPO_1222/train/policy_gradient_loss -0.00422
wandb:                  PPO_1222/train/std 0.6621
wandb:           PPO_1222/train/value_loss 96.60732
wandb:                PPO_1232/global_step 212992
wandb:        PPO_1232/rollout/ep_len_mean 200.0
wandb:        PPO_1232/rollout/ep_rew_mean -550.82733
wandb:                   PPO_1232/time/fps 1161.0
wandb:            PPO_1232/train/approx_kl 0.01366
wandb:        PPO_1232/train/clip_fraction 0.15359
wandb:           PPO_1232/train/clip_range 0.2
wandb:         PPO_1232/train/entropy_loss -6.49534
wandb:   PPO_1232/train/explained_variance 0.96984
wandb:        PPO_1232/train/learning_rate 0.0003
wandb:                 PPO_1232/train/loss 43.80255
wandb: PPO_1232/train/policy_gradient_loss -0.00377
wandb:                  PPO_1232/train/std 0.61127
wandb:           PPO_1232/train/value_loss 396.86841
wandb:                PPO_1242/global_step 212992
wandb:        PPO_1242/rollout/ep_len_mean 200.0
wandb:        PPO_1242/rollout/ep_rew_mean -690.26276
wandb:                   PPO_1242/time/fps 1166.0
wandb:            PPO_1242/train/approx_kl 0.01078
wandb:        PPO_1242/train/clip_fraction 0.1452
wandb:           PPO_1242/train/clip_range 0.2
wandb:         PPO_1242/train/entropy_loss -6.08313
wandb:   PPO_1242/train/explained_variance 0.97658
wandb:        PPO_1242/train/learning_rate 0.0003
wandb:                 PPO_1242/train/loss 363.82199
wandb: PPO_1242/train/policy_gradient_loss -0.00533
wandb:                  PPO_1242/train/std 0.57697
wandb:           PPO_1242/train/value_loss 782.2677
wandb:                PPO_1252/global_step 212992
wandb:        PPO_1252/rollout/ep_len_mean 200.0
wandb:        PPO_1252/rollout/ep_rew_mean -576.09406
wandb:                   PPO_1252/time/fps 1165.0
wandb:            PPO_1252/train/approx_kl 0.01061
wandb:        PPO_1252/train/clip_fraction 0.13873
wandb:           PPO_1252/train/clip_range 0.2
wandb:         PPO_1252/train/entropy_loss -5.67768
wandb:   PPO_1252/train/explained_variance 0.98077
wandb:        PPO_1252/train/learning_rate 0.0003
wandb:                 PPO_1252/train/loss 100.15254
wandb: PPO_1252/train/policy_gradient_loss -0.00255
wandb:                  PPO_1252/train/std 0.54456
wandb:           PPO_1252/train/value_loss 732.95209
wandb:                PPO_1262/global_step 212992
wandb:        PPO_1262/rollout/ep_len_mean 200.0
wandb:        PPO_1262/rollout/ep_rew_mean -599.93506
wandb:                   PPO_1262/time/fps 1170.0
wandb:            PPO_1262/train/approx_kl 0.01397
wandb:        PPO_1262/train/clip_fraction 0.19852
wandb:           PPO_1262/train/clip_range 0.2
wandb:         PPO_1262/train/entropy_loss -5.23186
wandb:   PPO_1262/train/explained_variance 0.98567
wandb:        PPO_1262/train/learning_rate 0.0003
wandb:                 PPO_1262/train/loss 52.41804
wandb: PPO_1262/train/policy_gradient_loss -0.00021
wandb:                  PPO_1262/train/std 0.51195
wandb:           PPO_1262/train/value_loss 367.24252
wandb:                PPO_1272/global_step 212992
wandb:        PPO_1272/rollout/ep_len_mean 200.0
wandb:        PPO_1272/rollout/ep_rew_mean -566.59949
wandb:                   PPO_1272/time/fps 1167.0
wandb:            PPO_1272/train/approx_kl 0.0139
wandb:        PPO_1272/train/clip_fraction 0.18967
wandb:           PPO_1272/train/clip_range 0.2
wandb:         PPO_1272/train/entropy_loss -4.9559
wandb:   PPO_1272/train/explained_variance 0.97794
wandb:        PPO_1272/train/learning_rate 0.0003
wandb:                 PPO_1272/train/loss 50.6247
wandb: PPO_1272/train/policy_gradient_loss -0.0011
wandb:                  PPO_1272/train/std 0.49314
wandb:           PPO_1272/train/value_loss 347.87076
wandb:                PPO_1282/global_step 212992
wandb:        PPO_1282/rollout/ep_len_mean 200.0
wandb:        PPO_1282/rollout/ep_rew_mean -550.52777
wandb:                   PPO_1282/time/fps 1167.0
wandb:            PPO_1282/train/approx_kl 0.01884
wandb:        PPO_1282/train/clip_fraction 0.24489
wandb:           PPO_1282/train/clip_range 0.2
wandb:         PPO_1282/train/entropy_loss -4.56825
wandb:   PPO_1282/train/explained_variance 0.98319
wandb:        PPO_1282/train/learning_rate 0.0003
wandb:                 PPO_1282/train/loss 21.68955
wandb: PPO_1282/train/policy_gradient_loss 0.00255
wandb:                  PPO_1282/train/std 0.46523
wandb:           PPO_1282/train/value_loss 187.49783
wandb:                PPO_1292/global_step 212992
wandb:        PPO_1292/rollout/ep_len_mean 200.0
wandb:        PPO_1292/rollout/ep_rew_mean -558.2218
wandb:                   PPO_1292/time/fps 1163.0
wandb:            PPO_1292/train/approx_kl 0.01919
wandb:        PPO_1292/train/clip_fraction 0.24666
wandb:           PPO_1292/train/clip_range 0.2
wandb:         PPO_1292/train/entropy_loss -4.12414
wandb:   PPO_1292/train/explained_variance 0.98103
wandb:        PPO_1292/train/learning_rate 0.0003
wandb:                 PPO_1292/train/loss 89.72524
wandb: PPO_1292/train/policy_gradient_loss 0.00353
wandb:                  PPO_1292/train/std 0.43655
wandb:           PPO_1292/train/value_loss 150.632
wandb:                    global_mean_eval -480.87509
wandb:                         global_step 212992
wandb:                       mean_reward_0 -449.20001
wandb:                       mean_reward_1 -426.11885
wandb:                      mean_reward_10 -514.59573
wandb:                      mean_reward_11 -529.37154
wandb:                      mean_reward_12 -455.14948
wandb:                      mean_reward_13 -427.33525
wandb:                      mean_reward_14 -464.13729
wandb:                      mean_reward_15 -492.65876
wandb:                      mean_reward_16 -514.87872
wandb:                      mean_reward_17 -529.41546
wandb:                      mean_reward_18 -457.72114
wandb:                      mean_reward_19 -426.59896
wandb:                       mean_reward_2 -465.26489
wandb:                      mean_reward_20 -464.09054
wandb:                      mean_reward_21 -493.00421
wandb:                      mean_reward_22 -515.01137
wandb:                      mean_reward_23 -529.73933
wandb:                      mean_reward_24 -470.42924
wandb:                      mean_reward_25 -430.74864
wandb:                      mean_reward_26 -465.40074
wandb:                      mean_reward_27 -493.71491
wandb:                      mean_reward_28 -514.93551
wandb:                      mean_reward_29 -529.95218
wandb:                       mean_reward_3 -493.53603
wandb:                      mean_reward_30 -450.3453
wandb:                      mean_reward_31 -424.86366
wandb:                      mean_reward_32 -464.79356
wandb:                      mean_reward_33 -494.11024
wandb:                      mean_reward_34 -515.13467
wandb:                      mean_reward_35 -529.58228
wandb:                       mean_reward_4 -514.67643
wandb:                       mean_reward_5 -529.57313
wandb:                       mean_reward_6 -449.40232
wandb:                       mean_reward_7 -428.18713
wandb:                       mean_reward_8 -463.90891
wandb:                       mean_reward_9 -493.91699
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -903.27844
wandb:                        std_reward_0 66.6543
wandb:                        std_reward_1 17.66048
wandb:                       std_reward_10 2.7029
wandb:                       std_reward_11 1.74139
wandb:                       std_reward_12 76.76003
wandb:                       std_reward_13 17.12052
wandb:                       std_reward_14 5.66549
wandb:                       std_reward_15 1.01453
wandb:                       std_reward_16 3.25734
wandb:                       std_reward_17 1.82705
wandb:                       std_reward_18 79.73865
wandb:                       std_reward_19 19.52794
wandb:                        std_reward_2 7.91902
wandb:                       std_reward_20 5.52565
wandb:                       std_reward_21 3.23612
wandb:                       std_reward_22 4.19104
wandb:                       std_reward_23 2.86713
wandb:                       std_reward_24 80.58834
wandb:                       std_reward_25 23.55113
wandb:                       std_reward_26 8.53552
wandb:                       std_reward_27 5.15243
wandb:                       std_reward_28 3.63305
wandb:                       std_reward_29 2.85025
wandb:                        std_reward_3 5.59195
wandb:                       std_reward_30 74.42151
wandb:                       std_reward_31 17.89093
wandb:                       std_reward_32 6.46825
wandb:                       std_reward_33 6.35833
wandb:                       std_reward_34 4.18227
wandb:                       std_reward_35 2.62742
wandb:                        std_reward_4 3.19557
wandb:                        std_reward_5 1.81247
wandb:                        std_reward_6 63.75943
wandb:                        std_reward_7 18.39735
wandb:                        std_reward_8 4.64034
wandb:                        std_reward_9 5.4775
wandb:                            time/fps 1153.0
wandb:                     train/approx_kl 0.01145
wandb:                 train/clip_fraction 0.14658
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.87514
wandb:            train/explained_variance 0.94396
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.10234
wandb:          train/policy_gradient_loss -0.01207
wandb:                           train/std 0.85747
wandb:                    train/value_loss 31.23324
wandb: 
wandb: Synced splendid-gorge-35: https://wandb.ai/tidiane/meta_rl_context/runs/27vz0el0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_032708-27vz0el0/logs
wandb: 
wandb: Run history:
wandb:                PPO_1213/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1213/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1213/rollout/ep_rew_mean ▁▁▂▃▄▄▄▄▅▇▇█
wandb:                   PPO_1213/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1213/train/approx_kl ▄▇▃▅▁▂▇▆▆▃█
wandb:        PPO_1213/train/clip_fraction ▅▇▃▆▁▄▆▇▇▆█
wandb:           PPO_1213/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1213/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1213/train/explained_variance █▇▆▇▃▁▃▂▃▃▂
wandb:        PPO_1213/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1213/train/loss ▁▁█▃▄▅▅▄█▇█
wandb: PPO_1213/train/policy_gradient_loss ▃▅▆▅██▇▇▁▇▆
wandb:                  PPO_1213/train/std █▇▆▆▅▄▄▃▂▂▁
wandb:           PPO_1213/train/value_loss ▁▁▄▃▆▇▇▆█▆█
wandb:                PPO_1223/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1223/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1223/rollout/ep_rew_mean ▁▂▂▂▄▅▄▆▇▇▇█
wandb:                   PPO_1223/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1223/train/approx_kl ▁▃▂▄▅▃▆█▆█▇
wandb:        PPO_1223/train/clip_fraction ▂▂▁▃▆▂▃█▅▆▅
wandb:           PPO_1223/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1223/train/entropy_loss ▁▁▂▃▃▄▅▅▆▆█
wandb:   PPO_1223/train/explained_variance ▄▂▃▄▁▆▂▆█▅▇
wandb:        PPO_1223/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1223/train/loss ▄█▄█▄▁▄▄▅▆▁
wandb: PPO_1223/train/policy_gradient_loss ▁▃▄█▃▇▅▅▇█▇
wandb:                  PPO_1223/train/std █▇▇▆▆▅▄▄▃▂▁
wandb:           PPO_1223/train/value_loss █▇▇▅▃▅█▁▂▂▄
wandb:                PPO_1233/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1233/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1233/rollout/ep_rew_mean ▁▁▃▃▂▄▄▆▅▆▅█
wandb:                   PPO_1233/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1233/train/approx_kl ▅▅█▆▅▁▃▅▂▃▄
wandb:        PPO_1233/train/clip_fraction ▆█▆▅▁▃▂▅▁▂▃
wandb:           PPO_1233/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1233/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1233/train/explained_variance ▅▂▁▂▂▂▄▄▄█▅
wandb:        PPO_1233/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1233/train/loss ▃▁▂▁▃▆▇█▄▇▃
wandb: PPO_1233/train/policy_gradient_loss ▁▅▅▄▅▇▄█▇▇▇
wandb:                  PPO_1233/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1233/train/value_loss ▂▁▂▃▆▅▄▄▄▄█
wandb:                PPO_1243/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1243/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1243/rollout/ep_rew_mean ▄▄▆▇▆▇▁▅▇██▇
wandb:                   PPO_1243/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1243/train/approx_kl ▆▃▃▃▂█▁▃▄▇▃
wandb:        PPO_1243/train/clip_fraction █▄▅▇▁▆▆▁█▆▄
wandb:           PPO_1243/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1243/train/entropy_loss ▁▂▂▄▄▄▅▆▆▇█
wandb:   PPO_1243/train/explained_variance ▂▁▃▄▅▆▅▆▃█▅
wandb:        PPO_1243/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1243/train/loss ▃▆▃▁▅▃▅▃▂▃█
wandb: PPO_1243/train/policy_gradient_loss ▁▄▅▄▅▃▅▄██▆
wandb:                  PPO_1243/train/std █▇▆▅▅▄▃▃▃▂▁
wandb:           PPO_1243/train/value_loss ▁▃▃▁▂▂▃▃▃▂█
wandb:                PPO_1253/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1253/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1253/rollout/ep_rew_mean ▁▂▃▄▅█▇▇▇▆█▄
wandb:                   PPO_1253/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1253/train/approx_kl ▁▆▅▇▄▅▄█▂▆▆
wandb:        PPO_1253/train/clip_fraction ▁▅▂▆▂▅▅█▃▅▃
wandb:           PPO_1253/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1253/train/entropy_loss ▁▁▂▂▃▃▄▅▅▆█
wandb:   PPO_1253/train/explained_variance ▆▆▆▆▁▂▇▅▅▆█
wandb:        PPO_1253/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1253/train/loss ▅▄▁█▇▇▁█▅▂▆
wandb: PPO_1253/train/policy_gradient_loss ▇▆▄▇▁▇▅▇▅▆█
wandb:                  PPO_1253/train/std ██▇▇▆▆▅▄▄▃▁
wandb:           PPO_1253/train/value_loss ▅▁▆▁█▄▂▃▅▄▄
wandb:                PPO_1263/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1263/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1263/rollout/ep_rew_mean ▇▇▆▅▅▅▆█▄▁▄▃
wandb:                   PPO_1263/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1263/train/approx_kl ▇▅▅▅▇▆▅▄█▁▃
wandb:        PPO_1263/train/clip_fraction ▇▅▅▅▅▇▂█▂▁▁
wandb:           PPO_1263/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1263/train/entropy_loss ▁▂▄▅▅▅▆▆▇██
wandb:   PPO_1263/train/explained_variance ▆▇▅▇▅█▁▇▅▆▇
wandb:        PPO_1263/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1263/train/loss ▅▃▄▂▁▂▂▄█▃▃
wandb: PPO_1263/train/policy_gradient_loss ▂▄▂▆▃▄▁█▂▆▁
wandb:                  PPO_1263/train/std █▇▅▄▄▄▃▃▂▁▁
wandb:           PPO_1263/train/value_loss ▂▂▃▃▂▁▄▁▅▆█
wandb:                PPO_1273/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1273/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1273/rollout/ep_rew_mean █▄▅▇▂▃▄▁▆▇▂▆
wandb:                   PPO_1273/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1273/train/approx_kl ▅▅▄▅▃▇▁▄▂█▅
wandb:        PPO_1273/train/clip_fraction ▃▄▃▅▃█▁▃▂▆▄
wandb:           PPO_1273/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1273/train/entropy_loss ▁▂▁▁▃▄▄▆▇██
wandb:   PPO_1273/train/explained_variance ▃▅▁▅▆█▆▅▄▆▅
wandb:        PPO_1273/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1273/train/loss ▂▇▂▁▃▁█▃▃▅▄
wandb: PPO_1273/train/policy_gradient_loss ▁▂▁▅▁█▃▁▃█▅
wandb:                  PPO_1273/train/std █▇██▅▅▄▃▂▂▁
wandb:           PPO_1273/train/value_loss ▃▆█▃▅▁▆▆▇▃▆
wandb:                PPO_1283/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1283/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1283/rollout/ep_rew_mean █▇█▅▆▆▁▅██▃▄
wandb:                   PPO_1283/time/fps █▃▂▂▁▁▁▁▁▁▁▁
wandb:            PPO_1283/train/approx_kl ▂▆▆▄▄▃▃▄▁█▆
wandb:        PPO_1283/train/clip_fraction ▂▅▃▃▂▃▁▃▁█▂
wandb:           PPO_1283/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1283/train/entropy_loss ▁▁▂▂▃▄▅▅▆██
wandb:   PPO_1283/train/explained_variance ▁▇▄█▇█▇▇▃█▄
wandb:        PPO_1283/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1283/train/loss ▅▁▁▄▅▁█▅▃▂▄
wandb: PPO_1283/train/policy_gradient_loss ▅▅▆█▅▆▅▅▁█▅
wandb:                  PPO_1283/train/std █▇▇▆▅▅▄▃▂▁▁
wandb:           PPO_1283/train/value_loss ▄▂▇▅▅▅▇▅▇▁█
wandb:                PPO_1293/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1293/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1293/rollout/ep_rew_mean █▄▅▁▄▄▄▆▂▁▄▁
wandb:                   PPO_1293/time/fps █▃▂▁▁▁▁▁▁▁▁▁
wandb:            PPO_1293/train/approx_kl ▃▅▄▁▆█▃▇▂▃▆
wandb:        PPO_1293/train/clip_fraction ▃█▃▁▄▆▃▇▂▅█
wandb:           PPO_1293/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1293/train/entropy_loss ▁▂▂▃▄▄▅▆▆▆█
wandb:   PPO_1293/train/explained_variance ▁█▅▆▄▆▅▅▄▆▄
wandb:        PPO_1293/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1293/train/loss ▄▁▂▃▅▁▂▅▄█▂
wandb: PPO_1293/train/policy_gradient_loss ▁█▁▂▃▄▃▆▃▃▆
wandb:                  PPO_1293/train/std █▇▇▇▆▅▅▃▄▃▁
wandb:           PPO_1293/train/value_loss █▁▅▇▆▄▇▆█▄▃
wandb:                    global_mean_eval ▁▄▇█████▇▇
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▃▂▆█▅▅▂▁▁▃
wandb:                       mean_reward_1 ▁▃▅▆▇▇███▇
wandb:                      mean_reward_10 ▁▅▇▇█████▇
wandb:                      mean_reward_11 ▁▅▇█████▇▇
wandb:                      mean_reward_12 ▂▂▆█▅▅▂▁▁▃
wandb:                      mean_reward_13 ▁▃▅▆▇▇███▇
wandb:                      mean_reward_14 ▁▄▆▆▇▇████
wandb:                      mean_reward_15 ▁▅▆▇▇█████
wandb:                      mean_reward_16 ▁▅▇▇█████▇
wandb:                      mean_reward_17 ▁▅▇█████▇▇
wandb:                      mean_reward_18 ▃▂▆█▅▅▂▁▁▃
wandb:                      mean_reward_19 ▁▃▅▆▇▇███▇
wandb:                       mean_reward_2 ▁▄▆▆▇▇████
wandb:                      mean_reward_20 ▁▄▆▆▇▇████
wandb:                      mean_reward_21 ▁▅▆▇▇█████
wandb:                      mean_reward_22 ▁▅▇▇█████▇
wandb:                      mean_reward_23 ▁▅▇█████▇▇
wandb:                      mean_reward_24 ▂▂▆█▅▅▃▁▁▂
wandb:                      mean_reward_25 ▁▃▅▆▇▇███▇
wandb:                      mean_reward_26 ▁▄▆▆▇▇████
wandb:                      mean_reward_27 ▁▅▆▇▇█████
wandb:                      mean_reward_28 ▁▅▇▇█████▇
wandb:                      mean_reward_29 ▁▅▇█████▇▇
wandb:                       mean_reward_3 ▁▅▆▇▇█████
wandb:                      mean_reward_30 ▂▂▆█▅▅▂▁▁▃
wandb:                      mean_reward_31 ▁▃▅▆▇▇███▇
wandb:                      mean_reward_32 ▁▄▆▆▇▇████
wandb:                      mean_reward_33 ▁▅▇▇▇█████
wandb:                      mean_reward_34 ▁▅▇▇█████▇
wandb:                      mean_reward_35 ▁▅▇█████▇▇
wandb:                       mean_reward_4 ▁▅▇▇█████▇
wandb:                       mean_reward_5 ▁▅▇█████▇▇
wandb:                       mean_reward_6 ▂▂▆█▅▅▂▁▁▂
wandb:                       mean_reward_7 ▁▄▅▆▇▇███▇
wandb:                       mean_reward_8 ▁▄▆▆▇▇████
wandb:                       mean_reward_9 ▁▅▇▇▇█████
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▄▄▅▅▇▆█
wandb:                        std_reward_0 █▃▂▁▃▃▃▃▃▄
wandb:                        std_reward_1 █▂▁▁▁▂▁▁▁▂
wandb:                       std_reward_10 █▃▂▄▁▂▁▁▃▄
wandb:                       std_reward_11 █▃▂▇▃▂▁▁▄▃
wandb:                       std_reward_12 █▃▂▁▃▃▃▃▃▄
wandb:                       std_reward_13 █▂▁▁▁▂▁▁▁▂
wandb:                       std_reward_14 █▂▁▁▁▁▁▁▁▂
wandb:                       std_reward_15 █▂▃▂▁▂▁▁▂▃
wandb:                       std_reward_16 █▃▂▄▁▂▁▁▃▃
wandb:                       std_reward_17 ▇▃▂█▃▂▁▁▃▃
wandb:                       std_reward_18 █▃▂▁▃▃▃▃▃▄
wandb:                       std_reward_19 █▂▁▁▁▂▁▁▁▁
wandb:                        std_reward_2 █▂▁▁▁▁▁▁▁▂
wandb:                       std_reward_20 █▂▁▁▁▁▁▁▁▂
wandb:                       std_reward_21 █▃▃▂▁▂▁▁▂▃
wandb:                       std_reward_22 █▃▂▄▁▂▁▁▄▄
wandb:                       std_reward_23 ▆▃▂█▃▂▁▁▃▃
wandb:                       std_reward_24 █▃▁▁▃▃▃▃▃▄
wandb:                       std_reward_25 █▁▁▁▁▂▁▁▁▁
wandb:                       std_reward_26 █▂▁▁▁▁▁▁▁▂
wandb:                       std_reward_27 █▃▃▂▁▂▁▁▂▃
wandb:                       std_reward_28 █▃▂▄▁▂▁▁▃▄
wandb:                       std_reward_29 ▆▃▂█▃▂▁▁▃▃
wandb:                        std_reward_3 █▂▃▂▁▂▁▁▂▃
wandb:                       std_reward_30 █▃▂▁▃▃▃▃▃▄
wandb:                       std_reward_31 █▂▁▁▁▂▁▁▁▂
wandb:                       std_reward_32 █▂▁▁▁▁▁▁▁▂
wandb:                       std_reward_33 █▂▃▂▁▂▁▁▂▃
wandb:                       std_reward_34 █▃▂▄▁▂▁▁▃▃
wandb:                       std_reward_35 ▇▃▂█▃▂▁▁▃▄
wandb:                        std_reward_4 █▃▂▄▁▂▁▁▃▄
wandb:                        std_reward_5 ▆▃▂█▃▂▁▁▃▃
wandb:                        std_reward_6 █▃▁▁▃▃▄▃▃▅
wandb:                        std_reward_7 █▁▁▁▁▂▁▁▁▂
wandb:                        std_reward_8 █▂▁▁▁▁▁▁▁▂
wandb:                        std_reward_9 █▂▃▂▁▂▁▁▂▃
wandb:                            time/fps █▃▂▂▁▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▅█▁▂▄▅▅▆▇▆
wandb:                 train/clip_fraction ▂▃▄▃▄▁▃▅▆▇█▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▂▃▃▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▄▆▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▃▃▂▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▅▆█▆▄▃▂▂▁▂
wandb:                           train/std █▇▇▇▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1213/global_step 212992
wandb:        PPO_1213/rollout/ep_len_mean 200.0
wandb:        PPO_1213/rollout/ep_rew_mean -755.08856
wandb:                   PPO_1213/time/fps 1184.0
wandb:            PPO_1213/train/approx_kl 0.01333
wandb:        PPO_1213/train/clip_fraction 0.16381
wandb:           PPO_1213/train/clip_range 0.2
wandb:         PPO_1213/train/entropy_loss -7.66093
wandb:   PPO_1213/train/explained_variance 0.95629
wandb:        PPO_1213/train/learning_rate 0.0003
wandb:                 PPO_1213/train/loss 82.52277
wandb: PPO_1213/train/policy_gradient_loss -0.00855
wandb:                  PPO_1213/train/std 0.72296
wandb:           PPO_1213/train/value_loss 127.93329
wandb:                PPO_1223/global_step 212992
wandb:        PPO_1223/rollout/ep_len_mean 200.0
wandb:        PPO_1223/rollout/ep_rew_mean -618.29401
wandb:                   PPO_1223/time/fps 1161.0
wandb:            PPO_1223/train/approx_kl 0.01488
wandb:        PPO_1223/train/clip_fraction 0.19085
wandb:           PPO_1223/train/clip_range 0.2
wandb:         PPO_1223/train/entropy_loss -6.72739
wandb:   PPO_1223/train/explained_variance 0.96415
wandb:        PPO_1223/train/learning_rate 0.0003
wandb:                 PPO_1223/train/loss 14.71524
wandb: PPO_1223/train/policy_gradient_loss -0.0061
wandb:                  PPO_1223/train/std 0.63231
wandb:           PPO_1223/train/value_loss 58.13858
wandb:                PPO_1233/global_step 212992
wandb:        PPO_1233/rollout/ep_len_mean 200.0
wandb:        PPO_1233/rollout/ep_rew_mean -541.41119
wandb:                   PPO_1233/time/fps 1182.0
wandb:            PPO_1233/train/approx_kl 0.01454
wandb:        PPO_1233/train/clip_fraction 0.18992
wandb:           PPO_1233/train/clip_range 0.2
wandb:         PPO_1233/train/entropy_loss -5.90341
wandb:   PPO_1233/train/explained_variance 0.96298
wandb:        PPO_1233/train/learning_rate 0.0003
wandb:                 PPO_1233/train/loss 19.33183
wandb: PPO_1233/train/policy_gradient_loss -0.00339
wandb:                  PPO_1233/train/std 0.56273
wandb:           PPO_1233/train/value_loss 93.43872
wandb:                PPO_1243/global_step 212992
wandb:        PPO_1243/rollout/ep_len_mean 200.0
wandb:        PPO_1243/rollout/ep_rew_mean -543.84784
wandb:                   PPO_1243/time/fps 1178.0
wandb:            PPO_1243/train/approx_kl 0.01445
wandb:        PPO_1243/train/clip_fraction 0.19248
wandb:           PPO_1243/train/clip_range 0.2
wandb:         PPO_1243/train/entropy_loss -5.4425
wandb:   PPO_1243/train/explained_variance 0.97631
wandb:        PPO_1243/train/learning_rate 0.0003
wandb:                 PPO_1243/train/loss 108.76793
wandb: PPO_1243/train/policy_gradient_loss -0.00204
wandb:                  PPO_1243/train/std 0.52734
wandb:           PPO_1243/train/value_loss 223.34872
wandb:                PPO_1253/global_step 212992
wandb:        PPO_1253/rollout/ep_len_mean 200.0
wandb:        PPO_1253/rollout/ep_rew_mean -549.58661
wandb:                   PPO_1253/time/fps 1170.0
wandb:            PPO_1253/train/approx_kl 0.01571
wandb:        PPO_1253/train/clip_fraction 0.18912
wandb:           PPO_1253/train/clip_range 0.2
wandb:         PPO_1253/train/entropy_loss -5.0678
wandb:   PPO_1253/train/explained_variance 0.98451
wandb:        PPO_1253/train/learning_rate 0.0003
wandb:                 PPO_1253/train/loss 118.57575
wandb: PPO_1253/train/policy_gradient_loss -0.00066
wandb:                  PPO_1253/train/std 0.49881
wandb:           PPO_1253/train/value_loss 239.88011
wandb:                PPO_1263/global_step 212992
wandb:        PPO_1263/rollout/ep_len_mean 200.0
wandb:        PPO_1263/rollout/ep_rew_mean -565.42377
wandb:                   PPO_1263/time/fps 1175.0
wandb:            PPO_1263/train/approx_kl 0.01327
wandb:        PPO_1263/train/clip_fraction 0.17577
wandb:           PPO_1263/train/clip_range 0.2
wandb:         PPO_1263/train/entropy_loss -4.76892
wandb:   PPO_1263/train/explained_variance 0.98278
wandb:        PPO_1263/train/learning_rate 0.0003
wandb:                 PPO_1263/train/loss 103.50358
wandb: PPO_1263/train/policy_gradient_loss -0.00227
wandb:                  PPO_1263/train/std 0.47906
wandb:           PPO_1263/train/value_loss 595.14899
wandb:                PPO_1273/global_step 212992
wandb:        PPO_1273/rollout/ep_len_mean 200.0
wandb:        PPO_1273/rollout/ep_rew_mean -567.06134
wandb:                   PPO_1273/time/fps 1167.0
wandb:            PPO_1273/train/approx_kl 0.01549
wandb:        PPO_1273/train/clip_fraction 0.19781
wandb:           PPO_1273/train/clip_range 0.2
wandb:         PPO_1273/train/entropy_loss -4.54673
wandb:   PPO_1273/train/explained_variance 0.98243
wandb:        PPO_1273/train/learning_rate 0.0003
wandb:                 PPO_1273/train/loss 417.5011
wandb: PPO_1273/train/policy_gradient_loss 0.00054
wandb:                  PPO_1273/train/std 0.46314
wandb:           PPO_1273/train/value_loss 687.58997
wandb:                PPO_1283/global_step 212992
wandb:        PPO_1283/rollout/ep_len_mean 200.0
wandb:        PPO_1283/rollout/ep_rew_mean -637.14392
wandb:                   PPO_1283/time/fps 1164.0
wandb:            PPO_1283/train/approx_kl 0.01811
wandb:        PPO_1283/train/clip_fraction 0.18496
wandb:           PPO_1283/train/clip_range 0.2
wandb:         PPO_1283/train/entropy_loss -4.25394
wandb:   PPO_1283/train/explained_variance 0.98047
wandb:        PPO_1283/train/learning_rate 0.0003
wandb:                 PPO_1283/train/loss 471.44504
wandb: PPO_1283/train/policy_gradient_loss 0.00011
wandb:                  PPO_1283/train/std 0.44576
wandb:           PPO_1283/train/value_loss 1048.87366
wandb:                PPO_1293/global_step 212992
wandb:        PPO_1293/rollout/ep_len_mean 200.0
wandb:        PPO_1293/rollout/ep_rew_mean -651.1275
wandb:                   PPO_1293/time/fps 1145.0
wandb:            PPO_1293/train/approx_kl 0.01658
wandb:        PPO_1293/train/clip_fraction 0.24152
wandb:           PPO_1293/train/clip_range 0.2
wandb:         PPO_1293/train/entropy_loss -3.95685
wandb:   PPO_1293/train/explained_variance 0.98473
wandb:        PPO_1293/train/learning_rate 0.0003
wandb:                 PPO_1293/train/loss 206.33018
wandb: PPO_1293/train/policy_gradient_loss 0.00228
wandb:                  PPO_1293/train/std 0.42565
wandb:           PPO_1293/train/value_loss 592.1546
wandb:                    global_mean_eval -474.33106
wandb:                         global_step 212992
wandb:                       mean_reward_0 -607.80537
wandb:                       mean_reward_1 -305.12075
wandb:                      mean_reward_10 -516.64965
wandb:                      mean_reward_11 -553.45833
wandb:                      mean_reward_12 -609.0098
wandb:                      mean_reward_13 -305.55395
wandb:                      mean_reward_14 -393.93857
wandb:                      mean_reward_15 -465.34707
wandb:                      mean_reward_16 -516.31265
wandb:                      mean_reward_17 -553.49756
wandb:                      mean_reward_18 -611.55122
wandb:                      mean_reward_19 -303.25254
wandb:                       mean_reward_2 -395.2959
wandb:                      mean_reward_20 -395.29757
wandb:                      mean_reward_21 -465.17639
wandb:                      mean_reward_22 -515.66347
wandb:                      mean_reward_23 -551.97747
wandb:                      mean_reward_24 -616.48071
wandb:                      mean_reward_25 -304.27824
wandb:                      mean_reward_26 -395.70451
wandb:                      mean_reward_27 -464.40173
wandb:                      mean_reward_28 -515.97984
wandb:                      mean_reward_29 -553.17984
wandb:                       mean_reward_3 -465.40247
wandb:                      mean_reward_30 -603.50215
wandb:                      mean_reward_31 -305.75657
wandb:                      mean_reward_32 -394.9002
wandb:                      mean_reward_33 -466.17285
wandb:                      mean_reward_34 -515.39654
wandb:                      mean_reward_35 -552.8584
wandb:                       mean_reward_4 -515.54955
wandb:                       mean_reward_5 -552.84945
wandb:                       mean_reward_6 -621.59568
wandb:                       mean_reward_7 -306.02527
wandb:                       mean_reward_8 -395.94245
wandb:                       mean_reward_9 -465.03336
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -903.27844
wandb:                        std_reward_0 65.73527
wandb:                        std_reward_1 9.28275
wandb:                       std_reward_10 7.49656
wandb:                       std_reward_11 5.33268
wandb:                       std_reward_12 68.23034
wandb:                       std_reward_13 12.51639
wandb:                       std_reward_14 5.26448
wandb:                       std_reward_15 6.04736
wandb:                       std_reward_16 5.38444
wandb:                       std_reward_17 6.05493
wandb:                       std_reward_18 59.63169
wandb:                       std_reward_19 4.65522
wandb:                        std_reward_2 6.6487
wandb:                       std_reward_20 7.05126
wandb:                       std_reward_21 7.58883
wandb:                       std_reward_22 7.31539
wandb:                       std_reward_23 5.80287
wandb:                       std_reward_24 70.8095
wandb:                       std_reward_25 6.92872
wandb:                       std_reward_26 8.63082
wandb:                       std_reward_27 6.02778
wandb:                       std_reward_28 7.46214
wandb:                       std_reward_29 5.31364
wandb:                        std_reward_3 6.82454
wandb:                       std_reward_30 60.34328
wandb:                       std_reward_31 10.42484
wandb:                       std_reward_32 6.56076
wandb:                       std_reward_33 7.72587
wandb:                       std_reward_34 5.86097
wandb:                       std_reward_35 7.02137
wandb:                        std_reward_4 6.96175
wandb:                        std_reward_5 6.0329
wandb:                        std_reward_6 72.11563
wandb:                        std_reward_7 9.17702
wandb:                        std_reward_8 8.3314
wandb:                        std_reward_9 7.04281
wandb:                            time/fps 1153.0
wandb:                     train/approx_kl 0.01145
wandb:                 train/clip_fraction 0.14658
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.87514
wandb:            train/explained_variance 0.94396
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.10234
wandb:          train/policy_gradient_loss -0.01207
wandb:                           train/std 0.85747
wandb:                    train/value_loss 31.23324
wandb: 
wandb: Synced vague-night-38: https://wandb.ai/tidiane/meta_rl_context/runs/25ymiicf
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)
wandb: Find logs at: ./wandb/run-20230626_032708-25ymiicf/logs

real	119m42.790s
user	1174m6.756s
sys	3m22.342s
+ mpirun python dev/automl/meta_rl/scripts/orig_impl/striker_baselines.py --context latent
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: tidiane. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_052650-2ow35t7w
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_052650-c48c201m
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_052650-tyuqum18
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-terrain-50
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/2ow35t7w
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_052650-1mmglo7l
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_052650-1mm4memk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-capybara-44
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/c48c201m
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_052650-2uae6ict
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_052650-2if2cb72
wandb: Run `wandb offline` to turn off syncing.
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_052650-2gfbf78j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-yogurt-48
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/tyuqum18
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_052650-1gp8uu9i
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/fr/fr_fr/fr_tn110/wandb/run-20230626_052650-2y87u8r8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-mountain-51
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/1mmglo7l
wandb: Syncing run twilight-dawn-43
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/1mm4memk
wandb: Syncing run glowing-glitter-45
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/2if2cb72
wandb: Syncing run zany-fire-50
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/2uae6ict
wandb: Syncing run logical-forest-46
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/2gfbf78j
wandb: Syncing run lyric-morning-48
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/1gp8uu9i
wandb: Syncing run deft-thunder-45
wandb: ⭐️ View project at https://wandb.ai/tidiane/meta_rl_context
wandb: 🚀 View run at https://wandb.ai/tidiane/meta_rl_context/runs/2y87u8r8
/home/fr/fr_fr/fr_tn110/dev/automl/meta_rl/meta_rl/envs/striker_predictor.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)
  "s_context": torch.unsqueeze(torch.Tensor(s_[:-1]),0),
/home/fr/fr_fr/fr_tn110/dev/automl/meta_rl/meta_rl/envs/striker_predictor.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)
  "s_context": torch.unsqueeze(torch.Tensor(s_[:-1]),0),
/home/fr/fr_fr/fr_tn110/dev/automl/meta_rl/meta_rl/envs/striker_predictor.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)
  "s_context": torch.unsqueeze(torch.Tensor(s_[:-1]),0),
/home/fr/fr_fr/fr_tn110/dev/automl/meta_rl/meta_rl/envs/striker_predictor.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)
  "s_context": torch.unsqueeze(torch.Tensor(s_[:-1]),0),
/home/fr/fr_fr/fr_tn110/dev/automl/meta_rl/meta_rl/envs/striker_predictor.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)
  "s_context": torch.unsqueeze(torch.Tensor(s_[:-1]),0),
/home/fr/fr_fr/fr_tn110/dev/automl/meta_rl/meta_rl/envs/striker_predictor.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)
  "s_context": torch.unsqueeze(torch.Tensor(s_[:-1]),0),
/home/fr/fr_fr/fr_tn110/dev/automl/meta_rl/meta_rl/envs/striker_predictor.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)
  "s_context": torch.unsqueeze(torch.Tensor(s_[:-1]),0),
/home/fr/fr_fr/fr_tn110/dev/automl/meta_rl/meta_rl/envs/striker_predictor.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)
  "s_context": torch.unsqueeze(torch.Tensor(s_[:-1]),0),
/home/fr/fr_fr/fr_tn110/dev/automl/meta_rl/meta_rl/envs/striker_predictor.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)
  "s_context": torch.unsqueeze(torch.Tensor(s_[:-1]),0),
/home/fr/fr_fr/fr_tn110/dev/automl/meta_rl/meta_rl/envs/striker_predictor.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)
  "s_context": torch.unsqueeze(torch.Tensor(s_[:-1]),0),
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home/fr/fr_fr/fr_tn110/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1297/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1297/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1297/rollout/ep_rew_mean ▁▁▃▂▄▄▅▆▆▇▇█
wandb:                   PPO_1297/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1297/train/approx_kl ▄▃▁▃█▄▃▅▅▁▆
wandb:        PPO_1297/train/clip_fraction ▄▅▁▅▆▅▁█▆▄█
wandb:           PPO_1297/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1297/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1297/train/explained_variance ▁▆▇▇▅▇█▆▆▆▇
wandb:        PPO_1297/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1297/train/loss ▁▆▂▃▂▅▆▁▆█▇
wandb: PPO_1297/train/policy_gradient_loss ▃▁▅▂▄▆█▅▄▇▄
wandb:                  PPO_1297/train/std █▇▆▅▅▄▄▃▃▂▁
wandb:           PPO_1297/train/value_loss ▁▁▃▃▆▆▇▆██▆
wandb:                PPO_1307/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1307/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1307/rollout/ep_rew_mean ▁▃▁▁▃▃▆▇▇▆▇█
wandb:                   PPO_1307/time/fps █▄▃▂▂▂▂▁▁▁▁▁
wandb:            PPO_1307/train/approx_kl ▂▂▂▆▃▁▆▄█▅▄
wandb:        PPO_1307/train/clip_fraction ▂▁▁▁▂▁▅▃█▃▆
wandb:           PPO_1307/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1307/train/entropy_loss ▁▂▂▃▄▅▅▆▆▇█
wandb:   PPO_1307/train/explained_variance ▇▂█▁██▃▁▆█▆
wandb:        PPO_1307/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1307/train/loss █▃▁▅▄▆▄▅▆▄▂
wandb: PPO_1307/train/policy_gradient_loss ▁▅▅▆▃▅▆█▄▄█
wandb:                  PPO_1307/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1307/train/value_loss █▇▅▅▄▆▁▄▁▇▆
wandb:                PPO_1317/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1317/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1317/rollout/ep_rew_mean ▁▁▂▄▃▄▄▅▄▆██
wandb:                   PPO_1317/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1317/train/approx_kl ▄▅▁▇▄██▄▇▇▃
wandb:        PPO_1317/train/clip_fraction ▁▂▁▂▆▄█▄▆█▇
wandb:           PPO_1317/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1317/train/entropy_loss ▁▁▂▂▃▄▄▅▅▆█
wandb:   PPO_1317/train/explained_variance ▃▅▁▃▂▇█▃█▅▃
wandb:        PPO_1317/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1317/train/loss ▄▇▅▃▄█▃▃▁▃▃
wandb: PPO_1317/train/policy_gradient_loss ▂█▄▅▃▅▁▆▄█▅
wandb:                  PPO_1317/train/std ██▇▇▆▅▅▄▃▂▁
wandb:           PPO_1317/train/value_loss ▇▆█▇▂▅▂▁▄▁▆
wandb:                PPO_1327/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1327/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1327/rollout/ep_rew_mean ▁▃▅█▆▅▅▆▅▅▃▂
wandb:                   PPO_1327/time/fps █▃▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1327/train/approx_kl ▄█▆▁▅▆▆▃▄▃▄
wandb:        PPO_1327/train/clip_fraction ▅█▅▄▄▆▄▁▃▂▃
wandb:           PPO_1327/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1327/train/entropy_loss ▁▂▄▅▅▆▆▆▇██
wandb:   PPO_1327/train/explained_variance ██▆▇▁▁▅▂▃▅█
wandb:        PPO_1327/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1327/train/loss ▂▆▄▄▃▇▂▂▁▆█
wandb: PPO_1327/train/policy_gradient_loss ▇▁▃▃▄▄▄▂▇▆█
wandb:                  PPO_1327/train/std █▆▅▄▄▃▃▂▂▁▁
wandb:           PPO_1327/train/value_loss ▁▁▂▂▃▃▃▆▆█▇
wandb:                PPO_1337/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1337/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1337/rollout/ep_rew_mean ▁▃▄▅▇▇█▇█▇▆▇
wandb:                   PPO_1337/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1337/train/approx_kl ▄▃▅▂▅▇█▂▃▁█
wandb:        PPO_1337/train/clip_fraction ▁▄█▅▃▃▆▇▄▁█
wandb:           PPO_1337/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1337/train/entropy_loss ▁▂▃▃▄▄▄▆▇▇█
wandb:   PPO_1337/train/explained_variance ▁▃▅▅▃▇▆▇▇█▆
wandb:        PPO_1337/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1337/train/loss ▇▂▁▃▃▄▄▄█▂▇
wandb: PPO_1337/train/policy_gradient_loss ▁▃▁▂▃▄▄▄▅▅█
wandb:                  PPO_1337/train/std █▇▇▆▅▅▄▃▂▂▁
wandb:           PPO_1337/train/value_loss ▅▃▁▁▆▅▂▂▄▇█
wandb:                PPO_1347/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1347/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1347/rollout/ep_rew_mean ▃▂▄▁▃▄█▆█▇█▁
wandb:                   PPO_1347/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1347/train/approx_kl ▁▂█▃▃▃▇▆▂█▁
wandb:        PPO_1347/train/clip_fraction ▃▁▆▃▆▃█▁▆▇▄
wandb:           PPO_1347/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1347/train/entropy_loss ▁▂▃▃▄▄▄▅▇▇█
wandb:   PPO_1347/train/explained_variance ▅▄▄▅▅█▃▁▆▇▄
wandb:        PPO_1347/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1347/train/loss █▂▁▁▅▂▁▂▂▁▂
wandb: PPO_1347/train/policy_gradient_loss ▅▅▇▄▄▇█▁▃▅█
wandb:                  PPO_1347/train/std █▇▇▆▅▅▅▄▂▂▁
wandb:           PPO_1347/train/value_loss ▅█▄█▆▆▁▅▁▂▂
wandb:                PPO_1357/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1357/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1357/rollout/ep_rew_mean ▅▇▅█▁▂▆▇▅▅▆▃
wandb:                   PPO_1357/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1357/train/approx_kl ▁▃▂▃▁▁▅▃▂▁█
wandb:        PPO_1357/train/clip_fraction ▅▇▆█▁▇█▆█▆▇
wandb:           PPO_1357/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1357/train/entropy_loss ▁▂▃▄▄▅▅▆▇▇█
wandb:   PPO_1357/train/explained_variance ▂▅▆▃▁▅▇▄▆▅█
wandb:        PPO_1357/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1357/train/loss ▁▂▂▁▂▂▅█▂▄▅
wandb: PPO_1357/train/policy_gradient_loss ▄▅▃▃▁▆█▆▇▅▅
wandb:                  PPO_1357/train/std █▇▆▅▄▄▄▂▂▂▁
wandb:           PPO_1357/train/value_loss ▄▂▁▁█▇▃▅▅▄▅
wandb:                PPO_1367/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1367/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1367/rollout/ep_rew_mean ▂▆▃▁▃▃▂▃▂▆█▃
wandb:                   PPO_1367/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1367/train/approx_kl ▂▂▁▃▃▂▄▄▃▃█
wandb:        PPO_1367/train/clip_fraction ▂▆▇█▅▂█▁▆▄▅
wandb:           PPO_1367/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1367/train/entropy_loss ▁▁▂▃▄▆▇██▆█
wandb:   PPO_1367/train/explained_variance ▅▁██▆▇▇▇█▆▇
wandb:        PPO_1367/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1367/train/loss ▃▃▁█▄▁▂▆█▃▆
wandb: PPO_1367/train/policy_gradient_loss ▃▆▃▅▆▁█▂▆▄▇
wandb:                  PPO_1367/train/std ██▆▆▄▂▂▁▃▂▁
wandb:           PPO_1367/train/value_loss ▇█▇▃▃▆▆▆▁▂▄
wandb:                PPO_1377/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1377/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1377/rollout/ep_rew_mean █▆▇▁▇▆▇▇▂▇▆▇
wandb:                   PPO_1377/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1377/train/approx_kl ▁▄█▁▅▂▃▁▆▄▆
wandb:        PPO_1377/train/clip_fraction ▁▄▇▁█▂▇▇▇▅▆
wandb:           PPO_1377/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1377/train/entropy_loss ▁▁▁▃▃▄▅▆▅▅█
wandb:   PPO_1377/train/explained_variance ▇█▇▇▁▇▇▇█▆▄
wandb:        PPO_1377/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1377/train/loss ▄▃▁█▁▂▁▂▁▃▂
wandb: PPO_1377/train/policy_gradient_loss ▁▄█▃▆▃▄▇▇▆▅
wandb:                  PPO_1377/train/std ██▇▆▅▄▄▃▄▃▁
wandb:           PPO_1377/train/value_loss ▅▂▇▇▆█▁▆▁▆▅
wandb:                    global_mean_eval ▁▂▃▇▇██▇▆▇
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▂▄▇▇██▇█▆
wandb:                       mean_reward_1 ▁▂▄▇███▇▆▇
wandb:                      mean_reward_10 ▁▂▄▇▇██▇▆▇
wandb:                      mean_reward_11 ▁▂▄▇████▇▇
wandb:                      mean_reward_12 ▁▂▃▇▇▆█▇▆▆
wandb:                      mean_reward_13 ▁▂▃▇▇▆█▇▇▅
wandb:                      mean_reward_14 ▁▂▄▇█▇█▇▆▇
wandb:                      mean_reward_15 ▁▂▃▇▇▇██▇▇
wandb:                      mean_reward_16 ▁▂▄▆▇█▇▇▆▆
wandb:                      mean_reward_17 ▁▂▃▆▇██▇▇▆
wandb:                      mean_reward_18 ▁▂▃▇▇▇█▇▆▇
wandb:                      mean_reward_19 ▁▂▃▇▇██▇▆▇
wandb:                       mean_reward_2 ▁▂▄█▇▇██▅▇
wandb:                      mean_reward_20 ▁▂▃▆▇███▆▇
wandb:                      mean_reward_21 ▁▂▃▇▇███▆▇
wandb:                      mean_reward_22 ▁▂▄██▇█▇▇█
wandb:                      mean_reward_23 ▁▁▃▆████▆▆
wandb:                      mean_reward_24 ▁▂▃▇█▇█▇▇▆
wandb:                      mean_reward_25 ▁▂▃▇▅██▇▆▇
wandb:                      mean_reward_26 ▁▂▃▇▇███▅▆
wandb:                      mean_reward_27 ▁▂▄▇███▇▆█
wandb:                      mean_reward_28 ▁▂▄▇▇███▇▆
wandb:                      mean_reward_29 ▁▂▃▆███▇▇▆
wandb:                       mean_reward_3 ▁▂▄▇█▇████
wandb:                      mean_reward_30 ▁▂▃▇▇▇█▇█▇
wandb:                      mean_reward_31 ▁▂▄▇▇█▇▇▇▇
wandb:                      mean_reward_32 ▁▂▃▆▇▇██▅▇
wandb:                      mean_reward_33 ▁▂▃▇▇██▇▆▇
wandb:                      mean_reward_34 ▁▂▃▆▆▇█▆▆▅
wandb:                      mean_reward_35 ▁▂▄▇▇██▇▅▆
wandb:                       mean_reward_4 ▁▂▄▇▆▇█▇▆▆
wandb:                       mean_reward_5 ▁▂▃▇▇███▆▆
wandb:                       mean_reward_6 ▁▂▄▇▇▇██▆▆
wandb:                       mean_reward_7 ▁▂▃▇▇▇█▇▆▇
wandb:                       mean_reward_8 ▁▂▃▇▇▇█▇▇▆
wandb:                       mean_reward_9 ▁▂▃▇▆██▇▆▇
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▂▄▄▆▇▇█
wandb:                        std_reward_0 ▂▁▁▂▆▆▄▇▇█
wandb:                        std_reward_1 ▂▁▁▂▆▆▅▇█▇
wandb:                       std_reward_10 ▂▁▁▂▆▆▅▇█▇
wandb:                       std_reward_11 ▂▂▁▃▆▆▆███
wandb:                       std_reward_12 ▂▂▁▂▇█▅▇▇█
wandb:                       std_reward_13 ▂▁▁▂▆█▅▆▇█
wandb:                       std_reward_14 ▂▁▁▂▆█▅▇█▇
wandb:                       std_reward_15 ▂▂▁▃▇█▆▇█▇
wandb:                       std_reward_16 ▂▂▁▂▆▅▅▇█▇
wandb:                       std_reward_17 ▂▁▁▂▆▅▄▆▆█
wandb:                       std_reward_18 ▂▁▁▂█▇▅▇█▇
wandb:                       std_reward_19 ▂▁▁▂▅▅▅▇█▇
wandb:                        std_reward_2 ▂▁▁▂▆▆▅▆█▇
wandb:                       std_reward_20 ▂▁▁▂▅▅▄▆█▇
wandb:                       std_reward_21 ▂▁▁▂▇▆▅▆█▇
wandb:                       std_reward_22 ▂▁▁▂▆█▆██▇
wandb:                       std_reward_23 ▂▁▁▃▆▆▅▆██
wandb:                       std_reward_24 ▂▁▁▂▆▇▅▆██
wandb:                       std_reward_25 ▂▁▁▂█▆▄▇▇▇
wandb:                       std_reward_26 ▂▁▁▂▅▅▄▅█▆
wandb:                       std_reward_27 ▂▁▁▂▆▅▄▆█▆
wandb:                       std_reward_28 ▂▂▁▂▇▆▄▅█▇
wandb:                       std_reward_29 ▂▂▁▂▅▄▃▇▇█
wandb:                        std_reward_3 ▂▂▁▃▇█▆▇██
wandb:                       std_reward_30 ▂▂▁▃██▆█▇█
wandb:                       std_reward_31 ▂▁▁▂▇▇▆███
wandb:                       std_reward_32 ▂▁▁▂▆▆▄▅█▆
wandb:                       std_reward_33 ▂▂▁▃▇▆▆██▇
wandb:                       std_reward_34 ▂▁▁▂▇▆▃▇██
wandb:                       std_reward_35 ▂▂▁▂▆▆▄▆█▇
wandb:                        std_reward_4 ▂▁▁▃▇▇▅▇█▇
wandb:                        std_reward_5 ▂▁▁▂▆▅▅▇█▇
wandb:                        std_reward_6 ▂▂▁▂▆▇▅▆█▇
wandb:                        std_reward_7 ▂▁▁▂▆▆▄▅█▆
wandb:                        std_reward_8 ▂▂▁▂▇▆▅▆██
wandb:                        std_reward_9 ▂▂▁▂▇▅▅▇█▆
wandb:                            time/fps █▄▃▂▂▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▁▂▃█▆▂▁▃▃▅▄▄
wandb:                 train/clip_fraction ▄▄▄▅▅▂▁▄▇███
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▁▂▆▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▄▂▂▂▂▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▅▅▅██▆▄▂▁▁▂
wandb:                           train/std █▇▇▆▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1297/global_step 212992
wandb:        PPO_1297/rollout/ep_len_mean 200.0
wandb:        PPO_1297/rollout/ep_rew_mean -810.90198
wandb:                   PPO_1297/time/fps 602.0
wandb:            PPO_1297/train/approx_kl 0.01289
wandb:        PPO_1297/train/clip_fraction 0.17168
wandb:           PPO_1297/train/clip_range 0.2
wandb:         PPO_1297/train/entropy_loss -7.5924
wandb:   PPO_1297/train/explained_variance 0.95989
wandb:        PPO_1297/train/learning_rate 0.0003
wandb:                 PPO_1297/train/loss 42.32363
wandb: PPO_1297/train/policy_gradient_loss -0.00859
wandb:                  PPO_1297/train/std 0.71551
wandb:           PPO_1297/train/value_loss 67.43292
wandb:                PPO_1307/global_step 212992
wandb:        PPO_1307/rollout/ep_len_mean 200.0
wandb:        PPO_1307/rollout/ep_rew_mean -744.23724
wandb:                   PPO_1307/time/fps 609.0
wandb:            PPO_1307/train/approx_kl 0.01373
wandb:        PPO_1307/train/clip_fraction 0.1912
wandb:           PPO_1307/train/clip_range 0.2
wandb:         PPO_1307/train/entropy_loss -6.58458
wandb:   PPO_1307/train/explained_variance 0.95697
wandb:        PPO_1307/train/learning_rate 0.0003
wandb:                 PPO_1307/train/loss 17.00875
wandb: PPO_1307/train/policy_gradient_loss -0.00538
wandb:                  PPO_1307/train/std 0.61877
wandb:           PPO_1307/train/value_loss 58.2617
wandb:                PPO_1317/global_step 212992
wandb:        PPO_1317/rollout/ep_len_mean 200.0
wandb:        PPO_1317/rollout/ep_rew_mean -647.86694
wandb:                   PPO_1317/time/fps 601.0
wandb:            PPO_1317/train/approx_kl 0.01532
wandb:        PPO_1317/train/clip_fraction 0.2314
wandb:           PPO_1317/train/clip_range 0.2
wandb:         PPO_1317/train/entropy_loss -5.7819
wandb:   PPO_1317/train/explained_variance 0.95407
wandb:        PPO_1317/train/learning_rate 0.0003
wandb:                 PPO_1317/train/loss 14.01113
wandb: PPO_1317/train/policy_gradient_loss -0.00453
wandb:                  PPO_1317/train/std 0.55236
wandb:           PPO_1317/train/value_loss 50.03746
wandb:                PPO_1327/global_step 212992
wandb:        PPO_1327/rollout/ep_len_mean 200.0
wandb:        PPO_1327/rollout/ep_rew_mean -626.64215
wandb:                   PPO_1327/time/fps 606.0
wandb:            PPO_1327/train/approx_kl 0.01601
wandb:        PPO_1327/train/clip_fraction 0.20165
wandb:           PPO_1327/train/clip_range 0.2
wandb:         PPO_1327/train/entropy_loss -5.25219
wandb:   PPO_1327/train/explained_variance 0.96418
wandb:        PPO_1327/train/learning_rate 0.0003
wandb:                 PPO_1327/train/loss 58.92407
wandb: PPO_1327/train/policy_gradient_loss -0.00204
wandb:                  PPO_1327/train/std 0.51309
wandb:           PPO_1327/train/value_loss 128.97078
wandb:                PPO_1337/global_step 212992
wandb:        PPO_1337/rollout/ep_len_mean 200.0
wandb:        PPO_1337/rollout/ep_rew_mean -562.80688
wandb:                   PPO_1337/time/fps 605.0
wandb:            PPO_1337/train/approx_kl 0.01837
wandb:        PPO_1337/train/clip_fraction 0.22462
wandb:           PPO_1337/train/clip_range 0.2
wandb:         PPO_1337/train/entropy_loss -4.78217
wandb:   PPO_1337/train/explained_variance 0.97617
wandb:        PPO_1337/train/learning_rate 0.0003
wandb:                 PPO_1337/train/loss 117.47269
wandb: PPO_1337/train/policy_gradient_loss 0.0017
wandb:                  PPO_1337/train/std 0.47883
wandb:           PPO_1337/train/value_loss 237.15161
wandb:                PPO_1347/global_step 212992
wandb:        PPO_1347/rollout/ep_len_mean 200.0
wandb:        PPO_1347/rollout/ep_rew_mean -585.86896
wandb:                   PPO_1347/time/fps 604.0
wandb:            PPO_1347/train/approx_kl 0.01422
wandb:        PPO_1347/train/clip_fraction 0.202
wandb:           PPO_1347/train/clip_range 0.2
wandb:         PPO_1347/train/entropy_loss -4.44853
wandb:   PPO_1347/train/explained_variance 0.97552
wandb:        PPO_1347/train/learning_rate 0.0003
wandb:                 PPO_1347/train/loss 91.013
wandb: PPO_1347/train/policy_gradient_loss 0.0007
wandb:                  PPO_1347/train/std 0.45682
wandb:           PPO_1347/train/value_loss 231.11421
wandb:                PPO_1357/global_step 212992
wandb:        PPO_1357/rollout/ep_len_mean 200.0
wandb:        PPO_1357/rollout/ep_rew_mean -578.43091
wandb:                   PPO_1357/time/fps 603.0
wandb:            PPO_1357/train/approx_kl 0.0259
wandb:        PPO_1357/train/clip_fraction 0.22141
wandb:           PPO_1357/train/clip_range 0.2
wandb:         PPO_1357/train/entropy_loss -4.12698
wandb:   PPO_1357/train/explained_variance 0.98516
wandb:        PPO_1357/train/learning_rate 0.0003
wandb:                 PPO_1357/train/loss 380.4783
wandb: PPO_1357/train/policy_gradient_loss 0.00115
wandb:                  PPO_1357/train/std 0.43665
wandb:           PPO_1357/train/value_loss 376.22714
wandb:                PPO_1367/global_step 212992
wandb:        PPO_1367/rollout/ep_len_mean 200.0
wandb:        PPO_1367/rollout/ep_rew_mean -545.966
wandb:                   PPO_1367/time/fps 598.0
wandb:            PPO_1367/train/approx_kl 0.02505
wandb:        PPO_1367/train/clip_fraction 0.22617
wandb:           PPO_1367/train/clip_range 0.2
wandb:         PPO_1367/train/entropy_loss -3.92382
wandb:   PPO_1367/train/explained_variance 0.98334
wandb:        PPO_1367/train/learning_rate 0.0003
wandb:                 PPO_1367/train/loss 320.72751
wandb: PPO_1367/train/policy_gradient_loss 0.00251
wandb:                  PPO_1367/train/std 0.4247
wandb:           PPO_1367/train/value_loss 399.29773
wandb:                PPO_1377/global_step 212992
wandb:        PPO_1377/rollout/ep_len_mean 200.0
wandb:        PPO_1377/rollout/ep_rew_mean -561.92303
wandb:                   PPO_1377/time/fps 596.0
wandb:            PPO_1377/train/approx_kl 0.0203
wandb:        PPO_1377/train/clip_fraction 0.22671
wandb:           PPO_1377/train/clip_range 0.2
wandb:         PPO_1377/train/entropy_loss -3.77489
wandb:   PPO_1377/train/explained_variance 0.97868
wandb:        PPO_1377/train/learning_rate 0.0003
wandb:                 PPO_1377/train/loss 150.59523
wandb: PPO_1377/train/policy_gradient_loss 0.00207
wandb:                  PPO_1377/train/std 0.41519
wandb:           PPO_1377/train/value_loss 480.00906
wandb:                    global_mean_eval -534.09263
wandb:                         global_step 212992
wandb:                       mean_reward_0 -584.23988
wandb:                       mean_reward_1 -532.71359
wandb:                      mean_reward_10 -520.49007
wandb:                      mean_reward_11 -508.68852
wandb:                      mean_reward_12 -562.65065
wandb:                      mean_reward_13 -576.97154
wandb:                      mean_reward_14 -529.28561
wandb:                      mean_reward_15 -492.68724
wandb:                      mean_reward_16 -556.56203
wandb:                      mean_reward_17 -573.42627
wandb:                      mean_reward_18 -515.27653
wandb:                      mean_reward_19 -524.13483
wandb:                       mean_reward_2 -548.48444
wandb:                      mean_reward_20 -505.70783
wandb:                      mean_reward_21 -536.38052
wandb:                      mean_reward_22 -497.15831
wandb:                      mean_reward_23 -562.98405
wandb:                      mean_reward_24 -532.89307
wandb:                      mean_reward_25 -518.94803
wandb:                      mean_reward_26 -539.41671
wandb:                      mean_reward_27 -478.10749
wandb:                      mean_reward_28 -558.46144
wandb:                      mean_reward_29 -529.20282
wandb:                       mean_reward_3 -501.4809
wandb:                      mean_reward_30 -529.72869
wandb:                      mean_reward_31 -528.05006
wandb:                      mean_reward_32 -511.05523
wandb:                      mean_reward_33 -526.23947
wandb:                      mean_reward_34 -575.5968
wandb:                      mean_reward_35 -543.22037
wandb:                       mean_reward_4 -543.23563
wandb:                       mean_reward_5 -557.0231
wandb:                       mean_reward_6 -556.44236
wandb:                       mean_reward_7 -512.95587
wandb:                       mean_reward_8 -542.70323
wandb:                       mean_reward_9 -514.73165
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -901.89819
wandb:                        std_reward_0 312.79188
wandb:                        std_reward_1 290.77388
wandb:                       std_reward_10 268.3957
wandb:                       std_reward_11 248.58264
wandb:                       std_reward_12 305.52218
wandb:                       std_reward_13 304.96572
wandb:                       std_reward_14 274.73434
wandb:                       std_reward_15 229.40711
wandb:                       std_reward_16 290.70066
wandb:                       std_reward_17 313.30766
wandb:                       std_reward_18 247.60949
wandb:                       std_reward_19 263.70962
wandb:                        std_reward_2 288.10846
wandb:                       std_reward_20 258.79603
wandb:                       std_reward_21 283.62312
wandb:                       std_reward_22 249.52031
wandb:                       std_reward_23 305.10327
wandb:                       std_reward_24 273.44445
wandb:                       std_reward_25 268.9585
wandb:                       std_reward_26 279.38179
wandb:                       std_reward_27 237.27572
wandb:                       std_reward_28 283.95898
wandb:                       std_reward_29 287.40303
wandb:                        std_reward_3 262.94468
wandb:                       std_reward_30 263.14467
wandb:                       std_reward_31 282.45148
wandb:                       std_reward_32 251.09108
wandb:                       std_reward_33 265.105
wandb:                       std_reward_34 305.91825
wandb:                       std_reward_35 281.6857
wandb:                        std_reward_4 271.5696
wandb:                        std_reward_5 292.9678
wandb:                        std_reward_6 286.32467
wandb:                        std_reward_7 274.70951
wandb:                        std_reward_8 278.656
wandb:                        std_reward_9 254.10548
wandb:                            time/fps 597.0
wandb:                     train/approx_kl 0.0106
wandb:                 train/clip_fraction 0.13875
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.80915
wandb:            train/explained_variance 0.95603
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 9.91018
wandb:          train/policy_gradient_loss -0.00981
wandb:                           train/std 0.84977
wandb:                    train/value_loss 25.25248
wandb: 
wandb: Synced drawn-capybara-44: https://wandb.ai/tidiane/meta_rl_context/runs/c48c201m
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 13 other file(s)
wandb: Find logs at: ./wandb/run-20230626_052650-c48c201m/logs
wandb: Waiting for W&B process to finish... (success).
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1301/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1301/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1301/rollout/ep_rew_mean ▁▂▂▁▂▂▅▅▃▅▇█
wandb:                   PPO_1301/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1301/train/approx_kl ▆▁▅▁▃▇▅█▆█▄
wandb:        PPO_1301/train/clip_fraction ▅▁▅▂▃▇▄▇▅█▅
wandb:           PPO_1301/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1301/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1301/train/explained_variance ▁▆▆▇█▇█▇▇▇▆
wandb:        PPO_1301/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1301/train/loss ▂▁▁▂▁▃▄▄▃▁█
wandb: PPO_1301/train/policy_gradient_loss ▁▆▆▆▇▃▅▁▅▃█
wandb:                  PPO_1301/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1301/train/value_loss ▂▂▁▂▂▁▃▅▄▃█
wandb:                PPO_1311/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1311/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1311/rollout/ep_rew_mean ▁▂▃▂▄▅▅▅▆▆▇█
wandb:                   PPO_1311/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1311/train/approx_kl ▁▄▂▅▇▇█▆█▆▆
wandb:        PPO_1311/train/clip_fraction ▁▄▃▄▆▇▆▆▆██
wandb:           PPO_1311/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1311/train/entropy_loss ▁▂▂▃▄▄▅▆▇▇█
wandb:   PPO_1311/train/explained_variance ▇█▅▆▇▁▃▆▄█▆
wandb:        PPO_1311/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1311/train/loss █▃▃▃▃▆▂▄▂▁▂
wandb: PPO_1311/train/policy_gradient_loss ▇▃▅▃▁▂█▇▆▄█
wandb:                  PPO_1311/train/std █▇▇▆▅▅▄▃▂▂▁
wandb:           PPO_1311/train/value_loss ▇█▃▅█▅▂▃▄▁▁
wandb:                PPO_1321/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1321/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1321/rollout/ep_rew_mean ▁▂▁▂▃▃▅▅▄▄▆█
wandb:                   PPO_1321/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1321/train/approx_kl ▁▂▄▅▅▄▆▆██▇
wandb:        PPO_1321/train/clip_fraction ▁▄▄▆▅▆▆▇▇█▇
wandb:           PPO_1321/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1321/train/entropy_loss ▁▂▂▃▄▅▆▆▆▇█
wandb:   PPO_1321/train/explained_variance ▄▃▃▆▃▃▁▄█▆▇
wandb:        PPO_1321/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1321/train/loss ▆▃▃▆▁▅▂▃▂█▂
wandb: PPO_1321/train/policy_gradient_loss ▃▂▆▃▃▁▅▅▅█▇
wandb:                  PPO_1321/train/std █▇▆▆▅▄▃▃▃▂▁
wandb:           PPO_1321/train/value_loss █▄▇▅▅█▅▃▂▄▁
wandb:                PPO_1331/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1331/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1331/rollout/ep_rew_mean ▁▃▂▃▂▄▅▆▆▇▇█
wandb:                   PPO_1331/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1331/train/approx_kl ▂▆▄▁▅▂▅██▁▆
wandb:        PPO_1331/train/clip_fraction ▁▆▂▃▃▄▃▄█▄▄
wandb:           PPO_1331/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1331/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1331/train/explained_variance ▅▇▄▅▆█▇▁▄▅▅
wandb:        PPO_1331/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1331/train/loss ▂▁▇▂▅▃▆█▃▃▄
wandb: PPO_1331/train/policy_gradient_loss ▆▃▁▃▃▅█▇▇▆█
wandb:                  PPO_1331/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1331/train/value_loss █▂█▃▆▁▅▄▅▄▂
wandb:                PPO_1340/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1340/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1340/rollout/ep_rew_mean ▁▃▃▄▄▃▆█▆▆▄▅
wandb:                   PPO_1340/time/fps █▃▂▂▂▁▁▁▁▁▁▁
wandb:            PPO_1340/train/approx_kl ▇▁▆▅▆▇▅█▂▆▆
wandb:        PPO_1340/train/clip_fraction ▂▂▅▄▁▅▅█▁▃▄
wandb:           PPO_1340/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1340/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1340/train/explained_variance ▅▅▂▄▄▃▅▁▆▅█
wandb:        PPO_1340/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1340/train/loss ▆█▇▁▅▆▁▁█▇▂
wandb: PPO_1340/train/policy_gradient_loss █▆▅▄▃▂▇▅▁▇▇
wandb:                  PPO_1340/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1340/train/value_loss █▆▅▅█▇▅▁▄▅▄
wandb:                PPO_1349/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1349/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1349/rollout/ep_rew_mean ▁▃▅▅▇▅█▇█▇▆█
wandb:                   PPO_1349/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1349/train/approx_kl ▅▆▅▇▅▅▃▁██▃
wandb:        PPO_1349/train/clip_fraction ▅▆▇▆▆█▆▁▅▆▃
wandb:           PPO_1349/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1349/train/entropy_loss ▁▂▃▄▄▄▅▅▆▆█
wandb:   PPO_1349/train/explained_variance ▁▁▄▂▅▄▄▅▅▅█
wandb:        PPO_1349/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1349/train/loss ▂▁▂▁▂▃▃▄▂█▇
wandb: PPO_1349/train/policy_gradient_loss ▃▁▅▄▅▃█▅▅▄▃
wandb:                  PPO_1349/train/std █▇▆▅▅▄▄▄▃▃▁
wandb:           PPO_1349/train/value_loss ▃▄▁▃▃▃▁▄▃▄█
wandb:                PPO_1359/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1359/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1359/rollout/ep_rew_mean ▆▄▆▅▇▆▁█▅▄▁▄
wandb:                   PPO_1359/time/fps █▄▃▂▂▂▂▁▁▁▁▁
wandb:            PPO_1359/train/approx_kl ▅▁▁▅▄▇▂▅▂▃█
wandb:        PPO_1359/train/clip_fraction ▃▃▄█▃▃▄▄▁▆█
wandb:           PPO_1359/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1359/train/entropy_loss ▁▂▂▃▄▄▄▅▅▆█
wandb:   PPO_1359/train/explained_variance ▁▅▃▁▄▅█▆▄▅▆
wandb:        PPO_1359/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1359/train/loss ▄▆▄▃█▂▃▂▁▆▆
wandb: PPO_1359/train/policy_gradient_loss ▅▃▃▄▅▄▁█▄▇▁
wandb:                  PPO_1359/train/std ██▇▆▅▅▅▄▄▃▁
wandb:           PPO_1359/train/value_loss ▁▃▂▃▄▅▄▄█▄▅
wandb:                PPO_1369/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1369/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1369/rollout/ep_rew_mean ▆▄▄▃▅▅▁█▇▃▃▅
wandb:                   PPO_1369/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1369/train/approx_kl ▄▅▁▃▅█▆▅▆▂▆
wandb:        PPO_1369/train/clip_fraction ▄▂▃▁▃█▂▆▁▂▄
wandb:           PPO_1369/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1369/train/entropy_loss ▁▂▂▃▄▅▅▆▆▇█
wandb:   PPO_1369/train/explained_variance ▁▄▆▄▁▁█▄▃▄█
wandb:        PPO_1369/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1369/train/loss ▃▂▃█▆▂▃▂▂▇▁
wandb: PPO_1369/train/policy_gradient_loss ▅▄▅▄▁▆▃▆▂▅█
wandb:                  PPO_1369/train/std █▇▇▆▅▄▄▄▃▂▁
wandb:           PPO_1369/train/value_loss ▅▆▄▇▆▃▃▁▅█▂
wandb:                PPO_1379/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1379/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1379/rollout/ep_rew_mean ▄▆▆▃▆▁▅▄▄▃▅█
wandb:                   PPO_1379/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1379/train/approx_kl ▁▅█▁▃▂▆▃▁▅▂
wandb:        PPO_1379/train/clip_fraction ▃▇█▅▃▃▆▄▁█▅
wandb:           PPO_1379/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1379/train/entropy_loss ▁▂▂▄▄▅▅▅▅▆█
wandb:   PPO_1379/train/explained_variance ▁▇▂▅▂▃▁▄▁█▅
wandb:        PPO_1379/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1379/train/loss ▄█▁▂▅▃▃▂▂▆▆
wandb: PPO_1379/train/policy_gradient_loss ▁█▃▃▂▅▆▄▁▇▆
wandb:                  PPO_1379/train/std ██▇▆▅▅▅▄▄▃▁
wandb:           PPO_1379/train/value_loss ▃▁▂▃▅▅▂▄█▁▅
wandb:                    global_mean_eval ▁▂▄▅█▇███▇
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▂▄▅█▇███▇
wandb:                       mean_reward_1 ▁▂▄▅█▇██▇▇
wandb:                      mean_reward_10 ▁▂▅▅█▇▇██▇
wandb:                      mean_reward_11 ▁▂▄▅█▇███▇
wandb:                      mean_reward_12 ▁▂▄▅▇▇▇█▇▇
wandb:                      mean_reward_13 ▁▂▄▅█▇████
wandb:                      mean_reward_14 ▁▂▄▅█▇██▇▇
wandb:                      mean_reward_15 ▁▁▄▅▇▇▇█▇▆
wandb:                      mean_reward_16 ▁▂▄▅▇▇▇█▇▇
wandb:                      mean_reward_17 ▁▂▄▅█▇███▇
wandb:                      mean_reward_18 ▁▂▄▅█▇███▇
wandb:                      mean_reward_19 ▁▂▄▅█▇███▇
wandb:                       mean_reward_2 ▁▂▄▅█████▇
wandb:                      mean_reward_20 ▁▂▄▅█▇▇█▇▇
wandb:                      mean_reward_21 ▁▂▄▅█▇██▇█
wandb:                      mean_reward_22 ▁▂▄▅██▇█▇▇
wandb:                      mean_reward_23 ▁▂▅▅███▇█▇
wandb:                      mean_reward_24 ▁▂▄▅█████▇
wandb:                      mean_reward_25 ▁▂▄▅▇▇▇█▇▇
wandb:                      mean_reward_26 ▁▁▄▅█▇▇█▇█
wandb:                      mean_reward_27 ▁▂▅▅█▇▇█▇▇
wandb:                      mean_reward_28 ▁▂▅▅███▇█▇
wandb:                      mean_reward_29 ▁▂▄▅█▇▇█▇▇
wandb:                       mean_reward_3 ▁▂▄▅█▇█▇▇▇
wandb:                      mean_reward_30 ▁▁▄▅█▇█▇█▇
wandb:                      mean_reward_31 ▁▂▅▅█▇▇█▇▇
wandb:                      mean_reward_32 ▁▂▄▅█▇▇███
wandb:                      mean_reward_33 ▁▂▅▅██▇█▇█
wandb:                      mean_reward_34 ▁▂▄▅█████▇
wandb:                      mean_reward_35 ▁▂▄▅▇▇▇██▇
wandb:                       mean_reward_4 ▁▂▄▅█▇███▇
wandb:                       mean_reward_5 ▁▂▄▅█▇▇██▇
wandb:                       mean_reward_6 ▁▂▄▅█▇███▇
wandb:                       mean_reward_7 ▁▂▅▅████▇▇
wandb:                       mean_reward_8 ▁▂▄▅▇▇▇▇█▇
wandb:                       mean_reward_9 ▁▂▅▅██████
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▃▁▂▂▃▄▄▅▆█
wandb:                        std_reward_0 ▂▂▂▁▁▅▅▆▇█
wandb:                        std_reward_1 ▂▂▂▁▁▅▅▇▇█
wandb:                       std_reward_10 ▂▂▂▁▁▅▅▆▇█
wandb:                       std_reward_11 ▂▂▂▁▁▅▅▆▆█
wandb:                       std_reward_12 ▂▂▂▁▁▆▆▇██
wandb:                       std_reward_13 ▂▂▂▁▂▆▆▇▇█
wandb:                       std_reward_14 ▂▂▂▁▁▅▅▆██
wandb:                       std_reward_15 ▃▂▂▁▁▅▅▆▇█
wandb:                       std_reward_16 ▂▁▂▁▁▄▆▆▇█
wandb:                       std_reward_17 ▂▂▂▁▁▅▄▅▆█
wandb:                       std_reward_18 ▂▁▂▁▁▅▅▇▇█
wandb:                       std_reward_19 ▂▁▂▁▁▅▆▆▇█
wandb:                        std_reward_2 ▂▂▂▁▁▅▆▇▇█
wandb:                       std_reward_20 ▂▂▂▁▁▅▅▆▅█
wandb:                       std_reward_21 ▂▂▂▁▁▆▅▇██
wandb:                       std_reward_22 ▂▁▂▁▁▅▅▆▇█
wandb:                       std_reward_23 ▃▂▂▁▁▅▆▇██
wandb:                       std_reward_24 ▂▁▂▁▁▅▆▆▆█
wandb:                       std_reward_25 ▂▂▂▁▁▅▆▆██
wandb:                       std_reward_26 ▂▁▂▁▁▅▆▇██
wandb:                       std_reward_27 ▂▂▂▁▁▆▅▆██
wandb:                       std_reward_28 ▂▁▂▁▁▅▅▅▆█
wandb:                       std_reward_29 ▂▂▂▁▁▅▅▆▇█
wandb:                        std_reward_3 ▂▁▂▁▁▅▄▆▇█
wandb:                       std_reward_30 ▂▁▂▁▁▅▅▇▆█
wandb:                       std_reward_31 ▂▁▂▁▁▅▅▆▇█
wandb:                       std_reward_32 ▂▂▂▁▁▆▆██▇
wandb:                       std_reward_33 ▂▂▂▁▁▅▅▇██
wandb:                       std_reward_34 ▂▂▂▁▁▅▅▆▆█
wandb:                       std_reward_35 ▂▁▂▁▁▅▆▆▆█
wandb:                        std_reward_4 ▂▂▂▁▁▅▅▆▇█
wandb:                        std_reward_5 ▃▂▂▁▁▆▇▇▇█
wandb:                        std_reward_6 ▂▂▂▁▁▅▅▆▇█
wandb:                        std_reward_7 ▂▂▂▁▁▅▄▆▇█
wandb:                        std_reward_8 ▂▂▂▁▁▆▅▇▅█
wandb:                        std_reward_9 ▂▂▂▁▁▅▆▆▇█
wandb:                            time/fps █▄▃▂▂▂▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▄█▇▆▄▃▁▂▄▄
wandb:                 train/clip_fraction ▂▃▃▄▇▂▁▂▁▂▅█
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▁▁▁▁▆██
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▄▃▂▂▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▄▅▅█▆▇▆▅▃▂▁
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1301/global_step 212992
wandb:        PPO_1301/rollout/ep_len_mean 200.0
wandb:        PPO_1301/rollout/ep_rew_mean -844.30353
wandb:                   PPO_1301/time/fps 595.0
wandb:            PPO_1301/train/approx_kl 0.0112
wandb:        PPO_1301/train/clip_fraction 0.14424
wandb:           PPO_1301/train/clip_range 0.2
wandb:         PPO_1301/train/entropy_loss -7.55232
wandb:   PPO_1301/train/explained_variance 0.94968
wandb:        PPO_1301/train/learning_rate 0.0003
wandb:                 PPO_1301/train/loss 62.21507
wandb: PPO_1301/train/policy_gradient_loss -0.00538
wandb:                  PPO_1301/train/std 0.71094
wandb:           PPO_1301/train/value_loss 109.49551
wandb:                PPO_1311/global_step 212992
wandb:        PPO_1311/rollout/ep_len_mean 200.0
wandb:        PPO_1311/rollout/ep_rew_mean -757.08746
wandb:                   PPO_1311/time/fps 596.0
wandb:            PPO_1311/train/approx_kl 0.01273
wandb:        PPO_1311/train/clip_fraction 0.175
wandb:           PPO_1311/train/clip_range 0.2
wandb:         PPO_1311/train/entropy_loss -6.72545
wandb:   PPO_1311/train/explained_variance 0.96152
wandb:        PPO_1311/train/learning_rate 0.0003
wandb:                 PPO_1311/train/loss 25.72913
wandb: PPO_1311/train/policy_gradient_loss -0.0059
wandb:                  PPO_1311/train/std 0.63294
wandb:           PPO_1311/train/value_loss 66.42207
wandb:                PPO_1321/global_step 212992
wandb:        PPO_1321/rollout/ep_len_mean 200.0
wandb:        PPO_1321/rollout/ep_rew_mean -659.16974
wandb:                   PPO_1321/time/fps 594.0
wandb:            PPO_1321/train/approx_kl 0.01611
wandb:        PPO_1321/train/clip_fraction 0.19787
wandb:           PPO_1321/train/clip_range 0.2
wandb:         PPO_1321/train/entropy_loss -6.05237
wandb:   PPO_1321/train/explained_variance 0.97113
wandb:        PPO_1321/train/learning_rate 0.0003
wandb:                 PPO_1321/train/loss 22.1079
wandb: PPO_1321/train/policy_gradient_loss -0.00353
wandb:                  PPO_1321/train/std 0.5753
wandb:           PPO_1321/train/value_loss 49.95579
wandb:                PPO_1331/global_step 212992
wandb:        PPO_1331/rollout/ep_len_mean 200.0
wandb:        PPO_1331/rollout/ep_rew_mean -566.21906
wandb:                   PPO_1331/time/fps 602.0
wandb:            PPO_1331/train/approx_kl 0.01755
wandb:        PPO_1331/train/clip_fraction 0.22244
wandb:           PPO_1331/train/clip_range 0.2
wandb:         PPO_1331/train/entropy_loss -5.32586
wandb:   PPO_1331/train/explained_variance 0.96876
wandb:        PPO_1331/train/learning_rate 0.0003
wandb:                 PPO_1331/train/loss 31.77514
wandb: PPO_1331/train/policy_gradient_loss -0.00197
wandb:                  PPO_1331/train/std 0.51779
wandb:           PPO_1331/train/value_loss 42.64041
wandb:                PPO_1340/global_step 212992
wandb:        PPO_1340/rollout/ep_len_mean 200.0
wandb:        PPO_1340/rollout/ep_rew_mean -547.72687
wandb:                   PPO_1340/time/fps 599.0
wandb:            PPO_1340/train/approx_kl 0.01745
wandb:        PPO_1340/train/clip_fraction 0.22529
wandb:           PPO_1340/train/clip_range 0.2
wandb:         PPO_1340/train/entropy_loss -4.72037
wandb:   PPO_1340/train/explained_variance 0.97731
wandb:        PPO_1340/train/learning_rate 0.0003
wandb:                 PPO_1340/train/loss 12.71041
wandb: PPO_1340/train/policy_gradient_loss -0.00198
wandb:                  PPO_1340/train/std 0.47556
wandb:           PPO_1340/train/value_loss 49.99202
wandb:                PPO_1349/global_step 212992
wandb:        PPO_1349/rollout/ep_len_mean 200.0
wandb:        PPO_1349/rollout/ep_rew_mean -530.19379
wandb:                   PPO_1349/time/fps 600.0
wandb:            PPO_1349/train/approx_kl 0.01603
wandb:        PPO_1349/train/clip_fraction 0.21555
wandb:           PPO_1349/train/clip_range 0.2
wandb:         PPO_1349/train/entropy_loss -4.25689
wandb:   PPO_1349/train/explained_variance 0.9819
wandb:        PPO_1349/train/learning_rate 0.0003
wandb:                 PPO_1349/train/loss 61.56349
wandb: PPO_1349/train/policy_gradient_loss -0.00285
wandb:                  PPO_1349/train/std 0.44458
wandb:           PPO_1349/train/value_loss 120.18243
wandb:                PPO_1359/global_step 212992
wandb:        PPO_1359/rollout/ep_len_mean 200.0
wandb:        PPO_1359/rollout/ep_rew_mean -533.82062
wandb:                   PPO_1359/time/fps 597.0
wandb:            PPO_1359/train/approx_kl 0.01924
wandb:        PPO_1359/train/clip_fraction 0.24708
wandb:           PPO_1359/train/clip_range 0.2
wandb:         PPO_1359/train/entropy_loss -3.89775
wandb:   PPO_1359/train/explained_variance 0.98513
wandb:        PPO_1359/train/learning_rate 0.0003
wandb:                 PPO_1359/train/loss 50.78387
wandb: PPO_1359/train/policy_gradient_loss -0.002
wandb:                  PPO_1359/train/std 0.4223
wandb:           PPO_1359/train/value_loss 128.90108
wandb:                PPO_1369/global_step 212992
wandb:        PPO_1369/rollout/ep_len_mean 200.0
wandb:        PPO_1369/rollout/ep_rew_mean -515.25757
wandb:                   PPO_1369/time/fps 598.0
wandb:            PPO_1369/train/approx_kl 0.01975
wandb:        PPO_1369/train/clip_fraction 0.24427
wandb:           PPO_1369/train/clip_range 0.2
wandb:         PPO_1369/train/entropy_loss -3.55475
wandb:   PPO_1369/train/explained_variance 0.99014
wandb:        PPO_1369/train/learning_rate 0.0003
wandb:                 PPO_1369/train/loss 10.73959
wandb: PPO_1369/train/policy_gradient_loss 0.00225
wandb:                  PPO_1369/train/std 0.40251
wandb:           PPO_1369/train/value_loss 99.68839
wandb:                PPO_1379/global_step 212992
wandb:        PPO_1379/rollout/ep_len_mean 200.0
wandb:        PPO_1379/rollout/ep_rew_mean -502.23993
wandb:                   PPO_1379/time/fps 601.0
wandb:            PPO_1379/train/approx_kl 0.01654
wandb:        PPO_1379/train/clip_fraction 0.24799
wandb:           PPO_1379/train/clip_range 0.2
wandb:         PPO_1379/train/entropy_loss -3.30315
wandb:   PPO_1379/train/explained_variance 0.98788
wandb:        PPO_1379/train/learning_rate 0.0003
wandb:                 PPO_1379/train/loss 124.57511
wandb: PPO_1379/train/policy_gradient_loss 0.00248
wandb:                  PPO_1379/train/std 0.38871
wandb:           PPO_1379/train/value_loss 212.866
wandb:                    global_mean_eval -512.7558
wandb:                         global_step 212992
wandb:                       mean_reward_0 -543.7995
wandb:                       mean_reward_1 -521.35764
wandb:                      mean_reward_10 -508.36906
wandb:                      mean_reward_11 -526.55397
wandb:                      mean_reward_12 -488.85894
wandb:                      mean_reward_13 -484.99412
wandb:                      mean_reward_14 -511.77694
wandb:                      mean_reward_15 -528.4226
wandb:                      mean_reward_16 -518.97618
wandb:                      mean_reward_17 -518.65362
wandb:                      mean_reward_18 -512.42209
wandb:                      mean_reward_19 -526.56176
wandb:                       mean_reward_2 -517.77268
wandb:                      mean_reward_20 -513.05269
wandb:                      mean_reward_21 -484.79706
wandb:                      mean_reward_22 -534.83722
wandb:                      mean_reward_23 -508.20946
wandb:                      mean_reward_24 -533.07181
wandb:                      mean_reward_25 -518.97465
wandb:                      mean_reward_26 -491.13164
wandb:                      mean_reward_27 -499.21648
wandb:                      mean_reward_28 -547.43699
wandb:                      mean_reward_29 -502.5384
wandb:                       mean_reward_3 -513.08751
wandb:                      mean_reward_30 -524.28636
wandb:                      mean_reward_31 -508.22029
wandb:                      mean_reward_32 -503.10243
wandb:                      mean_reward_33 -492.78568
wandb:                      mean_reward_34 -513.56429
wandb:                      mean_reward_35 -515.51464
wandb:                       mean_reward_4 -515.57331
wandb:                       mean_reward_5 -489.22696
wandb:                       mean_reward_6 -518.00024
wandb:                       mean_reward_7 -523.70558
wandb:                       mean_reward_8 -509.7879
wandb:                       mean_reward_9 -490.56794
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -935.32892
wandb:                        std_reward_0 186.66603
wandb:                        std_reward_1 172.5908
wandb:                       std_reward_10 190.25908
wandb:                       std_reward_11 191.21732
wandb:                       std_reward_12 163.59867
wandb:                       std_reward_13 167.60356
wandb:                       std_reward_14 183.74414
wandb:                       std_reward_15 179.85489
wandb:                       std_reward_16 188.6409
wandb:                       std_reward_17 197.99994
wandb:                       std_reward_18 184.29904
wandb:                       std_reward_19 183.58906
wandb:                        std_reward_2 182.72258
wandb:                       std_reward_20 195.06284
wandb:                       std_reward_21 163.29174
wandb:                       std_reward_22 198.42877
wandb:                       std_reward_23 167.22026
wandb:                       std_reward_24 183.68175
wandb:                       std_reward_25 166.63949
wandb:                       std_reward_26 179.72795
wandb:                       std_reward_27 161.6134
wandb:                       std_reward_28 219.6442
wandb:                       std_reward_29 183.07093
wandb:                        std_reward_3 186.76011
wandb:                       std_reward_30 186.88418
wandb:                       std_reward_31 187.22383
wandb:                       std_reward_32 150.99516
wandb:                       std_reward_33 174.77395
wandb:                       std_reward_34 190.22823
wandb:                       std_reward_35 189.53982
wandb:                        std_reward_4 183.47765
wandb:                        std_reward_5 164.18563
wandb:                        std_reward_6 198.4082
wandb:                        std_reward_7 190.61666
wandb:                        std_reward_8 193.92551
wandb:                        std_reward_9 181.88884
wandb:                            time/fps 600.0
wandb:                     train/approx_kl 0.01017
wandb:                 train/clip_fraction 0.12781
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.67882
wandb:            train/explained_variance 0.87368
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 7.2388
wandb:          train/policy_gradient_loss -0.01025
wandb:                           train/std 0.83444
wandb:                    train/value_loss 20.95161
wandb: 
wandb: Synced twilight-dawn-43: https://wandb.ai/tidiane/meta_rl_context/runs/1mm4memk
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 13 other file(s)
wandb: Find logs at: ./wandb/run-20230626_052650-1mm4memk/logs
wandb: 
wandb: Run history:
wandb:                PPO_1300/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1300/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1300/rollout/ep_rew_mean ▁▁▃▃▄▄▅▆▇▇▇█
wandb:                   PPO_1300/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1300/train/approx_kl ▁▂▃█▁▇▆▆█▆▅
wandb:        PPO_1300/train/clip_fraction ▁▁▅▇▅▅▅▆█▇▅
wandb:           PPO_1300/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1300/train/entropy_loss ▁▂▃▄▄▅▅▆▇▇█
wandb:   PPO_1300/train/explained_variance ▁▆▇█▆█▄▇▅█▅
wandb:        PPO_1300/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1300/train/loss ▃▅▂▁▂▄▂▄▄▄█
wandb: PPO_1300/train/policy_gradient_loss █▆▃▁▄▃▆▄▃▁▆
wandb:                  PPO_1300/train/std █▇▆▅▅▄▃▃▂▂▁
wandb:           PPO_1300/train/value_loss ▅▃▁▁▄▄▇▇▅▅█
wandb:                PPO_1308/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1308/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1308/rollout/ep_rew_mean ▁▂▃▃▃▄▄▄▅▅▆█
wandb:                   PPO_1308/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1308/train/approx_kl ▁▁▆▂▄▂▄▆▅▃█
wandb:        PPO_1308/train/clip_fraction ▁▁▃▂▄▅▇▆█▅▅
wandb:           PPO_1308/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1308/train/entropy_loss ▁▂▃▃▄▄▅▅▆▇█
wandb:   PPO_1308/train/explained_variance ▆▄▅█▁▃▄▅▆▂▅
wandb:        PPO_1308/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1308/train/loss ▆▄▅█▄▅█▇▁▂▇
wandb: PPO_1308/train/policy_gradient_loss ▁▄▅▇▄▅▁▂▆▇█
wandb:                  PPO_1308/train/std █▇▆▆▅▅▄▃▃▂▁
wandb:           PPO_1308/train/value_loss ▆██▅▆▃▃▄▁▁▃
wandb:                PPO_1318/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1318/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1318/rollout/ep_rew_mean ▁▁▃▄▅▄▅▅▅▇██
wandb:                   PPO_1318/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1318/train/approx_kl ▄▃▃▂▂▃█▃█▁▂
wandb:        PPO_1318/train/clip_fraction ▃▁▃▄▄▆█▂▆▁▂
wandb:           PPO_1318/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1318/train/entropy_loss ▁▂▃▃▄▄▅▆▆▇█
wandb:   PPO_1318/train/explained_variance ▆█▆▅▆▁▅▇▄▆▂
wandb:        PPO_1318/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1318/train/loss ▂█▂▄▇▁▁▅▂▆▅
wandb: PPO_1318/train/policy_gradient_loss ▁▁▃▂▂▂▂▅██▆
wandb:                  PPO_1318/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1318/train/value_loss ▄▅▃█▃▆▁▃▄▄▇
wandb:                PPO_1328/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1328/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1328/rollout/ep_rew_mean ▂▂▅▁▄▅▇▂▇▃██
wandb:                   PPO_1328/time/fps █▃▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1328/train/approx_kl ▁█▃▄▇▂█▅▇▇▇
wandb:        PPO_1328/train/clip_fraction ▁▄▃▂▅▃█▁▅▃▅
wandb:           PPO_1328/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1328/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1328/train/explained_variance ▁▂▂▄▆▆▇▆█▇█
wandb:        PPO_1328/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1328/train/loss ▁▁▄▅▅█▃█▁▄▄
wandb: PPO_1328/train/policy_gradient_loss ▅▆▃▂▃▁▄▆▅▄█
wandb:                  PPO_1328/train/std █▇▇▆▅▅▄▄▃▂▁
wandb:           PPO_1328/train/value_loss ▂▂▃▇▄▅▁▅▂█▆
wandb:                PPO_1338/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1338/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1338/rollout/ep_rew_mean ▆█▇█▇▄▆▄▆▂▁▇
wandb:                   PPO_1338/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1338/train/approx_kl ▅▇▆█▄▃▆▁▅▃▅
wandb:        PPO_1338/train/clip_fraction ▄█▆▇▃▂▄▁▅▂▅
wandb:           PPO_1338/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1338/train/entropy_loss ▁▂▃▃▅▅▅▅▆▇█
wandb:   PPO_1338/train/explained_variance ▃▄▁▇▄▂█▅▅▆▃
wandb:        PPO_1338/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1338/train/loss ▁▁▁▁█▆▃▃▂▅▃
wandb: PPO_1338/train/policy_gradient_loss ▁█▆▇▁▅▃▅▆▅▆
wandb:                  PPO_1338/train/std █▇▆▅▄▄▄▄▃▂▁
wandb:           PPO_1338/train/value_loss ▃▂▂▁▅▇▃█▅▇█
wandb:                PPO_1348/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1348/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1348/rollout/ep_rew_mean ▇▄▄█▄▃▃▃▅▃▁▄
wandb:                   PPO_1348/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1348/train/approx_kl ▇█▂█▂▃▅▁▂▄▃
wandb:        PPO_1348/train/clip_fraction ▅▃▂█▁▃▄▂▃▂▃
wandb:           PPO_1348/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1348/train/entropy_loss ▁▂▂▄▄▄▅▆▆▆█
wandb:   PPO_1348/train/explained_variance ▇██▁████▇▇█
wandb:        PPO_1348/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1348/train/loss ▁▅█▁▅▃▅▅▇▂▄
wandb: PPO_1348/train/policy_gradient_loss ▇▇▂▁▁█▃▃▃▂▅
wandb:                  PPO_1348/train/std ██▇▆▆▅▄▄▃▃▁
wandb:           PPO_1348/train/value_loss ▃▅█▁▆▆▆▇▅▇█
wandb:                PPO_1358/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1358/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1358/rollout/ep_rew_mean ▁▄▁▂▄▆▆▄▇█▅▄
wandb:                   PPO_1358/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1358/train/approx_kl ▁▂▂▇▁▄▃▅▄█▆
wandb:        PPO_1358/train/clip_fraction ▁▄▂▆▁▇▃▅▃█▄
wandb:           PPO_1358/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1358/train/entropy_loss ▁▂▄▅▆▇▆▇▇▇█
wandb:   PPO_1358/train/explained_variance ▃▇█▇▄▇▅█▇▁▅
wandb:        PPO_1358/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1358/train/loss ▄▄▃▁█▃▃▃▃▄█
wandb: PPO_1358/train/policy_gradient_loss ▁▆▃▁▃▄▄▆▆█▄
wandb:                  PPO_1358/train/std █▇▆▄▃▃▄▃▂▂▁
wandb:           PPO_1358/train/value_loss █▅▆▃█▄▆▄▆▁▅
wandb:                PPO_1368/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1368/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1368/rollout/ep_rew_mean ▃▁▅▅▇██▄█▅▅▅
wandb:                   PPO_1368/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1368/train/approx_kl ▇▄█▃▃▅▅▁▅▅▅
wandb:        PPO_1368/train/clip_fraction ▄▅▄▄▃▇▆▁▄▅█
wandb:           PPO_1368/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1368/train/entropy_loss ▁▂▂▃▃▄▄▄▅▇█
wandb:   PPO_1368/train/explained_variance ▂█▆▂▁█▆▄▅▅▄
wandb:        PPO_1368/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1368/train/loss ▅█▄▄▄▂▁▂▃▁▁
wandb: PPO_1368/train/policy_gradient_loss ▅▅▅▁▃▅▇▄▇▆█
wandb:                  PPO_1368/train/std ██▇▇▆▅▅▅▄▂▁
wandb:           PPO_1368/train/value_loss ▇▃▅▅▅▁▄█▄▄▂
wandb:                PPO_1378/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1378/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1378/rollout/ep_rew_mean █▅▁▆▇▂▁▃▆▆▅▅
wandb:                   PPO_1378/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1378/train/approx_kl ▄█▅▆▂▄▁▂█▃▂
wandb:        PPO_1378/train/clip_fraction ▇▆▁█▂▄▁▁█▃▆
wandb:           PPO_1378/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1378/train/entropy_loss ▁▂▃▃▄▄▅▅▆▇█
wandb:   PPO_1378/train/explained_variance ▁▇██▂▄▇▆▅▅█
wandb:        PPO_1378/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1378/train/loss ▃▆▃▃▁▃▄█▁▄▃
wandb: PPO_1378/train/policy_gradient_loss ▆▅▁▆▂▂▁▂█▃▇
wandb:                  PPO_1378/train/std █▇▇▆▅▄▅▄▃▃▁
wandb:           PPO_1378/train/value_loss ▃▂▇▁▆▅▄█▂▅▆
wandb:                    global_mean_eval ▁▄▆█▇▇▆▆▆▆
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▄▆█▆▇█▆▆▆
wandb:                       mean_reward_1 ▁▄▆█▇▆▆▅▇▅
wandb:                      mean_reward_10 ▁▄▆█▆█▆▇▇▇
wandb:                      mean_reward_11 ▁▄▆█▆▅▅▅▇▆
wandb:                      mean_reward_12 ▁▄▇███▇▆▆▆
wandb:                      mean_reward_13 ▁▄▇██▇█▆▆▅
wandb:                      mean_reward_14 ▁▄▇██▅▆▆▆▆
wandb:                      mean_reward_15 ▁▅▇█▇▇█▇▆▆
wandb:                      mean_reward_16 ▁▅▇██▆▇▆▇▅
wandb:                      mean_reward_17 ▁▄▆██▆▅▅▇▄
wandb:                      mean_reward_18 ▁▄▆█▇▇▇▇▆▆
wandb:                      mean_reward_19 ▁▄▆█▇▆▅▆▆▇
wandb:                       mean_reward_2 ▁▄▆██▆▆█▄▅
wandb:                      mean_reward_20 ▁▅▆█▇▇▇▅▅▆
wandb:                      mean_reward_21 ▁▄▆██▇▆▇▄▇
wandb:                      mean_reward_22 ▁▄▆█▇▆▇▇▇▆
wandb:                      mean_reward_23 ▁▅▇██▇█▅▅▇
wandb:                      mean_reward_24 ▁▄▆█▇█▅▆▇▅
wandb:                      mean_reward_25 ▁▅▆█▇▆▆▇▆▆
wandb:                      mean_reward_26 ▁▄▆█▇▇▆▆▆▅
wandb:                      mean_reward_27 ▁▄▆█▇▇▆▆▆▄
wandb:                      mean_reward_28 ▁▄▆█▇▆▆▆█▆
wandb:                      mean_reward_29 ▁▄▆█▇▇▅▇▇▆
wandb:                       mean_reward_3 ▁▅▆██▇▇▅▅▆
wandb:                      mean_reward_30 ▁▅▇██▆▇▇▆▇
wandb:                      mean_reward_31 ▁▄▆█▇▅▆▆▅▅
wandb:                      mean_reward_32 ▁▄▆▇▇▆█▅▇▄
wandb:                      mean_reward_33 ▁▄▆█▇▇▇▇▅▆
wandb:                      mean_reward_34 ▁▄▆██▆▆▆▆▆
wandb:                      mean_reward_35 ▁▄▆██▆▆▆▆▆
wandb:                       mean_reward_4 ▁▄▇██▇▆▅▇▅
wandb:                       mean_reward_5 ▁▄▆██▆▇▇▇▇
wandb:                       mean_reward_6 ▁▄▆▇▇▆▆▅▆█
wandb:                       mean_reward_7 ▁▄▇██▇▇▇▆▆
wandb:                       mean_reward_8 ▁▄▆█▇▆▆▅▆▆
wandb:                       mean_reward_9 ▁▄▆█▇▆▇▆▆▆
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▂▂▂▃▅▃▆▇▇▇█▆
wandb:                        std_reward_0 ▁▁▁▃▆▇▆▇██
wandb:                        std_reward_1 ▁▁▁▃▆███▆█
wandb:                       std_reward_10 ▂▁▂▃▇▆█▇▇█
wandb:                       std_reward_11 ▁▁▂▃▆▇██▇▇
wandb:                       std_reward_12 ▁▁▁▃▅▆▇███
wandb:                       std_reward_13 ▁▁▁▃▅▆▅▇▇█
wandb:                       std_reward_14 ▁▁▁▃▅██▇▇█
wandb:                       std_reward_15 ▁▁▁▃▅▆▆▇▇█
wandb:                       std_reward_16 ▂▁▁▃▄▆▇▇▆█
wandb:                       std_reward_17 ▁▁▁▂▄▆▇▇▅█
wandb:                       std_reward_18 ▁▁▁▃▅▆▇▆██
wandb:                       std_reward_19 ▁▁▂▃▆▇█▇█▇
wandb:                        std_reward_2 ▁▁▁▃▅▇▇▆██
wandb:                       std_reward_20 ▂▁▁▃▆▇▇███
wandb:                       std_reward_21 ▁▁▁▃▅▆▇▆█▆
wandb:                       std_reward_22 ▁▁▁▃▆▇▇▇██
wandb:                       std_reward_23 ▁▁▁▃▅▆▆██▆
wandb:                       std_reward_24 ▁▁▁▃▅▆█▇▇█
wandb:                       std_reward_25 ▂▁▂▃▆█████
wandb:                       std_reward_26 ▁▁▁▃▅▆▇▇▇█
wandb:                       std_reward_27 ▁▁▁▃▅▆▆▇▇█
wandb:                       std_reward_28 ▁▁▁▃▅▇██▆█
wandb:                       std_reward_29 ▁▁▁▃▆▆█▇▇▇
wandb:                        std_reward_3 ▁▁▁▄▄▆▇███
wandb:                       std_reward_30 ▁▁▂▃▅▇█▇██
wandb:                       std_reward_31 ▁▁▁▃▅█▇███
wandb:                       std_reward_32 ▂▁▁▃▅▆▅█▇█
wandb:                       std_reward_33 ▂▁▁▃▆▇▇▆██
wandb:                       std_reward_34 ▁▁▁▃▅▇█▇▇█
wandb:                       std_reward_35 ▁▁▁▃▄▇▇▇██
wandb:                        std_reward_4 ▁▁▁▂▅▅▇█▇▇
wandb:                        std_reward_5 ▂▁▁▃▅█▇███
wandb:                        std_reward_6 ▂▁▂▃▅▇▇█▇▅
wandb:                        std_reward_7 ▁▁▁▃▅▆▇▇▇█
wandb:                        std_reward_8 ▁▁▁▃▆▇█▇▇█
wandb:                        std_reward_9 ▂▁▁▃▆▇█▇▇█
wandb:                            time/fps █▄▃▂▂▂▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▃▂▃█▇▂▁▃▃▄▄▅
wandb:                 train/clip_fraction ▅▄▄▅▅▂▁▅▆█▇▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▃▃▄▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▁▄▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▄▃▃▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▆▆██▇▃▃▁▃▃
wandb:                           train/std █▇▇▆▅▅▄▃▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1300/global_step 212992
wandb:        PPO_1300/rollout/ep_len_mean 200.0
wandb:        PPO_1300/rollout/ep_rew_mean -787.52649
wandb:                   PPO_1300/time/fps 608.0
wandb:            PPO_1300/train/approx_kl 0.01178
wandb:        PPO_1300/train/clip_fraction 0.14671
wandb:           PPO_1300/train/clip_range 0.2
wandb:         PPO_1300/train/entropy_loss -7.64494
wandb:   PPO_1300/train/explained_variance 0.95211
wandb:        PPO_1300/train/learning_rate 0.0003
wandb:                 PPO_1300/train/loss 72.50619
wandb: PPO_1300/train/policy_gradient_loss -0.00744
wandb:                  PPO_1300/train/std 0.72025
wandb:           PPO_1300/train/value_loss 97.59276
wandb:                PPO_1308/global_step 212992
wandb:        PPO_1308/rollout/ep_len_mean 200.0
wandb:        PPO_1308/rollout/ep_rew_mean -691.02704
wandb:                   PPO_1308/time/fps 604.0
wandb:            PPO_1308/train/approx_kl 0.01554
wandb:        PPO_1308/train/clip_fraction 0.18
wandb:           PPO_1308/train/clip_range 0.2
wandb:         PPO_1308/train/entropy_loss -6.82727
wandb:   PPO_1308/train/explained_variance 0.95242
wandb:        PPO_1308/train/learning_rate 0.0003
wandb:                 PPO_1308/train/loss 38.44964
wandb: PPO_1308/train/policy_gradient_loss -0.0061
wandb:                  PPO_1308/train/std 0.64219
wandb:           PPO_1308/train/value_loss 70.37889
wandb:                PPO_1318/global_step 212992
wandb:        PPO_1318/rollout/ep_len_mean 200.0
wandb:        PPO_1318/rollout/ep_rew_mean -577.26294
wandb:                   PPO_1318/time/fps 607.0
wandb:            PPO_1318/train/approx_kl 0.01418
wandb:        PPO_1318/train/clip_fraction 0.18947
wandb:           PPO_1318/train/clip_range 0.2
wandb:         PPO_1318/train/entropy_loss -6.13125
wandb:   PPO_1318/train/explained_variance 0.92549
wandb:        PPO_1318/train/learning_rate 0.0003
wandb:                 PPO_1318/train/loss 39.50974
wandb: PPO_1318/train/policy_gradient_loss -0.00403
wandb:                  PPO_1318/train/std 0.58114
wandb:           PPO_1318/train/value_loss 88.93178
wandb:                PPO_1328/global_step 212992
wandb:        PPO_1328/rollout/ep_len_mean 200.0
wandb:        PPO_1328/rollout/ep_rew_mean -544.65839
wandb:                   PPO_1328/time/fps 600.0
wandb:            PPO_1328/train/approx_kl 0.01455
wandb:        PPO_1328/train/clip_fraction 0.19086
wandb:           PPO_1328/train/clip_range 0.2
wandb:         PPO_1328/train/entropy_loss -5.67665
wandb:   PPO_1328/train/explained_variance 0.97439
wandb:        PPO_1328/train/learning_rate 0.0003
wandb:                 PPO_1328/train/loss 67.59825
wandb: PPO_1328/train/policy_gradient_loss -0.00291
wandb:                  PPO_1328/train/std 0.54334
wandb:           PPO_1328/train/value_loss 271.56732
wandb:                PPO_1338/global_step 212992
wandb:        PPO_1338/rollout/ep_len_mean 200.0
wandb:        PPO_1338/rollout/ep_rew_mean -568.77698
wandb:                   PPO_1338/time/fps 604.0
wandb:            PPO_1338/train/approx_kl 0.01365
wandb:        PPO_1338/train/clip_fraction 0.17439
wandb:           PPO_1338/train/clip_range 0.2
wandb:         PPO_1338/train/entropy_loss -5.32477
wandb:   PPO_1338/train/explained_variance 0.9703
wandb:        PPO_1338/train/learning_rate 0.0003
wandb:                 PPO_1338/train/loss 150.11108
wandb: PPO_1338/train/policy_gradient_loss -0.00241
wandb:                  PPO_1338/train/std 0.5179
wandb:           PPO_1338/train/value_loss 601.03796
wandb:                PPO_1348/global_step 212992
wandb:        PPO_1348/rollout/ep_len_mean 200.0
wandb:        PPO_1348/rollout/ep_rew_mean -578.24298
wandb:                   PPO_1348/time/fps 604.0
wandb:            PPO_1348/train/approx_kl 0.01326
wandb:        PPO_1348/train/clip_fraction 0.16091
wandb:           PPO_1348/train/clip_range 0.2
wandb:         PPO_1348/train/entropy_loss -4.9795
wandb:   PPO_1348/train/explained_variance 0.98282
wandb:        PPO_1348/train/learning_rate 0.0003
wandb:                 PPO_1348/train/loss 218.25394
wandb: PPO_1348/train/policy_gradient_loss -0.00217
wandb:                  PPO_1348/train/std 0.49296
wandb:           PPO_1348/train/value_loss 668.21997
wandb:                PPO_1358/global_step 212992
wandb:        PPO_1358/rollout/ep_len_mean 200.0
wandb:        PPO_1358/rollout/ep_rew_mean -566.76184
wandb:                   PPO_1358/time/fps 603.0
wandb:            PPO_1358/train/approx_kl 0.01527
wandb:        PPO_1358/train/clip_fraction 0.17438
wandb:           PPO_1358/train/clip_range 0.2
wandb:         PPO_1358/train/entropy_loss -4.66892
wandb:   PPO_1358/train/explained_variance 0.98198
wandb:        PPO_1358/train/learning_rate 0.0003
wandb:                 PPO_1358/train/loss 775.52191
wandb: PPO_1358/train/policy_gradient_loss -0.00242
wandb:                  PPO_1358/train/std 0.47234
wandb:           PPO_1358/train/value_loss 544.29004
wandb:                PPO_1368/global_step 212992
wandb:        PPO_1368/rollout/ep_len_mean 200.0
wandb:        PPO_1368/rollout/ep_rew_mean -591.97894
wandb:                   PPO_1368/time/fps 603.0
wandb:            PPO_1368/train/approx_kl 0.01609
wandb:        PPO_1368/train/clip_fraction 0.25594
wandb:           PPO_1368/train/clip_range 0.2
wandb:         PPO_1368/train/entropy_loss -4.28729
wandb:   PPO_1368/train/explained_variance 0.98306
wandb:        PPO_1368/train/learning_rate 0.0003
wandb:                 PPO_1368/train/loss 38.86061
wandb: PPO_1368/train/policy_gradient_loss 0.00114
wandb:                  PPO_1368/train/std 0.44715
wandb:           PPO_1368/train/value_loss 425.39679
wandb:                PPO_1378/global_step 212992
wandb:        PPO_1378/rollout/ep_len_mean 200.0
wandb:        PPO_1378/rollout/ep_rew_mean -570.50494
wandb:                   PPO_1378/time/fps 604.0
wandb:            PPO_1378/train/approx_kl 0.01349
wandb:        PPO_1378/train/clip_fraction 0.21747
wandb:           PPO_1378/train/clip_range 0.2
wandb:         PPO_1378/train/entropy_loss -4.03545
wandb:   PPO_1378/train/explained_variance 0.98734
wandb:        PPO_1378/train/learning_rate 0.0003
wandb:                 PPO_1378/train/loss 254.70189
wandb: PPO_1378/train/policy_gradient_loss 0.00276
wandb:                  PPO_1378/train/std 0.43082
wandb:           PPO_1378/train/value_loss 774.4314
wandb:                    global_mean_eval -585.45343
wandb:                         global_step 212992
wandb:                       mean_reward_0 -550.34551
wandb:                       mean_reward_1 -607.23639
wandb:                      mean_reward_10 -541.60634
wandb:                      mean_reward_11 -556.2765
wandb:                      mean_reward_12 -598.84091
wandb:                      mean_reward_13 -604.38065
wandb:                      mean_reward_14 -590.85316
wandb:                      mean_reward_15 -614.2403
wandb:                      mean_reward_16 -623.10307
wandb:                      mean_reward_17 -693.69952
wandb:                      mean_reward_18 -599.03946
wandb:                      mean_reward_19 -527.12079
wandb:                       mean_reward_2 -636.5894
wandb:                      mean_reward_20 -593.52224
wandb:                      mean_reward_21 -512.83258
wandb:                      mean_reward_22 -580.6403
wandb:                      mean_reward_23 -539.15343
wandb:                      mean_reward_24 -616.59632
wandb:                      mean_reward_25 -578.38291
wandb:                      mean_reward_26 -637.88786
wandb:                      mean_reward_27 -650.81984
wandb:                      mean_reward_28 -559.43742
wandb:                      mean_reward_29 -568.37157
wandb:                       mean_reward_3 -596.22445
wandb:                      mean_reward_30 -543.69959
wandb:                      mean_reward_31 -597.97854
wandb:                      mean_reward_32 -641.83267
wandb:                      mean_reward_33 -586.64528
wandb:                      mean_reward_34 -578.35785
wandb:                      mean_reward_35 -586.08723
wandb:                       mean_reward_4 -619.55645
wandb:                       mean_reward_5 -538.85307
wandb:                       mean_reward_6 -456.35632
wandb:                       mean_reward_7 -575.36386
wandb:                       mean_reward_8 -589.33793
wandb:                       mean_reward_9 -585.05393
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -939.39148
wandb:                        std_reward_0 354.79289
wandb:                        std_reward_1 395.45364
wandb:                       std_reward_10 369.76622
wandb:                       std_reward_11 370.17749
wandb:                       std_reward_12 408.84645
wandb:                       std_reward_13 424.07685
wandb:                       std_reward_14 395.57651
wandb:                       std_reward_15 416.57923
wandb:                       std_reward_16 427.12893
wandb:                       std_reward_17 457.78585
wandb:                       std_reward_18 413.48022
wandb:                       std_reward_19 340.66245
wandb:                        std_reward_2 427.23622
wandb:                       std_reward_20 388.47772
wandb:                       std_reward_21 328.12504
wandb:                       std_reward_22 388.46005
wandb:                       std_reward_23 338.99712
wandb:                       std_reward_24 404.91467
wandb:                       std_reward_25 377.02306
wandb:                       std_reward_26 426.07803
wandb:                       std_reward_27 430.24919
wandb:                       std_reward_28 358.33016
wandb:                       std_reward_29 372.71553
wandb:                        std_reward_3 408.24533
wandb:                       std_reward_30 370.62899
wandb:                       std_reward_31 403.6844
wandb:                       std_reward_32 411.20217
wandb:                       std_reward_33 394.79641
wandb:                       std_reward_34 386.34175
wandb:                       std_reward_35 388.28665
wandb:                        std_reward_4 410.96251
wandb:                        std_reward_5 355.26455
wandb:                        std_reward_6 249.69963
wandb:                        std_reward_7 398.51302
wandb:                        std_reward_8 395.3954
wandb:                        std_reward_9 389.40886
wandb:                            time/fps 600.0
wandb:                     train/approx_kl 0.01128
wandb:                 train/clip_fraction 0.12654
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7942
wandb:            train/explained_variance 0.92484
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.3112
wandb:          train/policy_gradient_loss -0.0089
wandb:                           train/std 0.84816
wandb:                    train/value_loss 56.39774
wandb: 
wandb: Synced hearty-mountain-51: https://wandb.ai/tidiane/meta_rl_context/runs/1mmglo7l
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 14 other file(s)
wandb: Find logs at: ./wandb/run-20230626_052650-1mmglo7l/logs
wandb: Waiting for W&B process to finish... (success).
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1299/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1299/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1299/rollout/ep_rew_mean ▁▁▃▂▃▄▄▆▅▇▇█
wandb:                   PPO_1299/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1299/train/approx_kl ▂▄▂▂▁▄██▅█▃
wandb:        PPO_1299/train/clip_fraction ▄▃▃▃▁▅█▅▆▇▄
wandb:           PPO_1299/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1299/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1299/train/explained_variance ▃▆▇█▃▅▆▄▆▁▃
wandb:        PPO_1299/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1299/train/loss ▂▁▇▄▆▄▆█▄▇▄
wandb: PPO_1299/train/policy_gradient_loss ▁▃▃▆█▇▅▂▄▄▅
wandb:                  PPO_1299/train/std █▇▆▅▅▄▄▃▃▂▁
wandb:           PPO_1299/train/value_loss ▁▂▂▃▆▅▄▅▄▅█
wandb:                PPO_1310/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1310/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1310/rollout/ep_rew_mean ▁▂▁▂▂▄▅▅▆▆▆█
wandb:                   PPO_1310/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1310/train/approx_kl ▁▃▃▁▁▅▁▆▄▇█
wandb:        PPO_1310/train/clip_fraction ▁▂▂▂▁▄▃▅▄▆█
wandb:           PPO_1310/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1310/train/entropy_loss ▁▂▂▃▃▄▅▅▆▇█
wandb:   PPO_1310/train/explained_variance ▇▅▅▄▄▃▅▁▂▃█
wandb:        PPO_1310/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1310/train/loss ▃█▃▄▃▃▂▂▂▁▂
wandb: PPO_1310/train/policy_gradient_loss ▄▄▃▁█▆▇▅▄▄▇
wandb:                  PPO_1310/train/std █▇▇▆▆▅▄▃▃▂▁
wandb:           PPO_1310/train/value_loss ▇▇▇██▇▄▄▂▂▁
wandb:                PPO_1320/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1320/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1320/rollout/ep_rew_mean ▂▁▂▃▄▄▄▅▆▅▆█
wandb:                   PPO_1320/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1320/train/approx_kl ▅▃▇▃▇▆█▃▂▁▆
wandb:        PPO_1320/train/clip_fraction ▃▆▇▁▇█▄▇▃▆▇
wandb:           PPO_1320/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1320/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1320/train/explained_variance ▂▂▄▅▄▃▁▄▃▇█
wandb:        PPO_1320/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1320/train/loss ▃▂▁▂▁▃▅▂█▆▁
wandb: PPO_1320/train/policy_gradient_loss ▃▃▄▄▁▂▂▅▅▆█
wandb:                  PPO_1320/train/std █▇▇▆▅▅▄▃▃▂▁
wandb:           PPO_1320/train/value_loss ▆▅▁▅▇▁▆▃██▄
wandb:                PPO_1330/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1330/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1330/rollout/ep_rew_mean ▄▃▃▆▄▁▃▄▆▆▆█
wandb:                   PPO_1330/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1330/train/approx_kl ▄▂▁█▃█▅▅▄▅▂
wandb:        PPO_1330/train/clip_fraction ▅▃▁█▂█▆▅▄█▁
wandb:           PPO_1330/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1330/train/entropy_loss ▁▂▂▃▃▄▅▆▆▇█
wandb:   PPO_1330/train/explained_variance ▆▅█▄▂▅▁▇▄█▁
wandb:        PPO_1330/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1330/train/loss ▁▄▃▂▂▂▃▂▁▃█
wandb: PPO_1330/train/policy_gradient_loss ▁▂▇▃▃▃▄▅▁█▄
wandb:                  PPO_1330/train/std █▇▇▆▆▅▄▃▃▂▁
wandb:           PPO_1330/train/value_loss ▁▃▅▂▅▆▁▄▇▃█
wandb:                PPO_1341/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1341/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1341/rollout/ep_rew_mean ▅▄▃▃▃▁▅▄▄▄▃█
wandb:                   PPO_1341/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1341/train/approx_kl █▄▁▅▅▅▂▁▁▂▆
wandb:        PPO_1341/train/clip_fraction ▄█▁▆▃▄▄▂▂▄▄
wandb:           PPO_1341/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1341/train/entropy_loss ▁▂▂▂▃▄▄▄▅▆█
wandb:   PPO_1341/train/explained_variance ▅▆▁▄▄▇▆▆▇██
wandb:        PPO_1341/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1341/train/loss ▃▂▁▅▃█▁▄█▃▃
wandb: PPO_1341/train/policy_gradient_loss ▃█▇▁▁▃▆▄▁▂▃
wandb:                  PPO_1341/train/std ██▇▆▆▅▅▄▃▃▁
wandb:           PPO_1341/train/value_loss ▅▆▇▁▅▄▇▄▄▄█
wandb:                PPO_1351/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1351/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1351/rollout/ep_rew_mean ▅▁▂▄▆▇▃▇▆▆█▃
wandb:                   PPO_1351/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1351/train/approx_kl ▇▇▂▆▂▄▂█▁▄▆
wandb:        PPO_1351/train/clip_fraction ▃▁▄▅▃▃▄█▂▆▆
wandb:           PPO_1351/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1351/train/entropy_loss ▁▁▂▂▃▃▄▅▆▆█
wandb:   PPO_1351/train/explained_variance ▂▁▅▅▂▅▄▁▃▆█
wandb:        PPO_1351/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1351/train/loss ▃█▆▃▄▁█▃▃▄▅
wandb: PPO_1351/train/policy_gradient_loss ▄▁▃▂▃█▁▅▁▅▂
wandb:                  PPO_1351/train/std ▇█▇▆▆▆▅▄▃▃▁
wandb:           PPO_1351/train/value_loss ▁▅▅▄▅▃▇▂█▂▁
wandb:                PPO_1360/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1360/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1360/rollout/ep_rew_mean ▆▄▃▂▂▂▅▄▂▇█▁
wandb:                   PPO_1360/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1360/train/approx_kl ▃▄█▅▄▆▂▂▃▁█
wandb:        PPO_1360/train/clip_fraction ▄▆█▁▄▆▃▅▄▂█
wandb:           PPO_1360/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1360/train/entropy_loss ▁▁▃▃▃▄▄▆▆▇█
wandb:   PPO_1360/train/explained_variance ▁▄▆▄▃▆▇▄█▃▆
wandb:        PPO_1360/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1360/train/loss ▁▂▄▆▅▄█▅▃▄▆
wandb: PPO_1360/train/policy_gradient_loss ▆█▆▁▆▅▅▅▇▃▄
wandb:                  PPO_1360/train/std █▇▆▆▆▅▅▃▃▂▁
wandb:           PPO_1360/train/value_loss ▂▃▁█▅▄▅▇▄▅▆
wandb:                PPO_1370/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1370/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1370/rollout/ep_rew_mean ▆▅▆▆▆▆▆▅▁▃▅█
wandb:                   PPO_1370/time/fps █▃▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1370/train/approx_kl ▂▂▄▄▂▁▆▄▇█▃
wandb:        PPO_1370/train/clip_fraction ▂▃▆▅▄▁█▇▅▆▅
wandb:           PPO_1370/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1370/train/entropy_loss ▁▁▁▂▃▃▃▄▃▄█
wandb:   PPO_1370/train/explained_variance ▅▆▃▇▇▃▁▄█▄▆
wandb:        PPO_1370/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1370/train/loss ▂█▂▁▅▄▃▂▂▂▆
wandb: PPO_1370/train/policy_gradient_loss ▁▄▆▆▄▅▆▄▆▇█
wandb:                  PPO_1370/train/std ██▇▇▆▆▅▆▇▄▁
wandb:           PPO_1370/train/value_loss ▃▅▃▃▁█▇▄▁▅▆
wandb:                PPO_1380/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1380/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1380/rollout/ep_rew_mean ▁▃▄▇▅▁▆▆▂▅▁█
wandb:                   PPO_1380/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1380/train/approx_kl ▃▁▆▁█▃▆▆▇▂▄
wandb:        PPO_1380/train/clip_fraction ▁▃▆▄█▃█▆▃▄▅
wandb:           PPO_1380/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1380/train/entropy_loss ▁▁▂▄▄▅▆▆▇██
wandb:   PPO_1380/train/explained_variance ▄▅▇▂▃▁▁▇▅█▅
wandb:        PPO_1380/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1380/train/loss █▄▄▄▄▇▂▄▃▁▂
wandb: PPO_1380/train/policy_gradient_loss ▃▄▅▄█▃▆▄▅▁▅
wandb:                  PPO_1380/train/std ██▇▅▅▄▃▂▂▁▁
wandb:           PPO_1380/train/value_loss ▇▅▅█▄█▃▁▃▃▅
wandb:                    global_mean_eval ▁▅▆▇██████
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▅▆▆█▇████
wandb:                       mean_reward_1 ▁▄▆▆███▇██
wandb:                      mean_reward_10 ▁▅▆▆█▇████
wandb:                      mean_reward_11 ▁▅▆▆███▇▇█
wandb:                      mean_reward_12 ▁▄▆▆███▇▇█
wandb:                      mean_reward_13 ▁▅▆▇▇▇█▇▇█
wandb:                      mean_reward_14 ▁▅▆▆█████▇
wandb:                      mean_reward_15 ▁▅▆▆▇█████
wandb:                      mean_reward_16 ▁▅▆▇▇█████
wandb:                      mean_reward_17 ▁▅▆▆▇▇██▇█
wandb:                      mean_reward_18 ▁▄▆▆▇▇█▇▇█
wandb:                      mean_reward_19 ▁▅▆▇██████
wandb:                       mean_reward_2 ▁▄▆▆▇▇▇▇▇█
wandb:                      mean_reward_20 ▁▄▅▆▇▇██▇█
wandb:                      mean_reward_21 ▁▅▆▆▇█████
wandb:                      mean_reward_22 ▁▅▆▇██████
wandb:                      mean_reward_23 ▁▅▆▇████▇█
wandb:                      mean_reward_24 ▁▅▆▆███▇▇█
wandb:                      mean_reward_25 ▁▅▆▇██████
wandb:                      mean_reward_26 ▁▅▆▇██████
wandb:                      mean_reward_27 ▁▅▆▆▇▇██▇█
wandb:                      mean_reward_28 ▁▄▆▆████▇█
wandb:                      mean_reward_29 ▁▅▆▆▇▇▇█▇█
wandb:                       mean_reward_3 ▁▅▆▇██████
wandb:                      mean_reward_30 ▁▅▆▆███▇██
wandb:                      mean_reward_31 ▁▅▆▆▇███▇█
wandb:                      mean_reward_32 ▁▅▆▆█▇█▇▇█
wandb:                      mean_reward_33 ▁▅▆▆▇▇▇███
wandb:                      mean_reward_34 ▁▅▆▇██████
wandb:                      mean_reward_35 ▁▄▆▆▇██▇██
wandb:                       mean_reward_4 ▁▄▆▆▇▇██▇█
wandb:                       mean_reward_5 ▁▄▅▆▇▇▇██▇
wandb:                       mean_reward_6 ▁▅▆▆▇▇████
wandb:                       mean_reward_7 ▁▅▆▇▇█████
wandb:                       mean_reward_8 ▁▅▆▇████▇█
wandb:                       mean_reward_9 ▁▄▆▆█▇█▇▇█
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▂▄▄▆▇▇█
wandb:                        std_reward_0 ▃▁▁▂▃▅▄█▆▇
wandb:                        std_reward_1 ▃▁▁▂▃▄▄█▅▇
wandb:                       std_reward_10 ▄▁▁▃▃▆▄█▇█
wandb:                       std_reward_11 ▃▁▁▂▃▄▃█▇▇
wandb:                       std_reward_12 ▃▁▁▂▃▅▄█▇▇
wandb:                       std_reward_13 ▃▁▁▃▃▅▄█▇▆
wandb:                       std_reward_14 ▃▁▁▂▃▄▄▇▆█
wandb:                       std_reward_15 ▃▁▁▂▃▅▄█▆█
wandb:                       std_reward_16 ▃▁▂▂▃▅▄███
wandb:                       std_reward_17 ▃▁▁▂▃▅▅██▇
wandb:                       std_reward_18 ▃▁▁▂▂▄▄█▇▅
wandb:                       std_reward_19 ▃▁▁▂▃▅▃█▆▇
wandb:                        std_reward_2 ▃▁▂▂▃▄▄█▇▆
wandb:                       std_reward_20 ▃▁▁▂▃▆▄█▇▇
wandb:                       std_reward_21 ▄▁▂▃▃▆▄██▇
wandb:                       std_reward_22 ▃▁▁▃▃▅▄█▇▇
wandb:                       std_reward_23 ▃▁▁▂▃▅▄██▇
wandb:                       std_reward_24 ▃▁▁▃▂▄▄█▇▆
wandb:                       std_reward_25 ▃▁▁▂▃▄▄█▇█
wandb:                       std_reward_26 ▃▁▁▃▃▄▄█▆▇
wandb:                       std_reward_27 ▃▁▁▃▃▆▄██▇
wandb:                       std_reward_28 ▃▁▁▃▃▄▃█▇▇
wandb:                       std_reward_29 ▃▁▂▂▃▅▄█▇▇
wandb:                        std_reward_3 ▃▁▁▂▃▅▄█▆█
wandb:                       std_reward_30 ▃▁▁▂▂▄▃█▇▇
wandb:                       std_reward_31 ▃▁▁▂▃▆▄██▇
wandb:                       std_reward_32 ▃▁▁▃▃▅▄██▇
wandb:                       std_reward_33 ▃▁▁▃▄▆▄█▆█
wandb:                       std_reward_34 ▄▁▂▃▃▆▄█▇█
wandb:                       std_reward_35 ▃▁▁▂▃▄▄█▆▇
wandb:                        std_reward_4 ▃▁▁▃▃▅▄█▇▇
wandb:                        std_reward_5 ▄▁▂▃▃▅▅▇▆█
wandb:                        std_reward_6 ▃▁▁▂▃▇▄▇▇█
wandb:                        std_reward_7 ▃▁▁▂▃▅▃█▆█
wandb:                        std_reward_8 ▃▁▂▃▃▅▄███
wandb:                        std_reward_9 ▃▁▁▂▃▅▄█▇█
wandb:                            time/fps █▄▃▂▂▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▁▂▃█▆▂▁▃▃▅▄▄
wandb:                 train/clip_fraction ▄▄▄▅▅▂▁▄▇███
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▁▂▆▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▄▂▂▂▂▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▅▅▅██▆▄▂▁▁▂
wandb:                           train/std █▇▇▆▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1299/global_step 212992
wandb:        PPO_1299/rollout/ep_len_mean 200.0
wandb:        PPO_1299/rollout/ep_rew_mean -816.10785
wandb:                   PPO_1299/time/fps 605.0
wandb:            PPO_1299/train/approx_kl 0.01152
wandb:        PPO_1299/train/clip_fraction 0.14532
wandb:           PPO_1299/train/clip_range 0.2
wandb:         PPO_1299/train/entropy_loss -7.74537
wandb:   PPO_1299/train/explained_variance 0.95942
wandb:        PPO_1299/train/learning_rate 0.0003
wandb:                 PPO_1299/train/loss 26.27252
wandb: PPO_1299/train/policy_gradient_loss -0.00723
wandb:                  PPO_1299/train/std 0.73185
wandb:           PPO_1299/train/value_loss 104.42182
wandb:                PPO_1310/global_step 212992
wandb:        PPO_1310/rollout/ep_len_mean 200.0
wandb:        PPO_1310/rollout/ep_rew_mean -680.54364
wandb:                   PPO_1310/time/fps 601.0
wandb:            PPO_1310/train/approx_kl 0.01628
wandb:        PPO_1310/train/clip_fraction 0.22734
wandb:           PPO_1310/train/clip_range 0.2
wandb:         PPO_1310/train/entropy_loss -6.83583
wandb:   PPO_1310/train/explained_variance 0.97101
wandb:        PPO_1310/train/learning_rate 0.0003
wandb:                 PPO_1310/train/loss 22.12281
wandb: PPO_1310/train/policy_gradient_loss -0.00618
wandb:                  PPO_1310/train/std 0.64236
wandb:           PPO_1310/train/value_loss 46.54818
wandb:                PPO_1320/global_step 212992
wandb:        PPO_1320/rollout/ep_len_mean 200.0
wandb:        PPO_1320/rollout/ep_rew_mean -594.95007
wandb:                   PPO_1320/time/fps 599.0
wandb:            PPO_1320/train/approx_kl 0.01647
wandb:        PPO_1320/train/clip_fraction 0.2158
wandb:           PPO_1320/train/clip_range 0.2
wandb:         PPO_1320/train/entropy_loss -5.9362
wandb:   PPO_1320/train/explained_variance 0.98002
wandb:        PPO_1320/train/learning_rate 0.0003
wandb:                 PPO_1320/train/loss 8.30973
wandb: PPO_1320/train/policy_gradient_loss -0.00115
wandb:                  PPO_1320/train/std 0.56422
wandb:           PPO_1320/train/value_loss 37.94209
wandb:                PPO_1330/global_step 212992
wandb:        PPO_1330/rollout/ep_len_mean 200.0
wandb:        PPO_1330/rollout/ep_rew_mean -563.10492
wandb:                   PPO_1330/time/fps 601.0
wandb:            PPO_1330/train/approx_kl 0.01572
wandb:        PPO_1330/train/clip_fraction 0.20475
wandb:           PPO_1330/train/clip_range 0.2
wandb:         PPO_1330/train/entropy_loss -5.29397
wandb:   PPO_1330/train/explained_variance 0.96393
wandb:        PPO_1330/train/learning_rate 0.0003
wandb:                 PPO_1330/train/loss 66.08321
wandb: PPO_1330/train/policy_gradient_loss -0.00238
wandb:                  PPO_1330/train/std 0.51643
wandb:           PPO_1330/train/value_loss 71.11455
wandb:                PPO_1341/global_step 212992
wandb:        PPO_1341/rollout/ep_len_mean 200.0
wandb:        PPO_1341/rollout/ep_rew_mean -512.79059
wandb:                   PPO_1341/time/fps 599.0
wandb:            PPO_1341/train/approx_kl 0.01851
wandb:        PPO_1341/train/clip_fraction 0.22601
wandb:           PPO_1341/train/clip_range 0.2
wandb:         PPO_1341/train/entropy_loss -4.78268
wandb:   PPO_1341/train/explained_variance 0.97956
wandb:        PPO_1341/train/learning_rate 0.0003
wandb:                 PPO_1341/train/loss 29.97879
wandb: PPO_1341/train/policy_gradient_loss -0.00188
wandb:                  PPO_1341/train/std 0.47902
wandb:           PPO_1341/train/value_loss 82.59895
wandb:                PPO_1351/global_step 212992
wandb:        PPO_1351/rollout/ep_len_mean 200.0
wandb:        PPO_1351/rollout/ep_rew_mean -540.96838
wandb:                   PPO_1351/time/fps 601.0
wandb:            PPO_1351/train/approx_kl 0.0173
wandb:        PPO_1351/train/clip_fraction 0.23205
wandb:           PPO_1351/train/clip_range 0.2
wandb:         PPO_1351/train/entropy_loss -4.39432
wandb:   PPO_1351/train/explained_variance 0.98789
wandb:        PPO_1351/train/learning_rate 0.0003
wandb:                 PPO_1351/train/loss 44.4884
wandb: PPO_1351/train/policy_gradient_loss -0.00265
wandb:                  PPO_1351/train/std 0.45321
wandb:           PPO_1351/train/value_loss 71.21587
wandb:                PPO_1360/global_step 212992
wandb:        PPO_1360/rollout/ep_len_mean 200.0
wandb:        PPO_1360/rollout/ep_rew_mean -528.23511
wandb:                   PPO_1360/time/fps 598.0
wandb:            PPO_1360/train/approx_kl 0.02029
wandb:        PPO_1360/train/clip_fraction 0.2513
wandb:           PPO_1360/train/clip_range 0.2
wandb:         PPO_1360/train/entropy_loss -3.97252
wandb:   PPO_1360/train/explained_variance 0.98611
wandb:        PPO_1360/train/learning_rate 0.0003
wandb:                 PPO_1360/train/loss 49.81297
wandb: PPO_1360/train/policy_gradient_loss -0.0015
wandb:                  PPO_1360/train/std 0.42787
wandb:           PPO_1360/train/value_loss 95.94772
wandb:                PPO_1370/global_step 212992
wandb:        PPO_1370/rollout/ep_len_mean 200.0
wandb:        PPO_1370/rollout/ep_rew_mean -492.89557
wandb:                   PPO_1370/time/fps 599.0
wandb:            PPO_1370/train/approx_kl 0.01753
wandb:        PPO_1370/train/clip_fraction 0.2366
wandb:           PPO_1370/train/clip_range 0.2
wandb:         PPO_1370/train/entropy_loss -3.71181
wandb:   PPO_1370/train/explained_variance 0.98843
wandb:        PPO_1370/train/learning_rate 0.0003
wandb:                 PPO_1370/train/loss 80.57842
wandb: PPO_1370/train/policy_gradient_loss 0.00111
wandb:                  PPO_1370/train/std 0.41132
wandb:           PPO_1370/train/value_loss 147.70277
wandb:                PPO_1380/global_step 212992
wandb:        PPO_1380/rollout/ep_len_mean 200.0
wandb:        PPO_1380/rollout/ep_rew_mean -482.48633
wandb:                   PPO_1380/time/fps 596.0
wandb:            PPO_1380/train/approx_kl 0.02061
wandb:        PPO_1380/train/clip_fraction 0.25441
wandb:           PPO_1380/train/clip_range 0.2
wandb:         PPO_1380/train/entropy_loss -3.35942
wandb:   PPO_1380/train/explained_variance 0.98871
wandb:        PPO_1380/train/learning_rate 0.0003
wandb:                 PPO_1380/train/loss 35.15711
wandb: PPO_1380/train/policy_gradient_loss 0.00143
wandb:                  PPO_1380/train/std 0.39148
wandb:           PPO_1380/train/value_loss 116.11802
wandb:                    global_mean_eval -464.28211
wandb:                         global_step 212992
wandb:                       mean_reward_0 -460.80676
wandb:                       mean_reward_1 -461.12579
wandb:                      mean_reward_10 -465.05369
wandb:                      mean_reward_11 -456.77135
wandb:                      mean_reward_12 -476.54181
wandb:                      mean_reward_13 -450.8529
wandb:                      mean_reward_14 -489.85206
wandb:                      mean_reward_15 -460.63691
wandb:                      mean_reward_16 -469.58486
wandb:                      mean_reward_17 -465.87663
wandb:                      mean_reward_18 -445.5069
wandb:                      mean_reward_19 -474.94796
wandb:                       mean_reward_2 -444.96009
wandb:                      mean_reward_20 -448.07496
wandb:                      mean_reward_21 -457.49113
wandb:                      mean_reward_22 -458.86729
wandb:                      mean_reward_23 -480.83348
wandb:                      mean_reward_24 -464.34645
wandb:                      mean_reward_25 -475.13636
wandb:                      mean_reward_26 -461.86032
wandb:                      mean_reward_27 -448.89439
wandb:                      mean_reward_28 -449.94301
wandb:                      mean_reward_29 -453.9734
wandb:                       mean_reward_3 -491.07002
wandb:                      mean_reward_30 -472.49038
wandb:                      mean_reward_31 -453.51761
wandb:                      mean_reward_32 -452.37608
wandb:                      mean_reward_33 -472.60163
wandb:                      mean_reward_34 -485.7428
wandb:                      mean_reward_35 -448.83999
wandb:                       mean_reward_4 -452.46611
wandb:                       mean_reward_5 -479.88679
wandb:                       mean_reward_6 -479.83903
wandb:                       mean_reward_7 -469.27658
wandb:                       mean_reward_8 -473.25627
wandb:                       mean_reward_9 -460.85434
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -901.89819
wandb:                        std_reward_0 159.53119
wandb:                        std_reward_1 166.317
wandb:                       std_reward_10 153.81947
wandb:                       std_reward_11 154.90922
wandb:                       std_reward_12 164.37992
wandb:                       std_reward_13 142.59854
wandb:                       std_reward_14 178.98849
wandb:                       std_reward_15 161.97997
wandb:                       std_reward_16 158.02886
wandb:                       std_reward_17 140.80068
wandb:                       std_reward_18 130.13637
wandb:                       std_reward_19 169.21654
wandb:                        std_reward_2 147.5925
wandb:                       std_reward_20 146.42237
wandb:                       std_reward_21 142.84923
wandb:                       std_reward_22 150.58497
wandb:                       std_reward_23 155.59811
wandb:                       std_reward_24 153.05012
wandb:                       std_reward_25 162.11926
wandb:                       std_reward_26 159.20288
wandb:                       std_reward_27 150.72958
wandb:                       std_reward_28 154.99191
wandb:                       std_reward_29 151.97536
wandb:                        std_reward_3 168.29096
wandb:                       std_reward_30 173.93184
wandb:                       std_reward_31 154.58948
wandb:                       std_reward_32 150.74663
wandb:                       std_reward_33 171.37473
wandb:                       std_reward_34 157.08061
wandb:                       std_reward_35 155.64454
wandb:                        std_reward_4 157.66341
wandb:                        std_reward_5 165.05926
wandb:                        std_reward_6 164.45607
wandb:                        std_reward_7 169.36916
wandb:                        std_reward_8 157.92744
wandb:                        std_reward_9 170.54831
wandb:                            time/fps 597.0
wandb:                     train/approx_kl 0.0106
wandb:                 train/clip_fraction 0.13875
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.80915
wandb:            train/explained_variance 0.95603
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 9.91018
wandb:          train/policy_gradient_loss -0.00981
wandb:                           train/std 0.84977
wandb:                    train/value_loss 25.25248
wandb: 
wandb: Synced lyric-morning-48: https://wandb.ai/tidiane/meta_rl_context/runs/1gp8uu9i
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 13 other file(s)
wandb: Find logs at: ./wandb/run-20230626_052650-1gp8uu9i/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1298/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1298/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1298/rollout/ep_rew_mean ▁▃▃▄▄▅▅▆▆▆▇█
wandb:                   PPO_1298/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1298/train/approx_kl ▃▁▅▄▆▅▆▄▆█▆
wandb:        PPO_1298/train/clip_fraction ▃▁▅▄▅▄▆▅▅█▆
wandb:           PPO_1298/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1298/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1298/train/explained_variance ▁█▆▇▄▃▅▇▅▄▁
wandb:        PPO_1298/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1298/train/loss ▅▄▂▃▃█▅▁▄▁▂
wandb: PPO_1298/train/policy_gradient_loss ▅▇▃▅▄▅▄▆▆▁█
wandb:                  PPO_1298/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1298/train/value_loss ▁▄▂▄▄█▃▃▄▂▄
wandb:                PPO_1309/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1309/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1309/rollout/ep_rew_mean ▁▁▁▁▂▃▃▄▅▅▅█
wandb:                   PPO_1309/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1309/train/approx_kl ▄▃▃▂▁▄▂█▅▅▇
wandb:        PPO_1309/train/clip_fraction ▂▁▂▃▃▄▃▆▃▃█
wandb:           PPO_1309/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1309/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1309/train/explained_variance ▁▄▂▅▃▄▄▆▃▇█
wandb:        PPO_1309/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1309/train/loss ▅▃██▂▄▇▇▁▄▂
wandb: PPO_1309/train/policy_gradient_loss ▃▃▁▄█▅█▇█▆█
wandb:                  PPO_1309/train/std █▇▇▆▅▅▄▃▃▂▁
wandb:           PPO_1309/train/value_loss ▇██▇▃▃▅▅▂▃▁
wandb:                PPO_1319/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1319/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1319/rollout/ep_rew_mean ▁▂▂▃▄▃▅▆▇▆▇█
wandb:                   PPO_1319/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1319/train/approx_kl ▄▃▃▄▂▅▁▃▁█▃
wandb:        PPO_1319/train/clip_fraction ▆▄█▇▂▆▁█▃▇▃
wandb:           PPO_1319/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1319/train/entropy_loss ▁▂▂▃▄▄▅▆▆▇█
wandb:   PPO_1319/train/explained_variance ▄▅██▃▅▅▇▁▂▁
wandb:        PPO_1319/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1319/train/loss ▂▄▆▃▆▂▅█▁▇▅
wandb: PPO_1319/train/policy_gradient_loss ▁▂▁▃▅▃▅█▄▅▄
wandb:                  PPO_1319/train/std █▇▇▆▆▅▄▃▃▂▁
wandb:           PPO_1319/train/value_loss ▅▁▁▂▄▅█▃▄█▇
wandb:                PPO_1329/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1329/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1329/rollout/ep_rew_mean ▁▄▃▅▆█▄▂▆█▄▄
wandb:                   PPO_1329/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1329/train/approx_kl ▂▅▃▅▃█▃▁▃▂▃
wandb:        PPO_1329/train/clip_fraction ▂█▁█▄▃▃▁▂▄▄
wandb:           PPO_1329/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1329/train/entropy_loss ▁▂▂▃▄▅▆▆▇▇█
wandb:   PPO_1329/train/explained_variance █▇▁▅▇██▇▇▅▇
wandb:        PPO_1329/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1329/train/loss ▅▁█▁▃▂▂▃▃▂▂
wandb: PPO_1329/train/policy_gradient_loss ▁▄█▆▁▅▄▅▄▄▅
wandb:                  PPO_1329/train/std █▇▇▅▄▄▃▃▂▂▁
wandb:           PPO_1329/train/value_loss ▄▁▅▂▄▃▆▆█▆▇
wandb:                PPO_1339/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1339/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1339/rollout/ep_rew_mean ▃▃▅▅▅▅▃▁▆█▂▅
wandb:                   PPO_1339/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1339/train/approx_kl ▅▄▆█▇▄▄▁▅▆▆
wandb:        PPO_1339/train/clip_fraction ▁█▅█▄▁▆▅▄▇▅
wandb:           PPO_1339/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1339/train/entropy_loss ▁▂▃▄▄▅▆▆▇▇█
wandb:   PPO_1339/train/explained_variance ▄▁▅▆▆▇▅▇▂▄█
wandb:        PPO_1339/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1339/train/loss ▅▁▃▁▂▇▅▄█▂▂
wandb: PPO_1339/train/policy_gradient_loss ▂▁█▆▂▃▇▆▅▃▃
wandb:                  PPO_1339/train/std █▇▆▆▅▄▃▃▂▂▁
wandb:           PPO_1339/train/value_loss ▄▁▄▂▃▆▇▆█▇▄
wandb:                PPO_1350/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1350/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1350/rollout/ep_rew_mean █▄▆▇▅▅▁▅█▄▁█
wandb:                   PPO_1350/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1350/train/approx_kl ▇▃▆▇▆▄▆▂█▅▁
wandb:        PPO_1350/train/clip_fraction ▅▂█▃▄▄▁▃▅▁▂
wandb:           PPO_1350/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1350/train/entropy_loss ▁▂▃▃▄▄▅▅▆▇█
wandb:   PPO_1350/train/explained_variance ▇▆█▅▁▆▇▃▆▆▇
wandb:        PPO_1350/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1350/train/loss ▄▃▂█▃▁▂▄▂▄▂
wandb: PPO_1350/train/policy_gradient_loss ▂▂▅▄▁▁▁█▇▃▆
wandb:                  PPO_1350/train/std ██▇▆▆▅▅▄▄▂▁
wandb:           PPO_1350/train/value_loss ▁▂▁▄▅▂▅▅▃█▆
wandb:                PPO_1361/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1361/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1361/rollout/ep_rew_mean ▅▆█▆▅▂▅██▇▁█
wandb:                   PPO_1361/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1361/train/approx_kl ▃▂▂▂▁█▁▃▄▁▂
wandb:        PPO_1361/train/clip_fraction ▃▄▇▄▂▇▃█▆▁▅
wandb:           PPO_1361/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1361/train/entropy_loss ▁▂▂▃▃▄▅▆▆▇█
wandb:   PPO_1361/train/explained_variance ▃▂▆▇▃█▆▄▆▁▆
wandb:        PPO_1361/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1361/train/loss ▃▁▁▃█▂▂▄▁▅▃
wandb: PPO_1361/train/policy_gradient_loss ▅▆▁▇▅▅▆█▆▃▇
wandb:                  PPO_1361/train/std █▇▇▆▆▅▄▄▃▂▁
wandb:           PPO_1361/train/value_loss ▃▄▁▂▅▁▃▂▃█▅
wandb:                PPO_1371/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1371/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1371/rollout/ep_rew_mean ▅▅█▄▄▅▁▁▁▁▃▃
wandb:                   PPO_1371/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1371/train/approx_kl ▅▃▅▂▂█▄▄▂▃▁
wandb:        PPO_1371/train/clip_fraction ███▃▅▇▇▆▁▆▂
wandb:           PPO_1371/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1371/train/entropy_loss ▁▂▂▃▄▅▅▅▆▇█
wandb:   PPO_1371/train/explained_variance ▃▅▅▄▁▄▇█▄▃▄
wandb:        PPO_1371/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1371/train/loss ▁▂▄▆▁▃▃▃▄█▃
wandb: PPO_1371/train/policy_gradient_loss █▅▆▄▁▆▅▆▂▃▂
wandb:                  PPO_1371/train/std ██▇▆▅▅▄▅▃▃▁
wandb:           PPO_1371/train/value_loss ▁▂▁▄▄▅▃▃█▇█
wandb:                PPO_1381/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1381/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1381/rollout/ep_rew_mean █▆▅▄▂▂▄▄█▂▂▁
wandb:                   PPO_1381/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1381/train/approx_kl ▂▂█▃▁▂▅▅▂▂▂
wandb:        PPO_1381/train/clip_fraction █▃▄▁▂▃▄▅▄▃▁
wandb:           PPO_1381/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1381/train/entropy_loss ▁▂▃▄▅▅▆▇▇▇█
wandb:   PPO_1381/train/explained_variance ▅▅▆▅▁▇▄▄▆█▇
wandb:        PPO_1381/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1381/train/loss ▁▂▁▆▄▄█▂▁▄▃
wandb: PPO_1381/train/policy_gradient_loss █▂▆▁▅▅▄▄▆▃▃
wandb:                  PPO_1381/train/std █▇▆▅▄▄▃▂▂▂▁
wandb:           PPO_1381/train/value_loss ▂▅▃▆█▄▆█▁▁▆
wandb:                    global_mean_eval ▁▄▆███▇▇▆▇
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▄▆███▇▇▄█
wandb:                       mean_reward_1 ▁▃▆▇▇█▆█▄▇
wandb:                      mean_reward_10 ▁▃▆███▇▇▇▆
wandb:                      mean_reward_11 ▁▄▆▇▇█▇▇▇▇
wandb:                      mean_reward_12 ▁▃▆█▇▇▆█▇▇
wandb:                      mean_reward_13 ▁▃▆▇██▇▇▆█
wandb:                      mean_reward_14 ▁▄▆▇█▇█▇▆█
wandb:                      mean_reward_15 ▁▄▆█▇█▇▇▇█
wandb:                      mean_reward_16 ▁▄▆█▇█▆▇▇█
wandb:                      mean_reward_17 ▁▄▆▇▇▇█▇▆█
wandb:                      mean_reward_18 ▁▄▆▇▇██▇▆▇
wandb:                      mean_reward_19 ▁▃▆▇██▇▇▅▆
wandb:                       mean_reward_2 ▁▄▆▇██▇▇█▇
wandb:                      mean_reward_20 ▁▃▆▇██▇█▇▇
wandb:                      mean_reward_21 ▁▃▆▇██▇█▆▆
wandb:                      mean_reward_22 ▁▃▅▇█▇▆▆▇▆
wandb:                      mean_reward_23 ▁▄▆███▇▇▇▇
wandb:                      mean_reward_24 ▁▃▆▇██▇▇▇█
wandb:                      mean_reward_25 ▁▄▆███▇▇▇▇
wandb:                      mean_reward_26 ▁▃▆▇▇█▆▆▄▇
wandb:                      mean_reward_27 ▁▄▆███▇▇▅█
wandb:                      mean_reward_28 ▁▃▆▇▇▇▆▇▇█
wandb:                      mean_reward_29 ▁▄▆██▇▇▇▇█
wandb:                       mean_reward_3 ▁▃▅▇█▇██▅▆
wandb:                      mean_reward_30 ▁▃▆██▇▆▇▆█
wandb:                      mean_reward_31 ▁▄▆█▇█▇█▆▇
wandb:                      mean_reward_32 ▁▃▅▇▇██▇▄▇
wandb:                      mean_reward_33 ▁▄▆▇████▇█
wandb:                      mean_reward_34 ▁▄▆███▇▇▇█
wandb:                      mean_reward_35 ▁▃▆▇▇█▆█▅▇
wandb:                       mean_reward_4 ▁▄▆▇▇█▇▇▇█
wandb:                       mean_reward_5 ▁▃▆██▇▇▆▅▅
wandb:                       mean_reward_6 ▁▄▆▇▇███▆▇
wandb:                       mean_reward_7 ▁▃▆███▆▆▇▇
wandb:                       mean_reward_8 ▁▄▆█▇█▇█▆▇
wandb:                       mean_reward_9 ▁▄▆██▇▆▇▆▇
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▂▂▂▃▅▃▆▇▇▇█▆
wandb:                        std_reward_0 ▂▁▁▁▃▄▅▆█▆
wandb:                        std_reward_1 ▃▁▁▂▄▄▅▅█▆
wandb:                       std_reward_10 ▃▁▁▂▄▅▇▆██
wandb:                       std_reward_11 ▃▁▁▁▄▅▆▇██
wandb:                       std_reward_12 ▃▁▁▂▄▅▇▇██
wandb:                       std_reward_13 ▂▁▁▂▃▄▅▆█▇
wandb:                       std_reward_14 ▃▁▁▂▃▅▅▆█▆
wandb:                       std_reward_15 ▃▁▁▂▄▅▆▇██
wandb:                       std_reward_16 ▃▁▁▂▄▅▇▇█▇
wandb:                       std_reward_17 ▃▁▁▂▃▅▅▆█▇
wandb:                       std_reward_18 ▃▁▁▁▃▄▆▆█▇
wandb:                       std_reward_19 ▃▁▁▂▃▄▆▆█▇
wandb:                        std_reward_2 ▃▁▁▂▄▅▆▇▇█
wandb:                       std_reward_20 ▃▁▁▂▃▅▆▆▇█
wandb:                       std_reward_21 ▃▁▁▂▄▄▆▅██
wandb:                       std_reward_22 ▃▁▁▂▃▄▇▇▇█
wandb:                       std_reward_23 ▂▁▁▁▄▅▇▇██
wandb:                       std_reward_24 ▃▁▁▂▄▅▇▆█▇
wandb:                       std_reward_25 ▃▁▁▂▃▅▆▇█▇
wandb:                       std_reward_26 ▃▁▁▂▃▄▆▆█▆
wandb:                       std_reward_27 ▃▁▁▂▃▄▆▆█▆
wandb:                       std_reward_28 ▃▁▁▂▄▅▇▆█▇
wandb:                       std_reward_29 ▃▁▁▂▃▅▇▇█▇
wandb:                        std_reward_3 ▂▁▁▁▃▄▅▅█▇
wandb:                       std_reward_30 ▃▁▁▁▃▄▆▆█▆
wandb:                       std_reward_31 ▃▁▁▁▄▅▆▆██
wandb:                       std_reward_32 ▃▁▁▁▃▄▄▆█▆
wandb:                       std_reward_33 ▃▁▁▂▄▅▇▇██
wandb:                       std_reward_34 ▃▁▁▂▃▄▆▆█▆
wandb:                       std_reward_35 ▃▁▁▂▄▄▆▅█▇
wandb:                        std_reward_4 ▃▁▁▂▅▅▇▇██
wandb:                        std_reward_5 ▂▁▁▂▄▄▅▆██
wandb:                        std_reward_6 ▃▁▁▂▄▄▅▆█▇
wandb:                        std_reward_7 ▃▁▁▂▃▅▇▇▇█
wandb:                        std_reward_8 ▃▁▁▂▄▅▆▆█▇
wandb:                        std_reward_9 ▃▁▁▂▃▅▆▆██
wandb:                            time/fps █▄▃▂▂▂▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▃▂▃█▇▂▁▃▃▄▄▅
wandb:                 train/clip_fraction ▅▄▄▅▅▂▁▅▆█▇▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▃▃▄▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▁▄▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▄▃▃▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▆▆██▇▃▃▁▃▃
wandb:                           train/std █▇▇▆▅▅▄▃▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1298/global_step 212992
wandb:        PPO_1298/rollout/ep_len_mean 200.0
wandb:        PPO_1298/rollout/ep_rew_mean -789.26703
wandb:                   PPO_1298/time/fps 598.0
wandb:            PPO_1298/train/approx_kl 0.012
wandb:        PPO_1298/train/clip_fraction 0.1514
wandb:           PPO_1298/train/clip_range 0.2
wandb:         PPO_1298/train/entropy_loss -7.75913
wandb:   PPO_1298/train/explained_variance 0.94998
wandb:        PPO_1298/train/learning_rate 0.0003
wandb:                 PPO_1298/train/loss 30.58978
wandb: PPO_1298/train/policy_gradient_loss -0.00627
wandb:                  PPO_1298/train/std 0.73354
wandb:           PPO_1298/train/value_loss 91.47829
wandb:                PPO_1309/global_step 212992
wandb:        PPO_1309/rollout/ep_len_mean 200.0
wandb:        PPO_1309/rollout/ep_rew_mean -649.00568
wandb:                   PPO_1309/time/fps 597.0
wandb:            PPO_1309/train/approx_kl 0.01503
wandb:        PPO_1309/train/clip_fraction 0.20412
wandb:           PPO_1309/train/clip_range 0.2
wandb:         PPO_1309/train/entropy_loss -6.84929
wandb:   PPO_1309/train/explained_variance 0.97387
wandb:        PPO_1309/train/learning_rate 0.0003
wandb:                 PPO_1309/train/loss 16.96134
wandb: PPO_1309/train/policy_gradient_loss -0.00501
wandb:                  PPO_1309/train/std 0.64469
wandb:           PPO_1309/train/value_loss 44.50845
wandb:                PPO_1319/global_step 212992
wandb:        PPO_1319/rollout/ep_len_mean 200.0
wandb:        PPO_1319/rollout/ep_rew_mean -560.24048
wandb:                   PPO_1319/time/fps 596.0
wandb:            PPO_1319/train/approx_kl 0.01604
wandb:        PPO_1319/train/clip_fraction 0.19731
wandb:           PPO_1319/train/clip_range 0.2
wandb:         PPO_1319/train/entropy_loss -5.96349
wandb:   PPO_1319/train/explained_variance 0.96896
wandb:        PPO_1319/train/learning_rate 0.0003
wandb:                 PPO_1319/train/loss 27.1461
wandb: PPO_1319/train/policy_gradient_loss -0.00414
wandb:                  PPO_1319/train/std 0.56806
wandb:           PPO_1319/train/value_loss 55.77895
wandb:                PPO_1329/global_step 212992
wandb:        PPO_1329/rollout/ep_len_mean 200.0
wandb:        PPO_1329/rollout/ep_rew_mean -543.48138
wandb:                   PPO_1329/time/fps 596.0
wandb:            PPO_1329/train/approx_kl 0.01561
wandb:        PPO_1329/train/clip_fraction 0.20561
wandb:           PPO_1329/train/clip_range 0.2
wandb:         PPO_1329/train/entropy_loss -5.45199
wandb:   PPO_1329/train/explained_variance 0.97
wandb:        PPO_1329/train/learning_rate 0.0003
wandb:                 PPO_1329/train/loss 23.32735
wandb: PPO_1329/train/policy_gradient_loss -0.00232
wandb:                  PPO_1329/train/std 0.52782
wandb:           PPO_1329/train/value_loss 64.57671
wandb:                PPO_1339/global_step 212992
wandb:        PPO_1339/rollout/ep_len_mean 200.0
wandb:        PPO_1339/rollout/ep_rew_mean -524.93781
wandb:                   PPO_1339/time/fps 596.0
wandb:            PPO_1339/train/approx_kl 0.01591
wandb:        PPO_1339/train/clip_fraction 0.20477
wandb:           PPO_1339/train/clip_range 0.2
wandb:         PPO_1339/train/entropy_loss -5.02296
wandb:   PPO_1339/train/explained_variance 0.98691
wandb:        PPO_1339/train/learning_rate 0.0003
wandb:                 PPO_1339/train/loss 16.10193
wandb: PPO_1339/train/policy_gradient_loss -0.0029
wandb:                  PPO_1339/train/std 0.4968
wandb:           PPO_1339/train/value_loss 74.32899
wandb:                PPO_1350/global_step 212992
wandb:        PPO_1350/rollout/ep_len_mean 200.0
wandb:        PPO_1350/rollout/ep_rew_mean -500.02206
wandb:                   PPO_1350/time/fps 595.0
wandb:            PPO_1350/train/approx_kl 0.01301
wandb:        PPO_1350/train/clip_fraction 0.19689
wandb:           PPO_1350/train/clip_range 0.2
wandb:         PPO_1350/train/entropy_loss -4.60574
wandb:   PPO_1350/train/explained_variance 0.98417
wandb:        PPO_1350/train/learning_rate 0.0003
wandb:                 PPO_1350/train/loss 35.27217
wandb: PPO_1350/train/policy_gradient_loss -0.00092
wandb:                  PPO_1350/train/std 0.46683
wandb:           PPO_1350/train/value_loss 155.52771
wandb:                PPO_1361/global_step 212992
wandb:        PPO_1361/rollout/ep_len_mean 200.0
wandb:        PPO_1361/rollout/ep_rew_mean -521.40625
wandb:                   PPO_1361/time/fps 595.0
wandb:            PPO_1361/train/approx_kl 0.0159
wandb:        PPO_1361/train/clip_fraction 0.21661
wandb:           PPO_1361/train/clip_range 0.2
wandb:         PPO_1361/train/entropy_loss -4.05229
wandb:   PPO_1361/train/explained_variance 0.98442
wandb:        PPO_1361/train/learning_rate 0.0003
wandb:                 PPO_1361/train/loss 106.86045
wandb: PPO_1361/train/policy_gradient_loss 0.00013
wandb:                  PPO_1361/train/std 0.43137
wandb:           PPO_1361/train/value_loss 239.77217
wandb:                PPO_1371/global_step 212992
wandb:        PPO_1371/rollout/ep_len_mean 200.0
wandb:        PPO_1371/rollout/ep_rew_mean -540.02673
wandb:                   PPO_1371/time/fps 594.0
wandb:            PPO_1371/train/approx_kl 0.01366
wandb:        PPO_1371/train/clip_fraction 0.19235
wandb:           PPO_1371/train/clip_range 0.2
wandb:         PPO_1371/train/entropy_loss -3.55667
wandb:   PPO_1371/train/explained_variance 0.98474
wandb:        PPO_1371/train/learning_rate 0.0003
wandb:                 PPO_1371/train/loss 78.27966
wandb: PPO_1371/train/policy_gradient_loss 0.00032
wandb:                  PPO_1371/train/std 0.40121
wandb:           PPO_1371/train/value_loss 503.16641
wandb:                PPO_1381/global_step 212992
wandb:        PPO_1381/rollout/ep_len_mean 200.0
wandb:        PPO_1381/rollout/ep_rew_mean -559.88934
wandb:                   PPO_1381/time/fps 593.0
wandb:            PPO_1381/train/approx_kl 0.01819
wandb:        PPO_1381/train/clip_fraction 0.22167
wandb:           PPO_1381/train/clip_range 0.2
wandb:         PPO_1381/train/entropy_loss -3.23325
wandb:   PPO_1381/train/explained_variance 0.98886
wandb:        PPO_1381/train/learning_rate 0.0003
wandb:                 PPO_1381/train/loss 123.14413
wandb: PPO_1381/train/policy_gradient_loss 0.00215
wandb:                  PPO_1381/train/std 0.38456
wandb:           PPO_1381/train/value_loss 376.5278
wandb:                    global_mean_eval -507.62096
wandb:                         global_step 212992
wandb:                       mean_reward_0 -499.84509
wandb:                       mean_reward_1 -494.29691
wandb:                      mean_reward_10 -545.75579
wandb:                      mean_reward_11 -516.13558
wandb:                      mean_reward_12 -526.31916
wandb:                      mean_reward_13 -485.00342
wandb:                      mean_reward_14 -486.66346
wandb:                      mean_reward_15 -474.91907
wandb:                      mean_reward_16 -476.54013
wandb:                      mean_reward_17 -477.06116
wandb:                      mean_reward_18 -505.72214
wandb:                      mean_reward_19 -534.49261
wandb:                       mean_reward_2 -508.94209
wandb:                      mean_reward_20 -525.46092
wandb:                      mean_reward_21 -539.07505
wandb:                      mean_reward_22 -549.48697
wandb:                      mean_reward_23 -518.43757
wandb:                      mean_reward_24 -497.82359
wandb:                      mean_reward_25 -511.77544
wandb:                      mean_reward_26 -509.94706
wandb:                      mean_reward_27 -486.69103
wandb:                      mean_reward_28 -476.60605
wandb:                      mean_reward_29 -485.41667
wandb:                       mean_reward_3 -534.80124
wandb:                      mean_reward_30 -484.91984
wandb:                      mean_reward_31 -526.51338
wandb:                      mean_reward_32 -486.67105
wandb:                      mean_reward_33 -497.92519
wandb:                      mean_reward_34 -481.39933
wandb:                      mean_reward_35 -511.88123
wandb:                       mean_reward_4 -487.77
wandb:                       mean_reward_5 -565.40151
wandb:                       mean_reward_6 -511.02496
wandb:                       mean_reward_7 -510.64272
wandb:                       mean_reward_8 -510.06075
wandb:                       mean_reward_9 -532.92651
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -939.39148
wandb:                        std_reward_0 238.69484
wandb:                        std_reward_1 227.70209
wandb:                       std_reward_10 261.35704
wandb:                       std_reward_11 243.93707
wandb:                       std_reward_12 250.47308
wandb:                       std_reward_13 226.07459
wandb:                       std_reward_14 220.90594
wandb:                       std_reward_15 232.93795
wandb:                       std_reward_16 229.53977
wandb:                       std_reward_17 216.84647
wandb:                       std_reward_18 240.49634
wandb:                       std_reward_19 266.03957
wandb:                        std_reward_2 245.88755
wandb:                       std_reward_20 261.89467
wandb:                       std_reward_21 262.17493
wandb:                       std_reward_22 260.0502
wandb:                       std_reward_23 243.1021
wandb:                       std_reward_24 233.4831
wandb:                       std_reward_25 244.40637
wandb:                       std_reward_26 239.73297
wandb:                       std_reward_27 234.06861
wandb:                       std_reward_28 210.2979
wandb:                       std_reward_29 231.9969
wandb:                        std_reward_3 253.80178
wandb:                       std_reward_30 216.10045
wandb:                       std_reward_31 251.32535
wandb:                       std_reward_32 230.16073
wandb:                       std_reward_33 242.60988
wandb:                       std_reward_34 213.60104
wandb:                       std_reward_35 250.87809
wandb:                        std_reward_4 223.49169
wandb:                        std_reward_5 276.38193
wandb:                        std_reward_6 249.95968
wandb:                        std_reward_7 254.53081
wandb:                        std_reward_8 244.64127
wandb:                        std_reward_9 258.64639
wandb:                            time/fps 600.0
wandb:                     train/approx_kl 0.01128
wandb:                 train/clip_fraction 0.12654
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7942
wandb:            train/explained_variance 0.92484
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.3112
wandb:          train/policy_gradient_loss -0.0089
wandb:                           train/std 0.84816
wandb:                    train/value_loss 56.39774
wandb: 
wandb: Synced zany-fire-50: https://wandb.ai/tidiane/meta_rl_context/runs/2uae6ict
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 14 other file(s)
wandb: Find logs at: ./wandb/run-20230626_052650-2uae6ict/logs
wandb: 
wandb: Run history:
wandb:                PPO_1302/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1302/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1302/rollout/ep_rew_mean ▂▂▁▂▄▅▄▅▇▆▇█
wandb:                   PPO_1302/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1302/train/approx_kl ▃▁▄▂▂▇▅▆▂██
wandb:        PPO_1302/train/clip_fraction ▂▅▄▃▁█▆█▆▇▇
wandb:           PPO_1302/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1302/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1302/train/explained_variance ▆▆▇██▆█▆▃▄▁
wandb:        PPO_1302/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1302/train/loss ▁▂▁▁▄▄▄▄█▄▃
wandb: PPO_1302/train/policy_gradient_loss ▃▃▃▇▇▁▂▅█▄▇
wandb:                  PPO_1302/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1302/train/value_loss ▁▁▂▂▇▃▄▅█▇▆
wandb:                PPO_1312/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1312/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1312/rollout/ep_rew_mean ▁▂▁▃▄▃▆███▇▇
wandb:                   PPO_1312/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1312/train/approx_kl ▃▄▄▄▃▄▇█▁▇▆
wandb:        PPO_1312/train/clip_fraction ▁▄▃▆▁▃▆▇▂█▇
wandb:           PPO_1312/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1312/train/entropy_loss ▁▂▂▃▃▄▅▆▆▇█
wandb:   PPO_1312/train/explained_variance ▁▅▇█▇▅▄▇▆▂▅
wandb:        PPO_1312/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1312/train/loss ▃▅▃▁█▂▂▆▄▂▁
wandb: PPO_1312/train/policy_gradient_loss ▃▁▂▃▆▃▃▄█▃▆
wandb:                  PPO_1312/train/std █▇▇▆▅▅▄▃▃▂▁
wandb:           PPO_1312/train/value_loss █▄▃▁▇▂▃▃▅▂▁
wandb:                PPO_1322/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1322/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1322/rollout/ep_rew_mean ▁▂▁▂▄▃▃▄▅▆▇█
wandb:                   PPO_1322/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1322/train/approx_kl ▄▂▃▁▆█▄▅▁▃█
wandb:        PPO_1322/train/clip_fraction ▄▁▃▁▂█▅▁▁▃▆
wandb:           PPO_1322/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1322/train/entropy_loss ▁▂▃▄▄▅▅▆▆▇█
wandb:   PPO_1322/train/explained_variance ▂█▇▅▂▁▇▄▄▅▂
wandb:        PPO_1322/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1322/train/loss ▅▃▃▃▆▂▁▃▂▃█
wandb: PPO_1322/train/policy_gradient_loss ▄▅█▅▃▆█▃█▃▁
wandb:                  PPO_1322/train/std █▇▆▅▅▄▄▃▃▂▁
wandb:           PPO_1322/train/value_loss ▄▄▃▇▇▃▁▄▄▅█
wandb:                PPO_1332/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1332/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1332/rollout/ep_rew_mean ▂▂▂▁▄▅▃▃██▆▄
wandb:                   PPO_1332/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1332/train/approx_kl ▃▅▁█▁▃▄▂▂▆▇
wandb:        PPO_1332/train/clip_fraction ▅▃▄▄▁▂▃▂▄█▃
wandb:           PPO_1332/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1332/train/entropy_loss ▁▂▃▃▄▄▅▅▆▇█
wandb:   PPO_1332/train/explained_variance ▄▆▆▂▃▆▂▁█▅▁
wandb:        PPO_1332/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1332/train/loss ▁▇▂▁▄▄▃▄▄▆█
wandb: PPO_1332/train/policy_gradient_loss ▆▆█▇▅▂▁▂▇▅▇
wandb:                  PPO_1332/train/std █▇▆▆▅▅▄▄▃▂▁
wandb:           PPO_1332/train/value_loss ▂▁▁▂▄▃▄▅▂▂█
wandb:                PPO_1342/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1342/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1342/rollout/ep_rew_mean ▃▃▃▄▁▄█▄▃▃█▅
wandb:                   PPO_1342/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1342/train/approx_kl ▇▆▂█▁█▂▄▆▃▆
wandb:        PPO_1342/train/clip_fraction ▃▄▃▆▁█▅▃▃▂▆
wandb:           PPO_1342/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1342/train/entropy_loss ▁▁▁▂▃▄▅▇▇▇█
wandb:   PPO_1342/train/explained_variance ▁▃▂█▅▇▇▇▆██
wandb:        PPO_1342/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1342/train/loss ▄▁█▃▁▁▃▄▂▆▄
wandb: PPO_1342/train/policy_gradient_loss ▂▄▁▃▇▃▃██▇█
wandb:                  PPO_1342/train/std ███▇▆▅▄▃▃▂▁
wandb:           PPO_1342/train/value_loss ▃▄▆▂▆▃▁▅▅█▅
wandb:                PPO_1352/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1352/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1352/rollout/ep_rew_mean ▇▇▆▅▇▁▄▅▅▆▅█
wandb:                   PPO_1352/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1352/train/approx_kl ▂▄▅▆█▁▄▆▅▅▄
wandb:        PPO_1352/train/clip_fraction ▃█▁▆▇▁▆▅▂▆▆
wandb:           PPO_1352/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1352/train/entropy_loss ▁▂▃▄▅▆▆▆▇██
wandb:   PPO_1352/train/explained_variance ▄▁▅▅▄▆█▅▆▄▆
wandb:        PPO_1352/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1352/train/loss ▂▁▂▁▂▁█▂▃▄▁
wandb: PPO_1352/train/policy_gradient_loss ▃▄▃▁▇▁▅█▁▂▃
wandb:                  PPO_1352/train/std ██▇▅▃▄▃▃▂▁▁
wandb:           PPO_1352/train/value_loss ▅▁▅▄▄█▁▄▆▅▅
wandb:                PPO_1362/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1362/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1362/rollout/ep_rew_mean ▆▇▇█▆█▅▂▁▅▅▆
wandb:                   PPO_1362/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1362/train/approx_kl ▆▅▄█▅▄▃▂▃▁▄
wandb:        PPO_1362/train/clip_fraction ▅▆▅█▁▄▅▂▃▂▇
wandb:           PPO_1362/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1362/train/entropy_loss ▁▂▃▄▄▄▅▆▆▇█
wandb:   PPO_1362/train/explained_variance ▄▁▃▆▅▄▃▆▇▄█
wandb:        PPO_1362/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1362/train/loss ▁▁▂▃▅▃▃▆█▃▂
wandb: PPO_1362/train/policy_gradient_loss ▄▆▆█▄▅▅▁▅▄▇
wandb:                  PPO_1362/train/std █▇▆▅▅▄▄▃▃▃▁
wandb:           PPO_1362/train/value_loss ▃▃▃▁▄▆▇█▇█▂
wandb:                PPO_1372/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1372/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1372/rollout/ep_rew_mean ▂▁▅▃▁▃▄▅▄▃█▁
wandb:                   PPO_1372/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1372/train/approx_kl ▃▃▇▁█▄▆▄▇▆▇
wandb:        PPO_1372/train/clip_fraction ▅▆█▁▇▄█▇▃▇▅
wandb:           PPO_1372/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1372/train/entropy_loss ▁▂▃▄▄▅▆▆▇█▇
wandb:   PPO_1372/train/explained_variance ▅█▁▄▇▅█▆▅▆▇
wandb:        PPO_1372/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1372/train/loss █▂▃▁▂▂▁▃▂▁▁
wandb: PPO_1372/train/policy_gradient_loss ▆▇▇▁▄▃█▆▄▅▅
wandb:                  PPO_1372/train/std █▇▆▅▅▄▄▃▂▁▂
wandb:           PPO_1372/train/value_loss ▄▃▃█▄▅▁▃▄▄▁
wandb:                PPO_1382/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1382/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1382/rollout/ep_rew_mean ▄▇▄▄▁▆▇▄█▆█▄
wandb:                   PPO_1382/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1382/train/approx_kl ▃▇▄▁▃▇█▄▄▇▆
wandb:        PPO_1382/train/clip_fraction ▆█▅▂▂█▆▁▇▂▇
wandb:           PPO_1382/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1382/train/entropy_loss ▁▁▁▂▄▅▇▇▆▆█
wandb:   PPO_1382/train/explained_variance █▁▅▂▅▅▅▅▄▁▅
wandb:        PPO_1382/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1382/train/loss ▁▁▂▂▄▄▂█▁▆▂
wandb: PPO_1382/train/policy_gradient_loss ▆▃▆▃▁█▆▄▇▇▇
wandb:                  PPO_1382/train/std █▇█▇▅▄▂▂▃▃▁
wandb:           PPO_1382/train/value_loss ▁▁▃█▆▃▃█▃█▄
wandb:                    global_mean_eval ▁▂▅▇█▇█▆▆▇
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▂▅▇▇▇▇▆█▇
wandb:                       mean_reward_1 ▁▂▄▇█▇▆▅▆█
wandb:                      mean_reward_10 ▁▂▅▇▇▇█▅██
wandb:                      mean_reward_11 ▁▂▄▇▆▆█▆▆▆
wandb:                      mean_reward_12 ▁▂▅▇█▆█▅▇▆
wandb:                      mean_reward_13 ▁▂▅▇█▆▆▆▇▄
wandb:                      mean_reward_14 ▁▂▅▇█▇▇▅▇▄
wandb:                      mean_reward_15 ▁▂▄▇█▆█▆▆▇
wandb:                      mean_reward_16 ▁▂▅▇███▆▇▅
wandb:                      mean_reward_17 ▁▂▅▇▇▇▇▆▆█
wandb:                      mean_reward_18 ▁▂▅██▆▇▂▇▆
wandb:                      mean_reward_19 ▁▁▄▇█▇▇▇▄█
wandb:                       mean_reward_2 ▁▂▄▇▇▆▇▅▅█
wandb:                      mean_reward_20 ▁▂▅▇▆█▆█▆▅
wandb:                      mean_reward_21 ▁▂▅▇█▇█▆▅▆
wandb:                      mean_reward_22 ▁▂▅▇█▇█▅▄▄
wandb:                      mean_reward_23 ▁▂▅▇▇▇█▅▅▅
wandb:                      mean_reward_24 ▁▁▄▆█▆▅▆▃▇
wandb:                      mean_reward_25 ▁▂▅▇█▆▇▆▆▇
wandb:                      mean_reward_26 ▁▂▅▆▇▅█▅▅▇
wandb:                      mean_reward_27 ▁▂▅▇█▆▇▆▇█
wandb:                      mean_reward_28 ▁▂▅▇█▆▆▇▇█
wandb:                      mean_reward_29 ▁▃▅▇▇█▇▆▅▆
wandb:                       mean_reward_3 ▁▃▅▇█▆▇▆▆█
wandb:                      mean_reward_30 ▁▂▅▇█▇▇▅▆█
wandb:                      mean_reward_31 ▁▂▅█▇▇█▅▇▄
wandb:                      mean_reward_32 ▁▂▅▇█▇▇▇▇▄
wandb:                      mean_reward_33 ▁▃▅▇█▆▇▅▇▇
wandb:                      mean_reward_34 ▁▁▄▇█▆█▅█▇
wandb:                      mean_reward_35 ▁▁▅▇█▆█▅█▇
wandb:                       mean_reward_4 ▁▂▅▇█▇▇▆▅█
wandb:                       mean_reward_5 ▁▂▅▆█▆▇▄▆▆
wandb:                       mean_reward_6 ▁▂▅▇█▇█▄██
wandb:                       mean_reward_7 ▁▁▄▇▇▇█▇▆▆
wandb:                       mean_reward_8 ▁▂▅▇█▅▆▄▆▆
wandb:                       mean_reward_9 ▁▂▅▇█▆█▆▆▄
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▂▃▃▂▄▄▆▇▇█
wandb:                        std_reward_0 ▂▁▁▂▅▅▆█▇▇
wandb:                        std_reward_1 ▂▁▁▂▄▅▆█▇▅
wandb:                       std_reward_10 ▂▁▁▂▅▅▅█▆▆
wandb:                       std_reward_11 ▂▁▁▂▅▆▅██▇
wandb:                       std_reward_12 ▃▁▁▂▄▅▅█▆▇
wandb:                       std_reward_13 ▂▁▁▂▃▆▆█▇█
wandb:                       std_reward_14 ▂▁▁▂▄▅▆█▇█
wandb:                       std_reward_15 ▂▁▁▂▄▅▅█▇▆
wandb:                       std_reward_16 ▃▁▁▂▄▅▅█▇█
wandb:                       std_reward_17 ▂▁▁▂▄▄▅█▇▆
wandb:                       std_reward_18 ▂▁▁▂▄▅▅█▆▆
wandb:                       std_reward_19 ▂▁▁▂▄▅▅▇█▇
wandb:                        std_reward_2 ▃▁▁▂▅▅▅█▇▆
wandb:                       std_reward_20 ▃▁▂▂▅▅▆▆██
wandb:                       std_reward_21 ▂▁▁▂▃▅▅▇█▇
wandb:                       std_reward_22 ▂▁▁▂▃▅▅▇█▇
wandb:                       std_reward_23 ▂▁▁▂▄▅▅█▇█
wandb:                       std_reward_24 ▂▁▁▁▃▅▆▇█▅
wandb:                       std_reward_25 ▃▁▁▂▄▆▅█▇▇
wandb:                       std_reward_26 ▃▁▁▂▅▆▅██▆
wandb:                       std_reward_27 ▃▁▂▂▄▆▅█▇▆
wandb:                       std_reward_28 ▃▁▁▂▄▆▇██▇
wandb:                       std_reward_29 ▂▁▁▂▅▄▆███
wandb:                        std_reward_3 ▃▁▂▂▄▆▆██▆
wandb:                       std_reward_30 ▃▁▁▂▃▅▅█▇▆
wandb:                       std_reward_31 ▂▁▁▂▄▅▅█▆█
wandb:                       std_reward_32 ▂▁▁▂▄▅▅▇▆█
wandb:                       std_reward_33 ▃▁▁▂▄▅▅█▆▆
wandb:                       std_reward_34 ▂▁▁▂▄▅▄█▆▆
wandb:                       std_reward_35 ▂▁▁▁▄▅▅█▆▆
wandb:                        std_reward_4 ▃▁▂▂▄▅▅██▆
wandb:                        std_reward_5 ▂▁▁▂▄▅▅█▇▆
wandb:                        std_reward_6 ▂▁▁▂▄▅▅█▅▆
wandb:                        std_reward_7 ▂▁▁▂▄▅▆███
wandb:                        std_reward_8 ▂▁▁▁▃▆▅█▇▇
wandb:                        std_reward_9 ▂▁▁▂▄▅▅▇██
wandb:                            time/fps █▄▃▂▂▁▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▁▂▃█▆▂▁▃▃▅▄▄
wandb:                 train/clip_fraction ▄▄▄▅▅▂▁▄▇███
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▁▂▆▇███
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▄▂▂▂▂▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▅▅▅██▆▄▂▁▁▂
wandb:                           train/std █▇▇▆▆▅▅▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1302/global_step 212992
wandb:        PPO_1302/rollout/ep_len_mean 200.0
wandb:        PPO_1302/rollout/ep_rew_mean -830.51654
wandb:                   PPO_1302/time/fps 598.0
wandb:            PPO_1302/train/approx_kl 0.01242
wandb:        PPO_1302/train/clip_fraction 0.1532
wandb:           PPO_1302/train/clip_range 0.2
wandb:         PPO_1302/train/entropy_loss -7.61768
wandb:   PPO_1302/train/explained_variance 0.94702
wandb:        PPO_1302/train/learning_rate 0.0003
wandb:                 PPO_1302/train/loss 24.45832
wandb: PPO_1302/train/policy_gradient_loss -0.00692
wandb:                  PPO_1302/train/std 0.71719
wandb:           PPO_1302/train/value_loss 72.70901
wandb:                PPO_1312/global_step 212992
wandb:        PPO_1312/rollout/ep_len_mean 200.0
wandb:        PPO_1312/rollout/ep_rew_mean -777.71552
wandb:                   PPO_1312/time/fps 598.0
wandb:            PPO_1312/train/approx_kl 0.01367
wandb:        PPO_1312/train/clip_fraction 0.189
wandb:           PPO_1312/train/clip_range 0.2
wandb:         PPO_1312/train/entropy_loss -6.62419
wandb:   PPO_1312/train/explained_variance 0.95369
wandb:        PPO_1312/train/learning_rate 0.0003
wandb:                 PPO_1312/train/loss 15.92439
wandb: PPO_1312/train/policy_gradient_loss -0.00423
wandb:                  PPO_1312/train/std 0.62174
wandb:           PPO_1312/train/value_loss 54.53857
wandb:                PPO_1322/global_step 212992
wandb:        PPO_1322/rollout/ep_len_mean 200.0
wandb:        PPO_1322/rollout/ep_rew_mean -665.87018
wandb:                   PPO_1322/time/fps 598.0
wandb:            PPO_1322/train/approx_kl 0.01675
wandb:        PPO_1322/train/clip_fraction 0.21343
wandb:           PPO_1322/train/clip_range 0.2
wandb:         PPO_1322/train/entropy_loss -5.77933
wandb:   PPO_1322/train/explained_variance 0.94581
wandb:        PPO_1322/train/learning_rate 0.0003
wandb:                 PPO_1322/train/loss 47.98217
wandb: PPO_1322/train/policy_gradient_loss -0.00499
wandb:                  PPO_1322/train/std 0.55201
wandb:           PPO_1322/train/value_loss 68.78239
wandb:                PPO_1332/global_step 212992
wandb:        PPO_1332/rollout/ep_len_mean 200.0
wandb:        PPO_1332/rollout/ep_rew_mean -626.44946
wandb:                   PPO_1332/time/fps 597.0
wandb:            PPO_1332/train/approx_kl 0.01683
wandb:        PPO_1332/train/clip_fraction 0.20101
wandb:           PPO_1332/train/clip_range 0.2
wandb:         PPO_1332/train/entropy_loss -5.22177
wandb:   PPO_1332/train/explained_variance 0.93654
wandb:        PPO_1332/train/learning_rate 0.0003
wandb:                 PPO_1332/train/loss 77.13206
wandb: PPO_1332/train/policy_gradient_loss -0.0036
wandb:                  PPO_1332/train/std 0.51085
wandb:           PPO_1332/train/value_loss 171.57169
wandb:                PPO_1342/global_step 212992
wandb:        PPO_1342/rollout/ep_len_mean 200.0
wandb:        PPO_1342/rollout/ep_rew_mean -595.92737
wandb:                   PPO_1342/time/fps 600.0
wandb:            PPO_1342/train/approx_kl 0.01616
wandb:        PPO_1342/train/clip_fraction 0.22092
wandb:           PPO_1342/train/clip_range 0.2
wandb:         PPO_1342/train/entropy_loss -4.86229
wandb:   PPO_1342/train/explained_variance 0.96823
wandb:        PPO_1342/train/learning_rate 0.0003
wandb:                 PPO_1342/train/loss 92.20898
wandb: PPO_1342/train/policy_gradient_loss -0.00278
wandb:                  PPO_1342/train/std 0.48414
wandb:           PPO_1342/train/value_loss 286.28943
wandb:                PPO_1352/global_step 212992
wandb:        PPO_1352/rollout/ep_len_mean 200.0
wandb:        PPO_1352/rollout/ep_rew_mean -559.29108
wandb:                   PPO_1352/time/fps 600.0
wandb:            PPO_1352/train/approx_kl 0.0155
wandb:        PPO_1352/train/clip_fraction 0.22002
wandb:           PPO_1352/train/clip_range 0.2
wandb:         PPO_1352/train/entropy_loss -4.59053
wandb:   PPO_1352/train/explained_variance 0.97515
wandb:        PPO_1352/train/learning_rate 0.0003
wandb:                 PPO_1352/train/loss 38.11616
wandb: PPO_1352/train/policy_gradient_loss -0.00216
wandb:                  PPO_1352/train/std 0.46785
wandb:           PPO_1352/train/value_loss 406.86066
wandb:                PPO_1362/global_step 212992
wandb:        PPO_1362/rollout/ep_len_mean 200.0
wandb:        PPO_1362/rollout/ep_rew_mean -600.82312
wandb:                   PPO_1362/time/fps 598.0
wandb:            PPO_1362/train/approx_kl 0.01552
wandb:        PPO_1362/train/clip_fraction 0.23214
wandb:           PPO_1362/train/clip_range 0.2
wandb:         PPO_1362/train/entropy_loss -4.19592
wandb:   PPO_1362/train/explained_variance 0.98257
wandb:        PPO_1362/train/learning_rate 0.0003
wandb:                 PPO_1362/train/loss 107.57084
wandb: PPO_1362/train/policy_gradient_loss -0.00033
wandb:                  PPO_1362/train/std 0.44265
wandb:           PPO_1362/train/value_loss 522.73517
wandb:                PPO_1372/global_step 212992
wandb:        PPO_1372/rollout/ep_len_mean 200.0
wandb:        PPO_1372/rollout/ep_rew_mean -634.30829
wandb:                   PPO_1372/time/fps 597.0
wandb:            PPO_1372/train/approx_kl 0.01748
wandb:        PPO_1372/train/clip_fraction 0.21818
wandb:           PPO_1372/train/clip_range 0.2
wandb:         PPO_1372/train/entropy_loss -3.9742
wandb:   PPO_1372/train/explained_variance 0.98105
wandb:        PPO_1372/train/learning_rate 0.0003
wandb:                 PPO_1372/train/loss 83.88232
wandb: PPO_1372/train/policy_gradient_loss -0.00021
wandb:                  PPO_1372/train/std 0.42904
wandb:           PPO_1372/train/value_loss 443.22116
wandb:                PPO_1382/global_step 212992
wandb:        PPO_1382/rollout/ep_len_mean 200.0
wandb:        PPO_1382/rollout/ep_rew_mean -627.68494
wandb:                   PPO_1382/time/fps 596.0
wandb:            PPO_1382/train/approx_kl 0.01769
wandb:        PPO_1382/train/clip_fraction 0.2298
wandb:           PPO_1382/train/clip_range 0.2
wandb:         PPO_1382/train/entropy_loss -3.75819
wandb:   PPO_1382/train/explained_variance 0.98132
wandb:        PPO_1382/train/learning_rate 0.0003
wandb:                 PPO_1382/train/loss 181.98555
wandb: PPO_1382/train/policy_gradient_loss -0.00012
wandb:                  PPO_1382/train/std 0.41581
wandb:           PPO_1382/train/value_loss 805.42407
wandb:                    global_mean_eval -583.10884
wandb:                         global_step 212992
wandb:                       mean_reward_0 -556.71758
wandb:                       mean_reward_1 -509.79422
wandb:                      mean_reward_10 -541.01072
wandb:                      mean_reward_11 -565.69051
wandb:                      mean_reward_12 -596.8119
wandb:                      mean_reward_13 -658.96323
wandb:                      mean_reward_14 -654.61454
wandb:                      mean_reward_15 -545.9936
wandb:                      mean_reward_16 -621.37119
wandb:                      mean_reward_17 -531.23221
wandb:                      mean_reward_18 -610.54982
wandb:                      mean_reward_19 -553.10044
wandb:                       mean_reward_2 -509.96856
wandb:                      mean_reward_20 -628.16637
wandb:                      mean_reward_21 -601.08938
wandb:                      mean_reward_22 -666.65913
wandb:                      mean_reward_23 -645.19448
wandb:                      mean_reward_24 -542.30735
wandb:                      mean_reward_25 -570.13942
wandb:                      mean_reward_26 -569.66261
wandb:                      mean_reward_27 -528.10234
wandb:                      mean_reward_28 -536.38496
wandb:                      mean_reward_29 -589.38904
wandb:                       mean_reward_3 -534.75899
wandb:                      mean_reward_30 -548.60144
wandb:                      mean_reward_31 -674.76846
wandb:                      mean_reward_32 -657.54041
wandb:                      mean_reward_33 -567.2976
wandb:                      mean_reward_34 -562.8785
wandb:                      mean_reward_35 -575.2194
wandb:                       mean_reward_4 -540.14267
wandb:                       mean_reward_5 -582.86471
wandb:                       mean_reward_6 -548.75614
wandb:                       mean_reward_7 -589.7865
wandb:                       mean_reward_8 -599.1141
wandb:                       mean_reward_9 -677.27589
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -901.89819
wandb:                        std_reward_0 344.00432
wandb:                        std_reward_1 248.07375
wandb:                       std_reward_10 324.11768
wandb:                       std_reward_11 351.48783
wandb:                       std_reward_12 365.05555
wandb:                       std_reward_13 433.82913
wandb:                       std_reward_14 419.41952
wandb:                       std_reward_15 346.03262
wandb:                       std_reward_16 420.34458
wandb:                       std_reward_17 304.7396
wandb:                       std_reward_18 383.03174
wandb:                       std_reward_19 354.80183
wandb:                        std_reward_2 300.02853
wandb:                       std_reward_20 402.1015
wandb:                       std_reward_21 372.01305
wandb:                       std_reward_22 437.39454
wandb:                       std_reward_23 438.90014
wandb:                       std_reward_24 306.49355
wandb:                       std_reward_25 360.21894
wandb:                       std_reward_26 346.52261
wandb:                       std_reward_27 304.34492
wandb:                       std_reward_28 303.09708
wandb:                       std_reward_29 405.86749
wandb:                        std_reward_3 318.8117
wandb:                       std_reward_30 350.8371
wandb:                       std_reward_31 426.09956
wandb:                       std_reward_32 421.3285
wandb:                       std_reward_33 334.95115
wandb:                       std_reward_34 345.89845
wandb:                       std_reward_35 353.40064
wandb:                        std_reward_4 323.72266
wandb:                        std_reward_5 346.32182
wandb:                        std_reward_6 329.05109
wandb:                        std_reward_7 375.62316
wandb:                        std_reward_8 377.02261
wandb:                        std_reward_9 435.11132
wandb:                            time/fps 597.0
wandb:                     train/approx_kl 0.0106
wandb:                 train/clip_fraction 0.13875
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.80915
wandb:            train/explained_variance 0.95603
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 9.91018
wandb:          train/policy_gradient_loss -0.00981
wandb:                           train/std 0.84977
wandb:                    train/value_loss 25.25248
wandb: 
wandb: Synced kind-terrain-50: https://wandb.ai/tidiane/meta_rl_context/runs/2ow35t7w
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 13 other file(s)
wandb: Find logs at: ./wandb/run-20230626_052650-2ow35t7w/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1303/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1303/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1303/rollout/ep_rew_mean ▁▁▂▃▂▂▄▅▆▆▆█
wandb:                   PPO_1303/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1303/train/approx_kl ▄█▂▃▂▆▂▁▅▃▂
wandb:        PPO_1303/train/clip_fraction ▇▆▄▁▃▇▃▃▆█▄
wandb:           PPO_1303/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1303/train/entropy_loss ▁▂▃▃▄▄▅▅▆▇█
wandb:   PPO_1303/train/explained_variance ▁▃▇▄▆▇▅▇▇▅█
wandb:        PPO_1303/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1303/train/loss ▂▁▁▅▂▄█▃▅▇▅
wandb: PPO_1303/train/policy_gradient_loss ▂▁▂▅▇▅▇▅▄▄█
wandb:                  PPO_1303/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1303/train/value_loss ▁▁▁▃▄▃█████
wandb:                PPO_1314/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1314/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1314/rollout/ep_rew_mean ▁▁▂▃▃▅▅▅▅▆██
wandb:                   PPO_1314/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1314/train/approx_kl ▂▁█▆▆▄▃▆█▆▅
wandb:        PPO_1314/train/clip_fraction ▄▁▅▆▄▆▄▄▅█▅
wandb:           PPO_1314/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1314/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1314/train/explained_variance ▇▄▆▆▅▄▇▆█▃▁
wandb:        PPO_1314/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1314/train/loss █▂▄▅▅▄▁▃▃▃▁
wandb: PPO_1314/train/policy_gradient_loss ▂█▁▄▃▅▇▆▆▆▆
wandb:                  PPO_1314/train/std █▇▆▆▅▄▄▃▂▂▁
wandb:           PPO_1314/train/value_loss █▆▄▅█▃▃▅▇▁▅
wandb:                PPO_1324/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1324/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1324/rollout/ep_rew_mean ▁▅▆▅▆▆▇▇▆███
wandb:                   PPO_1324/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1324/train/approx_kl ▁▅▂▅▆▃██▅▆▇
wandb:        PPO_1324/train/clip_fraction ▁▅▄▅▆▄█▅▄▇▄
wandb:           PPO_1324/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1324/train/entropy_loss ▁▂▂▃▄▄▅▆▆▆█
wandb:   PPO_1324/train/explained_variance █▄▅█▁▇▇██▆▇
wandb:        PPO_1324/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1324/train/loss █▂▁▂▂▃▁▅▂▄▄
wandb: PPO_1324/train/policy_gradient_loss ▃▅▇▁▄▅▃██▆▇
wandb:                  PPO_1324/train/std █▇▇▆▅▅▄▃▃▃▁
wandb:           PPO_1324/train/value_loss █▃▄▃▄▄▁▄▄▃▄
wandb:                PPO_1334/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1334/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1334/rollout/ep_rew_mean ▆▃▃▁▁▃▃▄▅█▇█
wandb:                   PPO_1334/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1334/train/approx_kl ▃▁▃▁▃▆▅▄▅█▄
wandb:        PPO_1334/train/clip_fraction ▂▃▄▁▄██▅▆█▇
wandb:           PPO_1334/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1334/train/entropy_loss ▁▂▂▃▃▄▅▆▆▇█
wandb:   PPO_1334/train/explained_variance ▁▅▅▄▄▄▄▇▅▇█
wandb:        PPO_1334/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1334/train/loss ▇▂▆▇█▂▂▄▁▂▂
wandb: PPO_1334/train/policy_gradient_loss ▄▄▁▄▂▂▃▂▄█▄
wandb:                  PPO_1334/train/std █▇▇▆▆▅▄▃▃▂▁
wandb:           PPO_1334/train/value_loss █▅▇██▄▃▅▅▁▂
wandb:                PPO_1344/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1344/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1344/rollout/ep_rew_mean ▆▃▂▄▄▅▁▃▅▅█▇
wandb:                   PPO_1344/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1344/train/approx_kl █▄▄▇▆▇▁█▄▂▄
wandb:        PPO_1344/train/clip_fraction ▃▆▄█▃▄▁▄▄▁▄
wandb:           PPO_1344/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1344/train/entropy_loss ▁▂▂▃▄▄▅▆▇▇█
wandb:   PPO_1344/train/explained_variance ▁▃▄▄▅▄▄▇▆▆█
wandb:        PPO_1344/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1344/train/loss ▁▂▃▁▃▃▂▁▅█▃
wandb: PPO_1344/train/policy_gradient_loss ▆▇▅▄▂▂▃▄▁█▃
wandb:                  PPO_1344/train/std █▇▇▆▅▅▄▃▂▂▁
wandb:           PPO_1344/train/value_loss ▁▁▃▂▄▄▆▅▄█▆
wandb:                PPO_1354/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1354/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1354/rollout/ep_rew_mean ▅▁▄▅▇█▇▂▇▇▅▄
wandb:                   PPO_1354/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1354/train/approx_kl ▄▄▁▅▂█▄▄▄▄▆
wandb:        PPO_1354/train/clip_fraction ▁▁▂▅▃█▃▂▂▅▄
wandb:           PPO_1354/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1354/train/entropy_loss ▁▁▂▂▃▄▄▄▅▆█
wandb:   PPO_1354/train/explained_variance ▇▇▆▃▇▇▅▇█▅▁
wandb:        PPO_1354/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1354/train/loss ▅▅▂▂▄▁▂▄▂▁█
wandb: PPO_1354/train/policy_gradient_loss ▂▁▂▄▃█▅▁▃▇▄
wandb:                  PPO_1354/train/std ██▇▇▆▅▅▅▄▃▁
wandb:           PPO_1354/train/value_loss ▂▂▂▄▃▁▄▆▃▅█
wandb:                PPO_1364/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1364/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1364/rollout/ep_rew_mean ▁▄█▇▅▅▆▅▇▅▃▄
wandb:                   PPO_1364/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1364/train/approx_kl ▄▅▁▂▅▅▆▄█▆▆
wandb:        PPO_1364/train/clip_fraction ▃▅▇▁▇█▅▅▅▆█
wandb:           PPO_1364/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1364/train/entropy_loss ▁▁▂▃▃▄▅▅▆▇█
wandb:   PPO_1364/train/explained_variance ▂▁▅▄▆▃▄█▇▁▅
wandb:        PPO_1364/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1364/train/loss █▂▁▃▂▂▁▂▂▆▂
wandb: PPO_1364/train/policy_gradient_loss ▃▂▃▁▄▄▇▄▃██
wandb:                  PPO_1364/train/std ██▆▆▆▄▄▄▃▂▁
wandb:           PPO_1364/train/value_loss █▅▂▇▄▁▂▃▆▅▃
wandb:                PPO_1373/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1373/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1373/rollout/ep_rew_mean ▅▅█▅▁▂▆▇▆█▅█
wandb:                   PPO_1373/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1373/train/approx_kl ▂▁█▃▃▂▄▄█▄▄
wandb:        PPO_1373/train/clip_fraction ▂▄▇▁▂▄▅▅██▇
wandb:           PPO_1373/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1373/train/entropy_loss ▁▁▂▃▃▄▄▅▆▆█
wandb:   PPO_1373/train/explained_variance ▁▇▂▂▆▃▇▅▄█▇
wandb:        PPO_1373/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1373/train/loss ▃▂▁▄▂▁█▂▁▆▃
wandb: PPO_1373/train/policy_gradient_loss ▂▇▇▁▁▁▅▇█▆▆
wandb:                  PPO_1373/train/std ██▇▇▆▅▅▄▃▃▁
wandb:           PPO_1373/train/value_loss ▇▄▃▇▆█▆▅▃▁▃
wandb:                PPO_1383/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1383/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1383/rollout/ep_rew_mean ▄▂▆▃▃█▅▃▄▃▅▁
wandb:                   PPO_1383/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1383/train/approx_kl ▄▄▂▁█▂▃▂▃▅▄
wandb:        PPO_1383/train/clip_fraction █▁▅▁▇▆▆▆██▇
wandb:           PPO_1383/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1383/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1383/train/explained_variance ▆▆▃██▅▁▆▂▅▇
wandb:        PPO_1383/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1383/train/loss ▁▁▂▃▁▁▁▄▄▃█
wandb: PPO_1383/train/policy_gradient_loss ▅▂▄▁▄▅▆▆▆▆█
wandb:                  PPO_1383/train/std █▇▆▆▄▄▄▄▃▂▁
wandb:           PPO_1383/train/value_loss ▁▇▃▆▃▄█▇▇▅▄
wandb:                    global_mean_eval ▁▄▇████▇██
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▄▇██▇█▇██
wandb:                       mean_reward_1 ▁▄▇████▇▇█
wandb:                      mean_reward_10 ▁▄▇▇█▇▇▇▇█
wandb:                      mean_reward_11 ▁▅▇█████▇█
wandb:                      mean_reward_12 ▁▄▇███▇▇▇▇
wandb:                      mean_reward_13 ▁▄▇████▇▇█
wandb:                      mean_reward_14 ▁▄▇████▇██
wandb:                      mean_reward_15 ▁▄▇████▇██
wandb:                      mean_reward_16 ▁▅▇███████
wandb:                      mean_reward_17 ▁▄████▇▇██
wandb:                      mean_reward_18 ▁▄▇████▇█▇
wandb:                      mean_reward_19 ▁▄▇▇▇▇▇▇▇█
wandb:                       mean_reward_2 ▁▄▇███▇▇▇█
wandb:                      mean_reward_20 ▁▄▇▇██▇███
wandb:                      mean_reward_21 ▁▄▇████▇██
wandb:                      mean_reward_22 ▁▄▇▇▇█▇▆▇█
wandb:                      mean_reward_23 ▁▄▇██████▇
wandb:                      mean_reward_24 ▁▄▇████▇█▇
wandb:                      mean_reward_25 ▁▄▇████▇█▇
wandb:                      mean_reward_26 ▁▄▇███▇▇▇█
wandb:                      mean_reward_27 ▁▄▇████▇█▇
wandb:                      mean_reward_28 ▁▄▇█▇▇█▇██
wandb:                      mean_reward_29 ▁▄▇██████▇
wandb:                       mean_reward_3 ▁▄▇▇▇█▇▇▇█
wandb:                      mean_reward_30 ▁▄▇▇▇█▇▇█▇
wandb:                      mean_reward_31 ▁▄▇▇██▇▇█▇
wandb:                      mean_reward_32 ▁▄▇█▇█▇▇██
wandb:                      mean_reward_33 ▁▄▇██▇█▇██
wandb:                      mean_reward_34 ▁▄▇██▇█▇▇█
wandb:                      mean_reward_35 ▁▄▇███▇▇██
wandb:                       mean_reward_4 ▁▄▇██▇█▇█▇
wandb:                       mean_reward_5 ▁▄▇███████
wandb:                       mean_reward_6 ▁▄▇▇█▇▇███
wandb:                       mean_reward_7 ▁▄▇█████▇█
wandb:                       mean_reward_8 ▁▄▇███▇███
wandb:                       mean_reward_9 ▁▄▇████▇██
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▂▂▂▃▅▃▆▇▇▇█▆
wandb:                        std_reward_0 ▁▁▂▁▂▅▅█▇█
wandb:                        std_reward_1 ▁▁▂▁▂▅▆█▇█
wandb:                       std_reward_10 ▁▁▂▁▂▅▆█▇▇
wandb:                       std_reward_11 ▁▁▂▁▂▅▆█▇█
wandb:                       std_reward_12 ▁▁▂▁▂▄▅▇▆█
wandb:                       std_reward_13 ▁▁▂▁▂▅▅█▇█
wandb:                       std_reward_14 ▁▁▂▁▂▅▆█▇▇
wandb:                       std_reward_15 ▁▁▂▁▂▅▅█▆▇
wandb:                       std_reward_16 ▁▁▃▁▂▅▆█▇█
wandb:                       std_reward_17 ▁▁▂▁▂▅▆█▆▇
wandb:                       std_reward_18 ▁▁▂▁▂▅▆▇▆█
wandb:                       std_reward_19 ▂▁▂▁▂▆▇█▇▇
wandb:                        std_reward_2 ▁▁▂▁▂▅▇█▇█
wandb:                       std_reward_20 ▁▁▂▁▂▅▆▇▇█
wandb:                       std_reward_21 ▁▁▂▁▂▅▆█▇█
wandb:                       std_reward_22 ▁▁▂▁▂▃▅█▆▇
wandb:                       std_reward_23 ▁▁▂▁▂▅▆▇▇█
wandb:                       std_reward_24 ▁▁▂▁▂▄▆█▆█
wandb:                       std_reward_25 ▁▁▂▁▂▅▅▇▆█
wandb:                       std_reward_26 ▁▁▂▁▂▅▆█▇▇
wandb:                       std_reward_27 ▁▁▂▁▂▅▅▇▆█
wandb:                       std_reward_28 ▁▁▂▁▂▅▆█▆█
wandb:                       std_reward_29 ▁▁▂▁▂▄▅▆▆█
wandb:                        std_reward_3 ▁▁▂▁▂▄▇▇▇█
wandb:                       std_reward_30 ▁▁▂▂▂▄▆█▆█
wandb:                       std_reward_31 ▁▁▂▁▂▄▆█▆█
wandb:                       std_reward_32 ▁▁▂▁▂▄▆█▆▇
wandb:                       std_reward_33 ▁▁▂▁▂▅▆█▅▇
wandb:                       std_reward_34 ▁▁▂▁▂▆▅█▆▇
wandb:                       std_reward_35 ▁▁▂▁▂▄▆█▆█
wandb:                        std_reward_4 ▁▁▂▁▂▅▆█▆█
wandb:                        std_reward_5 ▂▁▂▁▂▄▆▇▇█
wandb:                        std_reward_6 ▁▂▂▁▃▆██▇█
wandb:                        std_reward_7 ▁▁▂▁▂▄▅▇▆█
wandb:                        std_reward_8 ▁▁▂▁▁▅▇▇▇█
wandb:                        std_reward_9 ▁▁▂▁▂▅▅█▅█
wandb:                            time/fps █▄▃▂▂▂▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▃▂▃█▇▂▁▃▃▄▄▅
wandb:                 train/clip_fraction ▅▄▄▅▅▂▁▅▆█▇▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▃▃▄▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▁▄▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▄▃▃▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▆▆██▇▃▃▁▃▃
wandb:                           train/std █▇▇▆▅▅▄▃▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1303/global_step 212992
wandb:        PPO_1303/rollout/ep_len_mean 200.0
wandb:        PPO_1303/rollout/ep_rew_mean -729.87524
wandb:                   PPO_1303/time/fps 589.0
wandb:            PPO_1303/train/approx_kl 0.01015
wandb:        PPO_1303/train/clip_fraction 0.12627
wandb:           PPO_1303/train/clip_range 0.2
wandb:         PPO_1303/train/entropy_loss -7.61006
wandb:   PPO_1303/train/explained_variance 0.96932
wandb:        PPO_1303/train/learning_rate 0.0003
wandb:                 PPO_1303/train/loss 48.82642
wandb: PPO_1303/train/policy_gradient_loss -0.00575
wandb:                  PPO_1303/train/std 0.71597
wandb:           PPO_1303/train/value_loss 151.464
wandb:                PPO_1314/global_step 212992
wandb:        PPO_1314/rollout/ep_len_mean 200.0
wandb:        PPO_1314/rollout/ep_rew_mean -590.4024
wandb:                   PPO_1314/time/fps 587.0
wandb:            PPO_1314/train/approx_kl 0.01192
wandb:        PPO_1314/train/clip_fraction 0.16514
wandb:           PPO_1314/train/clip_range 0.2
wandb:         PPO_1314/train/entropy_loss -6.7463
wandb:   PPO_1314/train/explained_variance 0.95574
wandb:        PPO_1314/train/learning_rate 0.0003
wandb:                 PPO_1314/train/loss 22.26704
wandb: PPO_1314/train/policy_gradient_loss -0.0042
wandb:                  PPO_1314/train/std 0.63511
wandb:           PPO_1314/train/value_loss 89.26234
wandb:                PPO_1324/global_step 212992
wandb:        PPO_1324/rollout/ep_len_mean 200.0
wandb:        PPO_1324/rollout/ep_rew_mean -554.02301
wandb:                   PPO_1324/time/fps 587.0
wandb:            PPO_1324/train/approx_kl 0.01508
wandb:        PPO_1324/train/clip_fraction 0.1814
wandb:           PPO_1324/train/clip_range 0.2
wandb:         PPO_1324/train/entropy_loss -5.9861
wandb:   PPO_1324/train/explained_variance 0.96865
wandb:        PPO_1324/train/learning_rate 0.0003
wandb:                 PPO_1324/train/loss 44.40052
wandb: PPO_1324/train/policy_gradient_loss -0.00294
wandb:                  PPO_1324/train/std 0.56856
wandb:           PPO_1324/train/value_loss 84.94218
wandb:                PPO_1334/global_step 212992
wandb:        PPO_1334/rollout/ep_len_mean 200.0
wandb:        PPO_1334/rollout/ep_rew_mean -518.61517
wandb:                   PPO_1334/time/fps 597.0
wandb:            PPO_1334/train/approx_kl 0.01527
wandb:        PPO_1334/train/clip_fraction 0.21785
wandb:           PPO_1334/train/clip_range 0.2
wandb:         PPO_1334/train/entropy_loss -5.36873
wandb:   PPO_1334/train/explained_variance 0.98388
wandb:        PPO_1334/train/learning_rate 0.0003
wandb:                 PPO_1334/train/loss 15.58491
wandb: PPO_1334/train/policy_gradient_loss -0.00171
wandb:                  PPO_1334/train/std 0.52063
wandb:           PPO_1334/train/value_loss 41.41672
wandb:                PPO_1344/global_step 212992
wandb:        PPO_1344/rollout/ep_len_mean 200.0
wandb:        PPO_1344/rollout/ep_rew_mean -498.4812
wandb:                   PPO_1344/time/fps 596.0
wandb:            PPO_1344/train/approx_kl 0.01573
wandb:        PPO_1344/train/clip_fraction 0.20959
wandb:           PPO_1344/train/clip_range 0.2
wandb:         PPO_1344/train/entropy_loss -4.73924
wandb:   PPO_1344/train/explained_variance 0.98957
wandb:        PPO_1344/train/learning_rate 0.0003
wandb:                 PPO_1344/train/loss 29.30214
wandb: PPO_1344/train/policy_gradient_loss -0.00185
wandb:                  PPO_1344/train/std 0.47563
wandb:           PPO_1344/train/value_loss 74.98966
wandb:                PPO_1354/global_step 212992
wandb:        PPO_1354/rollout/ep_len_mean 200.0
wandb:        PPO_1354/rollout/ep_rew_mean -523.10956
wandb:                   PPO_1354/time/fps 596.0
wandb:            PPO_1354/train/approx_kl 0.01792
wandb:        PPO_1354/train/clip_fraction 0.22192
wandb:           PPO_1354/train/clip_range 0.2
wandb:         PPO_1354/train/entropy_loss -4.35645
wandb:   PPO_1354/train/explained_variance 0.98308
wandb:        PPO_1354/train/learning_rate 0.0003
wandb:                 PPO_1354/train/loss 95.87229
wandb: PPO_1354/train/policy_gradient_loss 0.00017
wandb:                  PPO_1354/train/std 0.44936
wandb:           PPO_1354/train/value_loss 164.11676
wandb:                PPO_1364/global_step 212992
wandb:        PPO_1364/rollout/ep_len_mean 200.0
wandb:        PPO_1364/rollout/ep_rew_mean -503.55756
wandb:                   PPO_1364/time/fps 595.0
wandb:            PPO_1364/train/approx_kl 0.0187
wandb:        PPO_1364/train/clip_fraction 0.24369
wandb:           PPO_1364/train/clip_range 0.2
wandb:         PPO_1364/train/entropy_loss -3.96428
wandb:   PPO_1364/train/explained_variance 0.98934
wandb:        PPO_1364/train/learning_rate 0.0003
wandb:                 PPO_1364/train/loss 30.67681
wandb: PPO_1364/train/policy_gradient_loss 0.0019
wandb:                  PPO_1364/train/std 0.42651
wandb:           PPO_1364/train/value_loss 103.77716
wandb:                PPO_1373/global_step 212992
wandb:        PPO_1373/rollout/ep_len_mean 200.0
wandb:        PPO_1373/rollout/ep_rew_mean -477.10565
wandb:                   PPO_1373/time/fps 596.0
wandb:            PPO_1373/train/approx_kl 0.01806
wandb:        PPO_1373/train/clip_fraction 0.26616
wandb:           PPO_1373/train/clip_range 0.2
wandb:         PPO_1373/train/entropy_loss -3.56929
wandb:   PPO_1373/train/explained_variance 0.99102
wandb:        PPO_1373/train/learning_rate 0.0003
wandb:                 PPO_1373/train/loss 38.72242
wandb: PPO_1373/train/policy_gradient_loss 0.00249
wandb:                  PPO_1373/train/std 0.40233
wandb:           PPO_1373/train/value_loss 97.44786
wandb:                PPO_1383/global_step 212992
wandb:        PPO_1383/rollout/ep_len_mean 200.0
wandb:        PPO_1383/rollout/ep_rew_mean -503.21542
wandb:                   PPO_1383/time/fps 595.0
wandb:            PPO_1383/train/approx_kl 0.02173
wandb:        PPO_1383/train/clip_fraction 0.27009
wandb:           PPO_1383/train/clip_range 0.2
wandb:         PPO_1383/train/entropy_loss -3.24381
wandb:   PPO_1383/train/explained_variance 0.99134
wandb:        PPO_1383/train/learning_rate 0.0003
wandb:                 PPO_1383/train/loss 176.81693
wandb: PPO_1383/train/policy_gradient_loss 0.00635
wandb:                  PPO_1383/train/std 0.38493
wandb:           PPO_1383/train/value_loss 99.35527
wandb:                    global_mean_eval -469.68518
wandb:                         global_step 212992
wandb:                       mean_reward_0 -469.0825
wandb:                       mean_reward_1 -468.90241
wandb:                      mean_reward_10 -439.88034
wandb:                      mean_reward_11 -472.95026
wandb:                      mean_reward_12 -490.65933
wandb:                      mean_reward_13 -483.50297
wandb:                      mean_reward_14 -461.42197
wandb:                      mean_reward_15 -458.44783
wandb:                      mean_reward_16 -465.2554
wandb:                      mean_reward_17 -470.8239
wandb:                      mean_reward_18 -475.7963
wandb:                      mean_reward_19 -427.71997
wandb:                       mean_reward_2 -451.66911
wandb:                      mean_reward_20 -464.32432
wandb:                      mean_reward_21 -465.34111
wandb:                      mean_reward_22 -452.51189
wandb:                      mean_reward_23 -507.41685
wandb:                      mean_reward_24 -493.90084
wandb:                      mean_reward_25 -513.96866
wandb:                      mean_reward_26 -446.45033
wandb:                      mean_reward_27 -490.27893
wandb:                      mean_reward_28 -456.52139
wandb:                      mean_reward_29 -490.36753
wandb:                       mean_reward_3 -453.19812
wandb:                      mean_reward_30 -474.38043
wandb:                      mean_reward_31 -481.93046
wandb:                      mean_reward_32 -462.75584
wandb:                      mean_reward_33 -462.18531
wandb:                      mean_reward_34 -452.85683
wandb:                      mean_reward_35 -474.24394
wandb:                       mean_reward_4 -488.05901
wandb:                       mean_reward_5 -471.30355
wandb:                       mean_reward_6 -433.14696
wandb:                       mean_reward_7 -478.00187
wandb:                       mean_reward_8 -479.09058
wandb:                       mean_reward_9 -480.31945
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -939.39148
wandb:                        std_reward_0 198.18216
wandb:                        std_reward_1 194.95411
wandb:                       std_reward_10 168.72266
wandb:                       std_reward_11 201.80472
wandb:                       std_reward_12 226.40378
wandb:                       std_reward_13 206.73302
wandb:                       std_reward_14 181.93964
wandb:                       std_reward_15 189.0107
wandb:                       std_reward_16 194.28409
wandb:                       std_reward_17 188.2336
wandb:                       std_reward_18 207.28729
wandb:                       std_reward_19 157.30751
wandb:                        std_reward_2 200.63611
wandb:                       std_reward_20 181.49755
wandb:                       std_reward_21 194.26099
wandb:                       std_reward_22 187.68408
wandb:                       std_reward_23 212.84965
wandb:                       std_reward_24 201.03211
wandb:                       std_reward_25 221.75476
wandb:                       std_reward_26 179.04361
wandb:                       std_reward_27 205.5408
wandb:                       std_reward_28 194.51403
wandb:                       std_reward_29 207.32605
wandb:                        std_reward_3 191.92639
wandb:                       std_reward_30 190.15393
wandb:                       std_reward_31 200.12858
wandb:                       std_reward_32 180.5598
wandb:                       std_reward_33 184.30964
wandb:                       std_reward_34 172.64872
wandb:                       std_reward_35 195.63763
wandb:                        std_reward_4 192.32128
wandb:                        std_reward_5 192.35446
wandb:                        std_reward_6 158.45726
wandb:                        std_reward_7 210.66693
wandb:                        std_reward_8 195.53172
wandb:                        std_reward_9 206.97628
wandb:                            time/fps 600.0
wandb:                     train/approx_kl 0.01128
wandb:                 train/clip_fraction 0.12654
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7942
wandb:            train/explained_variance 0.92484
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.3112
wandb:          train/policy_gradient_loss -0.0089
wandb:                           train/std 0.84816
wandb:                    train/value_loss 56.39774
wandb: 
wandb: Synced logical-forest-46: https://wandb.ai/tidiane/meta_rl_context/runs/2gfbf78j
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 14 other file(s)
wandb: Find logs at: ./wandb/run-20230626_052650-2gfbf78j/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1304/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1304/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1304/rollout/ep_rew_mean ▂▁▃▃▄▅▅▅▇▆▇█
wandb:                   PPO_1304/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1304/train/approx_kl ▁▃▅▂▅▄▄▃▄██
wandb:        PPO_1304/train/clip_fraction ▂▁▅▃▅▄▄▃▅▇█
wandb:           PPO_1304/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1304/train/entropy_loss ▁▂▃▃▄▅▅▆▆▇█
wandb:   PPO_1304/train/explained_variance ▇▃█▅▇▆▁▆▆▆▆
wandb:        PPO_1304/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1304/train/loss ▁▂▄▅█▂▃▅▇▅▃
wandb: PPO_1304/train/policy_gradient_loss ▄▃▁▆▂▃▂▆██▄
wandb:                  PPO_1304/train/std █▇▆▆▅▄▄▃▃▂▁
wandb:           PPO_1304/train/value_loss ▁▁▁▆▄█▆▅▆▅▇
wandb:                PPO_1313/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1313/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1313/rollout/ep_rew_mean ▃▂▁▄▃▄▃▃▄▆██
wandb:                   PPO_1313/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1313/train/approx_kl ▂▁▄▅▆▄▅▄▇██
wandb:        PPO_1313/train/clip_fraction ▁▂▄▅▄▄▂▅▇▇█
wandb:           PPO_1313/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1313/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1313/train/explained_variance ▇▄▆▄▅▄▄▁▅▆█
wandb:        PPO_1313/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1313/train/loss ▆▂▃▁█▁▆▁▁▁▄
wandb: PPO_1313/train/policy_gradient_loss ▃▄▂▁▇██▇▅▁▂
wandb:                  PPO_1313/train/std █▇▇▆▅▅▄▄▃▂▁
wandb:           PPO_1313/train/value_loss ▆█▅▆▆▄▅▄▁▂▁
wandb:                PPO_1323/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1323/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1323/rollout/ep_rew_mean ▁▂▂▂▃▄▅▆▄▅▆█
wandb:                   PPO_1323/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1323/train/approx_kl ▂▁▃▁▃▃▁▄▂▄█
wandb:        PPO_1323/train/clip_fraction ▁▂▂▅▆▆▄█▂▇█
wandb:           PPO_1323/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1323/train/entropy_loss ▁▂▂▃▄▅▅▆▆▇█
wandb:   PPO_1323/train/explained_variance ▆▆▁▅▅▆▄▆▆▃█
wandb:        PPO_1323/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1323/train/loss ▂▂▂█▁▄▂▁▂▂▂
wandb: PPO_1323/train/policy_gradient_loss ▄▃▁▅▄▃▆▇█▆▆
wandb:                  PPO_1323/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1323/train/value_loss ▅▅▆█▃▅▄▁▆▃▃
wandb:                PPO_1333/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1333/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1333/rollout/ep_rew_mean ▂▁▁▁▄▄▄▅▆▆██
wandb:                   PPO_1333/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1333/train/approx_kl ▁▂▂▃▁▄▆▅█▅▂
wandb:        PPO_1333/train/clip_fraction ▃▁▃▂▃▆▇▇█▇▅
wandb:           PPO_1333/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1333/train/entropy_loss ▁▂▂▂▃▄▄▅▆▇█
wandb:   PPO_1333/train/explained_variance ▁▂▃▄▃▅▄▅▅█▅
wandb:        PPO_1333/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1333/train/loss ▃▄▁▄█▄▁█▃▁▁
wandb: PPO_1333/train/policy_gradient_loss ▄▁▂▄▃▂▂█▄▂▄
wandb:                  PPO_1333/train/std █▇▇▇▆▅▅▄▃▂▁
wandb:           PPO_1333/train/value_loss ▄▆██▇▇▇▆▂▁▅
wandb:                PPO_1343/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1343/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1343/rollout/ep_rew_mean ▁▂▃▃▄▄▄▄▅▆██
wandb:                   PPO_1343/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1343/train/approx_kl ▄▃▄▆▆▇▅▁▂▅█
wandb:        PPO_1343/train/clip_fraction ▄▄▅▃▁█▆▂▆▅▄
wandb:           PPO_1343/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1343/train/entropy_loss ▁▁▂▃▄▄▅▅▆▇█
wandb:   PPO_1343/train/explained_variance ▅▃▆█▅▆▅▅▃▃▁
wandb:        PPO_1343/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1343/train/loss █▁▁▂▂▃▃▂▂▅▆
wandb: PPO_1343/train/policy_gradient_loss ▅▄▆▅▅▅▅█▃▁█
wandb:                  PPO_1343/train/std █▇▇▆▅▅▄▃▃▂▁
wandb:           PPO_1343/train/value_loss ▅█▂▁▅▃▂▃▆▇▇
wandb:                PPO_1353/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1353/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1353/rollout/ep_rew_mean ▁▄▄▅▆▅▆▇▇█▆▆
wandb:                   PPO_1353/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1353/train/approx_kl ▆▄▃▇▃█▄▄▂▁▄
wandb:        PPO_1353/train/clip_fraction ▆▆▅▇▁▆▅█▂▅▂
wandb:           PPO_1353/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1353/train/entropy_loss ▁▂▃▄▄▅▆▆▇▇█
wandb:   PPO_1353/train/explained_variance ▇▆█▇▆▇▅▆▅▅▁
wandb:        PPO_1353/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1353/train/loss ▂▄▅▁█▅▃▂▂▁▃
wandb: PPO_1353/train/policy_gradient_loss ▁▅▃█▃█▇▆▅▆▄
wandb:                  PPO_1353/train/std █▇▆▅▅▄▃▂▂▂▁
wandb:           PPO_1353/train/value_loss ▅▆▄▁▆▄▅▂▆▇█
wandb:                PPO_1363/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1363/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1363/rollout/ep_rew_mean ▃▁▁▃▃▆█▇▄█▄▆
wandb:                   PPO_1363/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1363/train/approx_kl ▅▃▄▅▂▄█▃▁▅▂
wandb:        PPO_1363/train/clip_fraction █▇▅▇▄█▃▁▁█▄
wandb:           PPO_1363/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1363/train/entropy_loss ▁▁▁▃▃▄▄▄▅▇█
wandb:   PPO_1363/train/explained_variance ▄▅██▁██▆▅▅▇
wandb:        PPO_1363/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1363/train/loss ▂▂▁█▂▂▁▂▆▂▂
wandb: PPO_1363/train/policy_gradient_loss █▆▁▃▆██▂▄▅▃
wandb:                  PPO_1363/train/std ██▇▆▆▅▅▅▃▂▁
wandb:           PPO_1363/train/value_loss ▂▃▆▆█▁▄▂▅▂▅
wandb:                PPO_1374/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1374/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1374/rollout/ep_rew_mean ▃▁▃▄▅▄▇▇▂▆█▆
wandb:                   PPO_1374/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1374/train/approx_kl █▇▇▁▆▅▃▁▅▁▄
wandb:        PPO_1374/train/clip_fraction ▆▃█▁▇▆▄▁▁▂▅
wandb:           PPO_1374/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1374/train/entropy_loss ▁▂▃▃▄▄▄▅▆▆█
wandb:   PPO_1374/train/explained_variance ▁▃▅▂▆▅█▇▅▇▅
wandb:        PPO_1374/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1374/train/loss ▃▅▁▅▇▁▁▂█▂▂
wandb: PPO_1374/train/policy_gradient_loss ▄▁█▁▄▃▇▆▃█▂
wandb:                  PPO_1374/train/std █▇▆▆▅▅▅▄▄▃▁
wandb:           PPO_1374/train/value_loss ▁▆▁▄▃▃▁▂█▆▃
wandb:                PPO_1384/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1384/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1384/rollout/ep_rew_mean █▅▃▄▆▇█▃▁▁▅▅
wandb:                   PPO_1384/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1384/train/approx_kl ▁▅▃▄▁▃█▁▄▇▆
wandb:        PPO_1384/train/clip_fraction ▆▇▄▅▄▇▇▁▃▆█
wandb:           PPO_1384/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1384/train/entropy_loss ▁▂▃▄▄▆▆▅▅▅█
wandb:   PPO_1384/train/explained_variance ▁▄▆▆▅▆▅██▆█
wandb:        PPO_1384/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1384/train/loss ▅▁▁▄▂▇▃▁█▇▃
wandb: PPO_1384/train/policy_gradient_loss ▃▂▄▁▅▅▅▁▄▂█
wandb:                  PPO_1384/train/std █▇▆▅▄▂▃▃▅▃▁
wandb:           PPO_1384/train/value_loss ▁▂▂▃▄▂▂▅▆█▅
wandb:                    global_mean_eval ▁▂▂▄▅▆▇██▇
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▂▂▄▅▆▇▇█▆
wandb:                       mean_reward_1 ▁▂▃▄▆▆▇██▇
wandb:                      mean_reward_10 ▁▂▂▄▆▆███▇
wandb:                      mean_reward_11 ▁▂▂▄▆▆▇██▇
wandb:                      mean_reward_12 ▁▂▂▄▆▆▇██▇
wandb:                      mean_reward_13 ▁▂▂▄▅▆▇▇█▆
wandb:                      mean_reward_14 ▁▂▂▄▆▇▇██▆
wandb:                      mean_reward_15 ▁▂▃▄▆▆▇██▇
wandb:                      mean_reward_16 ▁▂▂▄▆▆███▇
wandb:                      mean_reward_17 ▁▂▂▄▅▆▇▇█▆
wandb:                      mean_reward_18 ▁▂▂▄▆▇███▆
wandb:                      mean_reward_19 ▁▂▂▄▅▆███▆
wandb:                       mean_reward_2 ▁▂▂▄▅▆▇▇█▇
wandb:                      mean_reward_20 ▁▂▂▄▆▆█▇█▇
wandb:                      mean_reward_21 ▁▂▂▄▆▆▇██▇
wandb:                      mean_reward_22 ▁▂▂▄▅▆▇▇█▇
wandb:                      mean_reward_23 ▁▂▃▅▆▆▇██▇
wandb:                      mean_reward_24 ▁▂▃▄▆▇███▇
wandb:                      mean_reward_25 ▁▂▂▄▅▆▇██▆
wandb:                      mean_reward_26 ▁▂▂▄▅▆▇▇█▆
wandb:                      mean_reward_27 ▁▂▂▄▅▆▇▇█▇
wandb:                      mean_reward_28 ▁▂▂▄▆▆▇██▇
wandb:                      mean_reward_29 ▁▂▂▄▅▆▇██▆
wandb:                       mean_reward_3 ▁▂▂▄▆▆███▇
wandb:                      mean_reward_30 ▁▂▃▄▆▆███▆
wandb:                      mean_reward_31 ▁▂▂▄▅▆███▇
wandb:                      mean_reward_32 ▁▂▂▄▅▆▇▇█▇
wandb:                      mean_reward_33 ▁▂▂▄▆▆▇██▇
wandb:                      mean_reward_34 ▁▂▂▄▅▆███▆
wandb:                      mean_reward_35 ▁▂▂▄▅▆▇██▇
wandb:                       mean_reward_4 ▁▂▂▄▅▆▇▇█▇
wandb:                       mean_reward_5 ▁▂▂▄▆▇███▇
wandb:                       mean_reward_6 ▁▂▂▄▅▆▇▇█▆
wandb:                       mean_reward_7 ▁▂▃▄▆▆▇██▆
wandb:                       mean_reward_8 ▁▂▂▄▆▆▇██▇
wandb:                       mean_reward_9 ▁▂▂▄▅▆███▇
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▂▂▂▃▅▃▆▇▇▇█▆
wandb:                        std_reward_0 ▃▁▁▁▁▂▂▁▃█
wandb:                        std_reward_1 ▄▁▁▁▁▂▂▁▃█
wandb:                       std_reward_10 ▄▁▂▁▁▂▃▁▃█
wandb:                       std_reward_11 ▃▁▁▁▁▂▃▁▃█
wandb:                       std_reward_12 ▃▁▁▁▁▂▃▁▃█
wandb:                       std_reward_13 ▃▁▂▁▁▂▃▁▃█
wandb:                       std_reward_14 ▃▁▁▁▁▂▂▁▂█
wandb:                       std_reward_15 ▄▁▂▁▁▂▂▁▂█
wandb:                       std_reward_16 ▃▁▁▁▁▂▂▁▂█
wandb:                       std_reward_17 ▄▂▁▁▁▂▂▁▃█
wandb:                       std_reward_18 ▃▁▁▁▁▂▂▁▂█
wandb:                       std_reward_19 ▄▁▂▂▁▂▂▁▄█
wandb:                        std_reward_2 ▅▁▁▁▁▃▃▁▃█
wandb:                       std_reward_20 ▄▁▁▁▁▂▃▁▃█
wandb:                       std_reward_21 ▃▁▁▁▁▂▃▁▃█
wandb:                       std_reward_22 ▄▁▁▁▁▂▃▁▃█
wandb:                       std_reward_23 ▄▁▁▁▁▂▂▁▃█
wandb:                       std_reward_24 ▅▁▂▁▁▂▃▁▃█
wandb:                       std_reward_25 ▃▁▁▁▁▂▂▁▃█
wandb:                       std_reward_26 ▃▁▁▁▁▂▂▁▃█
wandb:                       std_reward_27 ▄▁▁▁▁▂▂▁▃█
wandb:                       std_reward_28 ▄▁▁▁▁▂▂▁▃█
wandb:                       std_reward_29 ▃▁▁▁▁▂▂▁▃█
wandb:                        std_reward_3 ▄▁▁▁▁▂▃▁▃█
wandb:                       std_reward_30 ▄▁▁▁▁▂▂▁▃█
wandb:                       std_reward_31 ▄▁▁▂▁▃▃▁▃█
wandb:                       std_reward_32 ▄▂▁▁▁▂▂▁▃█
wandb:                       std_reward_33 ▄▁▁▁▁▂▃▁▃█
wandb:                       std_reward_34 ▃▁▁▁▁▂▂▁▂█
wandb:                       std_reward_35 ▃▁▁▁▁▂▃▁▃█
wandb:                        std_reward_4 ▄▁▁▁▁▂▃▁▃█
wandb:                        std_reward_5 ▄▁▁▁▁▂▃▁▃█
wandb:                        std_reward_6 ▃▁▁▁▁▂▂▁▂█
wandb:                        std_reward_7 ▃▁▁▁▁▁▂▁▂█
wandb:                        std_reward_8 ▄▁▁▁▁▂▂▁▃█
wandb:                        std_reward_9 ▄▁▁▁▁▂▃▁▃█
wandb:                            time/fps █▄▃▂▂▂▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▃▂▃█▇▂▁▃▃▄▄▅
wandb:                 train/clip_fraction ▅▄▄▅▅▂▁▅▆█▇▇
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▁▂▃▃▄▄▅▆▇▇█
wandb:            train/explained_variance ▁▁▁▁▁▁▄▇████
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▆▄▃▃▂▁▁▁▁▂▁
wandb:          train/policy_gradient_loss ▆▆▆▆██▇▃▃▁▃▃
wandb:                           train/std █▇▇▆▅▅▄▃▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1304/global_step 212992
wandb:        PPO_1304/rollout/ep_len_mean 200.0
wandb:        PPO_1304/rollout/ep_rew_mean -825.61829
wandb:                   PPO_1304/time/fps 590.0
wandb:            PPO_1304/train/approx_kl 0.01303
wandb:        PPO_1304/train/clip_fraction 0.15875
wandb:           PPO_1304/train/clip_range 0.2
wandb:         PPO_1304/train/entropy_loss -7.67415
wandb:   PPO_1304/train/explained_variance 0.95915
wandb:        PPO_1304/train/learning_rate 0.0003
wandb:                 PPO_1304/train/loss 25.13453
wandb: PPO_1304/train/policy_gradient_loss -0.00829
wandb:                  PPO_1304/train/std 0.72256
wandb:           PPO_1304/train/value_loss 90.12502
wandb:                PPO_1313/global_step 212992
wandb:        PPO_1313/rollout/ep_len_mean 200.0
wandb:        PPO_1313/rollout/ep_rew_mean -781.992
wandb:                   PPO_1313/time/fps 594.0
wandb:            PPO_1313/train/approx_kl 0.01517
wandb:        PPO_1313/train/clip_fraction 0.19214
wandb:           PPO_1313/train/clip_range 0.2
wandb:         PPO_1313/train/entropy_loss -6.8544
wandb:   PPO_1313/train/explained_variance 0.9694
wandb:        PPO_1313/train/learning_rate 0.0003
wandb:                 PPO_1313/train/loss 34.24713
wandb: PPO_1313/train/policy_gradient_loss -0.00699
wandb:                  PPO_1313/train/std 0.64396
wandb:           PPO_1313/train/value_loss 45.7892
wandb:                PPO_1323/global_step 212992
wandb:        PPO_1323/rollout/ep_len_mean 200.0
wandb:        PPO_1323/rollout/ep_rew_mean -683.82178
wandb:                   PPO_1323/time/fps 595.0
wandb:            PPO_1323/train/approx_kl 0.01802
wandb:        PPO_1323/train/clip_fraction 0.21071
wandb:           PPO_1323/train/clip_range 0.2
wandb:         PPO_1323/train/entropy_loss -6.00281
wandb:   PPO_1323/train/explained_variance 0.98076
wandb:        PPO_1323/train/learning_rate 0.0003
wandb:                 PPO_1323/train/loss 14.66159
wandb: PPO_1323/train/policy_gradient_loss -0.0043
wandb:                  PPO_1323/train/std 0.57005
wandb:           PPO_1323/train/value_loss 36.12022
wandb:                PPO_1333/global_step 212992
wandb:        PPO_1333/rollout/ep_len_mean 200.0
wandb:        PPO_1333/rollout/ep_rew_mean -612.38165
wandb:                   PPO_1333/time/fps 594.0
wandb:            PPO_1333/train/approx_kl 0.01716
wandb:        PPO_1333/train/clip_fraction 0.2358
wandb:           PPO_1333/train/clip_range 0.2
wandb:         PPO_1333/train/entropy_loss -5.13992
wandb:   PPO_1333/train/explained_variance 0.97937
wandb:        PPO_1333/train/learning_rate 0.0003
wandb:                 PPO_1333/train/loss 9.9293
wandb: PPO_1333/train/policy_gradient_loss -0.00156
wandb:                  PPO_1333/train/std 0.50512
wandb:           PPO_1333/train/value_loss 35.01083
wandb:                PPO_1343/global_step 212992
wandb:        PPO_1343/rollout/ep_len_mean 200.0
wandb:        PPO_1343/rollout/ep_rew_mean -545.94513
wandb:                   PPO_1343/time/fps 593.0
wandb:            PPO_1343/train/approx_kl 0.02241
wandb:        PPO_1343/train/clip_fraction 0.24805
wandb:           PPO_1343/train/clip_range 0.2
wandb:         PPO_1343/train/entropy_loss -4.2777
wandb:   PPO_1343/train/explained_variance 0.97935
wandb:        PPO_1343/train/learning_rate 0.0003
wandb:                 PPO_1343/train/loss 15.17304
wandb: PPO_1343/train/policy_gradient_loss 0.00031
wandb:                  PPO_1343/train/std 0.44535
wandb:           PPO_1343/train/value_loss 24.3054
wandb:                PPO_1353/global_step 212992
wandb:        PPO_1353/rollout/ep_len_mean 200.0
wandb:        PPO_1353/rollout/ep_rew_mean -520.36609
wandb:                   PPO_1353/time/fps 592.0
wandb:            PPO_1353/train/approx_kl 0.01942
wandb:        PPO_1353/train/clip_fraction 0.23073
wandb:           PPO_1353/train/clip_range 0.2
wandb:         PPO_1353/train/entropy_loss -3.59917
wandb:   PPO_1353/train/explained_variance 0.95691
wandb:        PPO_1353/train/learning_rate 0.0003
wandb:                 PPO_1353/train/loss 15.02359
wandb: PPO_1353/train/policy_gradient_loss -0.00049
wandb:                  PPO_1353/train/std 0.40558
wandb:           PPO_1353/train/value_loss 44.73174
wandb:                PPO_1363/global_step 212992
wandb:        PPO_1363/rollout/ep_len_mean 200.0
wandb:        PPO_1363/rollout/ep_rew_mean -496.90869
wandb:                   PPO_1363/time/fps 592.0
wandb:            PPO_1363/train/approx_kl 0.01688
wandb:        PPO_1363/train/clip_fraction 0.24883
wandb:           PPO_1363/train/clip_range 0.2
wandb:         PPO_1363/train/entropy_loss -3.2356
wandb:   PPO_1363/train/explained_variance 0.97046
wandb:        PPO_1363/train/learning_rate 0.0003
wandb:                 PPO_1363/train/loss 15.45499
wandb: PPO_1363/train/policy_gradient_loss 6e-05
wandb:                  PPO_1363/train/std 0.38502
wandb:           PPO_1363/train/value_loss 38.0761
wandb:                PPO_1374/global_step 212992
wandb:        PPO_1374/rollout/ep_len_mean 200.0
wandb:        PPO_1374/rollout/ep_rew_mean -485.74792
wandb:                   PPO_1374/time/fps 592.0
wandb:            PPO_1374/train/approx_kl 0.0179
wandb:        PPO_1374/train/clip_fraction 0.24866
wandb:           PPO_1374/train/clip_range 0.2
wandb:         PPO_1374/train/entropy_loss -2.89897
wandb:   PPO_1374/train/explained_variance 0.97372
wandb:        PPO_1374/train/learning_rate 0.0003
wandb:                 PPO_1374/train/loss 19.54455
wandb: PPO_1374/train/policy_gradient_loss -0.00219
wandb:                  PPO_1374/train/std 0.3668
wandb:           PPO_1374/train/value_loss 46.46445
wandb:                PPO_1384/global_step 212992
wandb:        PPO_1384/rollout/ep_len_mean 200.0
wandb:        PPO_1384/rollout/ep_rew_mean -480.82297
wandb:                   PPO_1384/time/fps 591.0
wandb:            PPO_1384/train/approx_kl 0.01931
wandb:        PPO_1384/train/clip_fraction 0.25494
wandb:           PPO_1384/train/clip_range 0.2
wandb:         PPO_1384/train/entropy_loss -2.75321
wandb:   PPO_1384/train/explained_variance 0.98528
wandb:        PPO_1384/train/learning_rate 0.0003
wandb:                 PPO_1384/train/loss 26.17315
wandb: PPO_1384/train/policy_gradient_loss 0.00173
wandb:                  PPO_1384/train/std 0.35956
wandb:           PPO_1384/train/value_loss 108.80746
wandb:                    global_mean_eval -507.78991
wandb:                         global_step 212992
wandb:                       mean_reward_0 -511.51568
wandb:                       mean_reward_1 -497.87986
wandb:                      mean_reward_10 -510.46751
wandb:                      mean_reward_11 -502.57238
wandb:                      mean_reward_12 -501.22193
wandb:                      mean_reward_13 -509.67781
wandb:                      mean_reward_14 -535.44281
wandb:                      mean_reward_15 -502.0353
wandb:                      mean_reward_16 -516.18479
wandb:                      mean_reward_17 -511.6722
wandb:                      mean_reward_18 -526.46983
wandb:                      mean_reward_19 -528.06455
wandb:                       mean_reward_2 -490.37684
wandb:                      mean_reward_20 -503.54361
wandb:                      mean_reward_21 -506.29872
wandb:                      mean_reward_22 -482.59067
wandb:                      mean_reward_23 -512.90185
wandb:                      mean_reward_24 -496.60101
wandb:                      mean_reward_25 -532.98952
wandb:                      mean_reward_26 -522.12761
wandb:                      mean_reward_27 -502.5623
wandb:                      mean_reward_28 -487.95103
wandb:                      mean_reward_29 -518.99264
wandb:                       mean_reward_3 -491.63877
wandb:                      mean_reward_30 -527.68284
wandb:                      mean_reward_31 -495.30967
wandb:                      mean_reward_32 -491.83305
wandb:                      mean_reward_33 -485.17886
wandb:                      mean_reward_34 -518.85056
wandb:                      mean_reward_35 -492.18701
wandb:                       mean_reward_4 -503.22699
wandb:                       mean_reward_5 -499.31828
wandb:                       mean_reward_6 -533.75959
wandb:                       mean_reward_7 -549.54032
wandb:                       mean_reward_8 -492.72774
wandb:                       mean_reward_9 -489.04267
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -939.39148
wandb:                        std_reward_0 174.79256
wandb:                        std_reward_1 146.00595
wandb:                       std_reward_10 155.32738
wandb:                       std_reward_11 148.84504
wandb:                       std_reward_12 166.53231
wandb:                       std_reward_13 164.60903
wandb:                       std_reward_14 180.57267
wandb:                       std_reward_15 153.83455
wandb:                       std_reward_16 174.26685
wandb:                       std_reward_17 159.04655
wandb:                       std_reward_18 178.26579
wandb:                       std_reward_19 172.85496
wandb:                        std_reward_2 135.96934
wandb:                       std_reward_20 149.56059
wandb:                       std_reward_21 170.08227
wandb:                       std_reward_22 138.37993
wandb:                       std_reward_23 163.5463
wandb:                       std_reward_24 142.14255
wandb:                       std_reward_25 179.01717
wandb:                       std_reward_26 175.61365
wandb:                       std_reward_27 159.91723
wandb:                       std_reward_28 134.26925
wandb:                       std_reward_29 158.92809
wandb:                        std_reward_3 143.88105
wandb:                       std_reward_30 177.4682
wandb:                       std_reward_31 137.30183
wandb:                       std_reward_32 134.93941
wandb:                       std_reward_33 138.96079
wandb:                       std_reward_34 165.73718
wandb:                       std_reward_35 149.30883
wandb:                        std_reward_4 154.18777
wandb:                        std_reward_5 143.13039
wandb:                        std_reward_6 182.92919
wandb:                        std_reward_7 188.95228
wandb:                        std_reward_8 145.0767
wandb:                        std_reward_9 149.14571
wandb:                            time/fps 600.0
wandb:                     train/approx_kl 0.01128
wandb:                 train/clip_fraction 0.12654
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.7942
wandb:            train/explained_variance 0.92484
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 12.3112
wandb:          train/policy_gradient_loss -0.0089
wandb:                           train/std 0.84816
wandb:                    train/value_loss 56.39774
wandb: 
wandb: Synced stellar-yogurt-48: https://wandb.ai/tidiane/meta_rl_context/runs/tyuqum18
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 14 other file(s)
wandb: Find logs at: ./wandb/run-20230626_052650-tyuqum18/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1305/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1305/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1305/rollout/ep_rew_mean ▁▂▂▃▃▄▄▅▅▆▆█
wandb:                   PPO_1305/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1305/train/approx_kl ▁▂▂▁▃▅▁▆█▄▁
wandb:        PPO_1305/train/clip_fraction ▃▄▅▁▄▄▅██▅▄
wandb:           PPO_1305/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1305/train/entropy_loss ▁▂▃▄▄▅▅▆▇▇█
wandb:   PPO_1305/train/explained_variance ▅▅█▇▅▅▇▂▅▁▁
wandb:        PPO_1305/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1305/train/loss ▁▁▁█▃▃▂▂▃▇▅
wandb: PPO_1305/train/policy_gradient_loss ▃▄▅▇▇▆▇▁▂▅█
wandb:                  PPO_1305/train/std █▇▆▅▅▄▄▃▂▂▁
wandb:           PPO_1305/train/value_loss ▂▂▁▄▃▄▅▆▅█▇
wandb:                PPO_1315/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1315/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1315/rollout/ep_rew_mean ▁▂▃▃▅▅▆▇▇▇▇█
wandb:                   PPO_1315/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1315/train/approx_kl ▆▇▁▄█▅▄▆▄▄▅
wandb:        PPO_1315/train/clip_fraction ▄▆▂█▆▄▇▄▆▁▆
wandb:           PPO_1315/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1315/train/entropy_loss ▁▂▂▃▃▄▅▅▆▇█
wandb:   PPO_1315/train/explained_variance ▃▂▅▁▂▅▅▃▆▃█
wandb:        PPO_1315/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1315/train/loss █▇▃▄▁▃▂▆▁▃▂
wandb: PPO_1315/train/policy_gradient_loss ▂▃▃▁▂▃▅▆▇▇█
wandb:                  PPO_1315/train/std █▇▇▆▆▅▄▄▃▂▁
wandb:           PPO_1315/train/value_loss █▅▅▅▄▄▁▃▁▃▂
wandb:                PPO_1325/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1325/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1325/rollout/ep_rew_mean ▅▅▅▆█▆▅▅▄▁▃▄
wandb:                   PPO_1325/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1325/train/approx_kl ▅█▅▇▇▄▄▇▂▁▃
wandb:        PPO_1325/train/clip_fraction ▅█▅▇▄▆▆▄▂▁▄
wandb:           PPO_1325/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1325/train/entropy_loss ▁▂▃▃▄▅▆▆▇▇█
wandb:   PPO_1325/train/explained_variance ▄█▆▆▅▆▄▁▆▇▆
wandb:        PPO_1325/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1325/train/loss ▂▁▂▁█▂▅▄▆▇█
wandb: PPO_1325/train/policy_gradient_loss ▁▂▂▁▂▄▃▄█▆▄
wandb:                  PPO_1325/train/std █▇▆▅▅▄▃▃▂▂▁
wandb:           PPO_1325/train/value_loss ▂▁▂▂▂▃▅▅▇██
wandb:                PPO_1335/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1335/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1335/rollout/ep_rew_mean ▄▄▃▄▆▅█▇▁▅▄▄
wandb:                   PPO_1335/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1335/train/approx_kl ▁▄▃▄▃▄█▂▅▄▂
wandb:        PPO_1335/train/clip_fraction ▃▃▁▅▄▄█▁▂▄▂
wandb:           PPO_1335/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1335/train/entropy_loss ▁▁▂▃▄▄▅▅▆▆█
wandb:   PPO_1335/train/explained_variance ▁▅▄█▅▅▂▄▄▆█
wandb:        PPO_1335/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1335/train/loss ▂█▄▁▅▆▂▃▇▃█
wandb: PPO_1335/train/policy_gradient_loss ▅▇▇▂▆▅▇█▁▂▆
wandb:                  PPO_1335/train/std █▇▆▆▅▅▄▄▃▃▁
wandb:           PPO_1335/train/value_loss ▄▄▆▄▅▇▁▄█▆█
wandb:                PPO_1345/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1345/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1345/rollout/ep_rew_mean ▅▄▁▄▅█▃▂▅▅▃▁
wandb:                   PPO_1345/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1345/train/approx_kl ▁▆▃▄▇█▂▅▁▁▇
wandb:        PPO_1345/train/clip_fraction ▄▇▂▂█▇▄▁▂▂▇
wandb:           PPO_1345/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1345/train/entropy_loss ▁▂▃▄▄▅▅▆▇██
wandb:   PPO_1345/train/explained_variance ▃▆█▅▂▃▄▃▂▄▁
wandb:        PPO_1345/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1345/train/loss ▃▃▆▃▂▁▆▇█▆▄
wandb: PPO_1345/train/policy_gradient_loss ▄▃▅▂▆█▃▅▁▅▆
wandb:                  PPO_1345/train/std █▆▆▅▄▄▄▂▂▂▁
wandb:           PPO_1345/train/value_loss ▃▁▄▅▃▂▅██▅▃
wandb:                PPO_1355/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1355/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1355/rollout/ep_rew_mean ▁▅▃▇▅▇█▆▅█▄▆
wandb:                   PPO_1355/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1355/train/approx_kl ▁▄▂▂▅▅▄▄▅▃█
wandb:        PPO_1355/train/clip_fraction ▁▄▂▃▄▇█▃▃▄▇
wandb:           PPO_1355/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1355/train/entropy_loss ▁▁▁▃▃▄▅▅▆▇█
wandb:   PPO_1355/train/explained_variance ▆█▇▆▅▆▁▇█▄▇
wandb:        PPO_1355/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1355/train/loss ▂▂▁▁▃▁▁▁█▂▁
wandb: PPO_1355/train/policy_gradient_loss ▅▄▁▄▄▂█▃▆▂█
wandb:                  PPO_1355/train/std ██▇▆▆▅▄▄▃▂▁
wandb:           PPO_1355/train/value_loss ▅▃█▅█▃▄▅▃▄▁
wandb:                PPO_1365/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1365/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1365/rollout/ep_rew_mean ▄▃▃▁▃▅▁▇█▃▂▃
wandb:                   PPO_1365/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1365/train/approx_kl ▁▆▄▆▄▆▅▅█▁▅
wandb:        PPO_1365/train/clip_fraction ▂▆▄▅▃▃▃▅█▁▅
wandb:           PPO_1365/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1365/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1365/train/explained_variance ▅▆▁▇▇▃███▅▇
wandb:        PPO_1365/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1365/train/loss ▁▂▅▂▂██▂▇▂▄
wandb: PPO_1365/train/policy_gradient_loss ▄▃▅▄▃▅▂▂█▁▆
wandb:                  PPO_1365/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1365/train/value_loss ▅▃█▃▄▅▅▃▁█▂
wandb:                PPO_1375/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1375/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1375/rollout/ep_rew_mean ▆▃▆██▅▇▅▄▁▆▆
wandb:                   PPO_1375/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1375/train/approx_kl ▄▃▁█▃▃▂▆▂▁▂
wandb:        PPO_1375/train/clip_fraction ▅▅▄▇▆▂▅█▂▁▃
wandb:           PPO_1375/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1375/train/entropy_loss ▁▂▃▄▄▅▅▅▆▇█
wandb:   PPO_1375/train/explained_variance ▃▆▁▆█▃▄▅▃▆▇
wandb:        PPO_1375/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1375/train/loss ▂▆█▃█▁▂▃▅▄▆
wandb: PPO_1375/train/policy_gradient_loss ▇▅▆█▆▄▃▆▁▂▆
wandb:                  PPO_1375/train/std █▇▆▅▄▄▄▃▃▂▁
wandb:           PPO_1375/train/value_loss ▄▆▆▁▂▆▅▃▆█▄
wandb:                PPO_1385/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1385/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1385/rollout/ep_rew_mean ▇▅▂▅▇▆▇▆▁▅█▅
wandb:                   PPO_1385/time/fps █▄▃▂▂▁▁▁▁▁▁▁
wandb:            PPO_1385/train/approx_kl ▁▂▂▅▁▄▄▄▂▄█
wandb:        PPO_1385/train/clip_fraction ▁▂▁▂▃▅▄▂▁▄█
wandb:           PPO_1385/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1385/train/entropy_loss ▁▂▃▄▄▅▅▆▆▇█
wandb:   PPO_1385/train/explained_variance ▇▇▇▆▄▁▆▄█▇▆
wandb:        PPO_1385/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1385/train/loss ▂█▁▁▁▂▅▂▆▃▃
wandb: PPO_1385/train/policy_gradient_loss ▂▃▃▄▃▁▅▃▂▁█
wandb:                  PPO_1385/train/std █▇▆▅▄▄▄▃▂▂▁
wandb:           PPO_1385/train/value_loss ▆▅█▇▇▆▆██▅▁
wandb:                    global_mean_eval ▁▅█▇▆▆▆▇▇█
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▆█▇▆▅▇▇▇▇
wandb:                       mean_reward_1 ▁▅█▇▅▅▆█▇█
wandb:                      mean_reward_10 ▁▅█▇▆▇▆▆▇█
wandb:                      mean_reward_11 ▁▅██▆▆▅▇▆▇
wandb:                      mean_reward_12 ▁▆██▅▅▅▇██
wandb:                      mean_reward_13 ▁▅▇▆▆▆▇▇▇█
wandb:                      mean_reward_14 ▁▅█▇▅▆▆▇▆▇
wandb:                      mean_reward_15 ▁▅█▆▆▅▆▇▇█
wandb:                      mean_reward_16 ▁▅█▆▅▅▇▇▇█
wandb:                      mean_reward_17 ▁▅█▇▆▆▅▇▇▇
wandb:                      mean_reward_18 ▁▅█▆▆▅▆█▇▇
wandb:                      mean_reward_19 ▁▅█▇▅▆▆▆▇▇
wandb:                       mean_reward_2 ▁▅▇▆▄▅▆▆▇█
wandb:                      mean_reward_20 ▁▅██▅▅▇▇▇█
wandb:                      mean_reward_21 ▁▅█▇▆▇▅▆█▇
wandb:                      mean_reward_22 ▁▅█▆▅▆▇██▇
wandb:                      mean_reward_23 ▁▅█▇▆▄▆▇█▇
wandb:                      mean_reward_24 ▁▅█▇▅▅▇▇█▇
wandb:                      mean_reward_25 ▁▅█▇▆▅▆▆▇█
wandb:                      mean_reward_26 ▁▅█▆▅▆▆▆▇█
wandb:                      mean_reward_27 ▁▅█▆▆▅▆▇▇█
wandb:                      mean_reward_28 ▁▅█▆▆▅▆███
wandb:                      mean_reward_29 ▁▅█▇▆▇▇▇▇█
wandb:                       mean_reward_3 ▁▅█▇▅▄▆█▇█
wandb:                      mean_reward_30 ▁▅█▇▅▄▆▇█▇
wandb:                      mean_reward_31 ▁▆█▇▆▅▇███
wandb:                      mean_reward_32 ▁▅██▄▆▆▆▇▇
wandb:                      mean_reward_33 ▁▅█▇▇▅▅██▇
wandb:                      mean_reward_34 ▁▆██▅▆▇▆▇█
wandb:                      mean_reward_35 ▁▅▇▆▅▅▇▆▇█
wandb:                       mean_reward_4 ▁▅██▅▅▆█▇█
wandb:                       mean_reward_5 ▁▅██▆▇▆▇▇█
wandb:                       mean_reward_6 ▁▅█▇▄▆▅▇█▇
wandb:                       mean_reward_7 ▁▅█▆▅▆▇▆▇▇
wandb:                       mean_reward_8 ▁▅██▅▅█▇██
wandb:                       mean_reward_9 ▁▅█▅▇▇▅▇▇█
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▃▁▂▂▃▄▄▅▆█
wandb:                        std_reward_0 ▂▁▃▆▇█▆▆▅▄
wandb:                        std_reward_1 ▂▁▂▅██▆▄▅▄
wandb:                       std_reward_10 ▂▁▃▆█▇▇▆▅▄
wandb:                       std_reward_11 ▂▁▂▅█▇█▆▆▅
wandb:                       std_reward_12 ▂▁▂▅█▇▇▅▄▄
wandb:                       std_reward_13 ▂▁▃███▆▅▅▄
wandb:                       std_reward_14 ▂▁▃▆██▇▅▆▅
wandb:                       std_reward_15 ▂▁▃▆██▇▅▅▄
wandb:                       std_reward_16 ▂▁▃▆██▇▆▅▄
wandb:                       std_reward_17 ▂▁▂▆█▇█▆▅▄
wandb:                       std_reward_18 ▂▁▂▇▇█▇▅▅▄
wandb:                       std_reward_19 ▂▁▂▆█▇▇▅▅▄
wandb:                        std_reward_2 ▂▁▂▇██▆▅▄▃
wandb:                       std_reward_20 ▂▁▂▅██▆▅▅▄
wandb:                       std_reward_21 ▂▁▃▇█▇█▇▄▅
wandb:                       std_reward_22 ▂▁▃▇█▇▆▄▅▅
wandb:                       std_reward_23 ▂▁▃▆▇█▇▅▅▄
wandb:                       std_reward_24 ▂▁▃▆██▆▅▅▅
wandb:                       std_reward_25 ▂▁▂▆▇█▆▅▅▃
wandb:                       std_reward_26 ▂▁▃▇██▇▆▆▄
wandb:                       std_reward_27 ▂▁▃▇▇█▇▅▆▄
wandb:                       std_reward_28 ▂▁▃▇██▇▅▄▄
wandb:                       std_reward_29 ▂▁▃▇█▇▇▆▅▄
wandb:                        std_reward_3 ▂▁▂▆██▇▅▅▄
wandb:                       std_reward_30 ▂▁▂▆██▇▅▄▄
wandb:                       std_reward_31 ▂▁▃▆██▆▅▅▅
wandb:                       std_reward_32 ▂▁▂▅█▆▆▅▄▃
wandb:                       std_reward_33 ▂▁▂▇▇██▅▅▅
wandb:                       std_reward_34 ▂▁▂▅██▇▆▅▄
wandb:                       std_reward_35 ▂▁▂▆██▆▆▅▃
wandb:                        std_reward_4 ▂▁▃▅██▇▅▅▄
wandb:                        std_reward_5 ▂▁▂▆█▇█▅▅▄
wandb:                        std_reward_6 ▂▁▂▅█▆▇▅▄▄
wandb:                        std_reward_7 ▂▁▂▆█▇▆▅▄▄
wandb:                        std_reward_8 ▂▁▃▆██▆▅▄▄
wandb:                        std_reward_9 ▂▁▃▇▇▇█▅▅▄
wandb:                            time/fps █▄▃▂▂▂▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▄█▇▆▄▃▁▂▄▄
wandb:                 train/clip_fraction ▂▃▃▄▇▂▁▂▁▂▅█
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▁▁▁▁▆██
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▄▃▂▂▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▄▅▅█▆▇▆▅▃▂▁
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1305/global_step 212992
wandb:        PPO_1305/rollout/ep_len_mean 200.0
wandb:        PPO_1305/rollout/ep_rew_mean -754.05072
wandb:                   PPO_1305/time/fps 585.0
wandb:            PPO_1305/train/approx_kl 0.01095
wandb:        PPO_1305/train/clip_fraction 0.14146
wandb:           PPO_1305/train/clip_range 0.2
wandb:         PPO_1305/train/entropy_loss -7.84264
wandb:   PPO_1305/train/explained_variance 0.95934
wandb:        PPO_1305/train/learning_rate 0.0003
wandb:                 PPO_1305/train/loss 64.63321
wandb: PPO_1305/train/policy_gradient_loss -0.00728
wandb:                  PPO_1305/train/std 0.74042
wandb:           PPO_1305/train/value_loss 119.40968
wandb:                PPO_1315/global_step 212992
wandb:        PPO_1315/rollout/ep_len_mean 200.0
wandb:        PPO_1315/rollout/ep_rew_mean -570.17004
wandb:                   PPO_1315/time/fps 584.0
wandb:            PPO_1315/train/approx_kl 0.0133
wandb:        PPO_1315/train/clip_fraction 0.18489
wandb:           PPO_1315/train/clip_range 0.2
wandb:         PPO_1315/train/entropy_loss -6.91847
wandb:   PPO_1315/train/explained_variance 0.9743
wandb:        PPO_1315/train/learning_rate 0.0003
wandb:                 PPO_1315/train/loss 32.27102
wandb: PPO_1315/train/policy_gradient_loss -0.00325
wandb:                  PPO_1315/train/std 0.64978
wandb:           PPO_1315/train/value_loss 62.22541
wandb:                PPO_1325/global_step 212992
wandb:        PPO_1325/rollout/ep_len_mean 200.0
wandb:        PPO_1325/rollout/ep_rew_mean -570.44965
wandb:                   PPO_1325/time/fps 581.0
wandb:            PPO_1325/train/approx_kl 0.01123
wandb:        PPO_1325/train/clip_fraction 0.15079
wandb:           PPO_1325/train/clip_range 0.2
wandb:         PPO_1325/train/entropy_loss -6.39968
wandb:   PPO_1325/train/explained_variance 0.96527
wandb:        PPO_1325/train/learning_rate 0.0003
wandb:                 PPO_1325/train/loss 185.74689
wandb: PPO_1325/train/policy_gradient_loss -0.00286
wandb:                  PPO_1325/train/std 0.60361
wandb:           PPO_1325/train/value_loss 359.63986
wandb:                PPO_1335/global_step 212992
wandb:        PPO_1335/rollout/ep_len_mean 200.0
wandb:        PPO_1335/rollout/ep_rew_mean -590.05994
wandb:                   PPO_1335/time/fps 581.0
wandb:            PPO_1335/train/approx_kl 0.01018
wandb:        PPO_1335/train/clip_fraction 0.13958
wandb:           PPO_1335/train/clip_range 0.2
wandb:         PPO_1335/train/entropy_loss -5.94907
wandb:   PPO_1335/train/explained_variance 0.98168
wandb:        PPO_1335/train/learning_rate 0.0003
wandb:                 PPO_1335/train/loss 342.42581
wandb: PPO_1335/train/policy_gradient_loss -0.0029
wandb:                  PPO_1335/train/std 0.56392
wandb:           PPO_1335/train/value_loss 624.52924
wandb:                PPO_1345/global_step 212992
wandb:        PPO_1345/rollout/ep_len_mean 200.0
wandb:        PPO_1345/rollout/ep_rew_mean -667.07281
wandb:                   PPO_1345/time/fps 582.0
wandb:            PPO_1345/train/approx_kl 0.01255
wandb:        PPO_1345/train/clip_fraction 0.16456
wandb:           PPO_1345/train/clip_range 0.2
wandb:         PPO_1345/train/entropy_loss -5.45646
wandb:   PPO_1345/train/explained_variance 0.97724
wandb:        PPO_1345/train/learning_rate 0.0003
wandb:                 PPO_1345/train/loss 235.27417
wandb: PPO_1345/train/policy_gradient_loss -0.00265
wandb:                  PPO_1345/train/std 0.52709
wandb:           PPO_1345/train/value_loss 561.5968
wandb:                PPO_1355/global_step 212992
wandb:        PPO_1355/rollout/ep_len_mean 200.0
wandb:        PPO_1355/rollout/ep_rew_mean -606.81573
wandb:                   PPO_1355/time/fps 581.0
wandb:            PPO_1355/train/approx_kl 0.01235
wandb:        PPO_1355/train/clip_fraction 0.16294
wandb:           PPO_1355/train/clip_range 0.2
wandb:         PPO_1355/train/entropy_loss -5.07466
wandb:   PPO_1355/train/explained_variance 0.98407
wandb:        PPO_1355/train/learning_rate 0.0003
wandb:                 PPO_1355/train/loss 156.7691
wandb: PPO_1355/train/policy_gradient_loss -0.0011
wandb:                  PPO_1355/train/std 0.49843
wandb:           PPO_1355/train/value_loss 550.8316
wandb:                PPO_1365/global_step 212992
wandb:        PPO_1365/rollout/ep_len_mean 200.0
wandb:        PPO_1365/rollout/ep_rew_mean -594.99756
wandb:                   PPO_1365/time/fps 581.0
wandb:            PPO_1365/train/approx_kl 0.0116
wandb:        PPO_1365/train/clip_fraction 0.15532
wandb:           PPO_1365/train/clip_range 0.2
wandb:         PPO_1365/train/entropy_loss -4.76682
wandb:   PPO_1365/train/explained_variance 0.98298
wandb:        PPO_1365/train/learning_rate 0.0003
wandb:                 PPO_1365/train/loss 458.62384
wandb: PPO_1365/train/policy_gradient_loss -0.00141
wandb:                  PPO_1365/train/std 0.47891
wandb:           PPO_1365/train/value_loss 581.40631
wandb:                PPO_1375/global_step 212992
wandb:        PPO_1375/rollout/ep_len_mean 200.0
wandb:        PPO_1375/rollout/ep_rew_mean -572.96521
wandb:                   PPO_1375/time/fps 581.0
wandb:            PPO_1375/train/approx_kl 0.01151
wandb:        PPO_1375/train/clip_fraction 0.1495
wandb:           PPO_1375/train/clip_range 0.2
wandb:         PPO_1375/train/entropy_loss -4.41916
wandb:   PPO_1375/train/explained_variance 0.98357
wandb:        PPO_1375/train/learning_rate 0.0003
wandb:                 PPO_1375/train/loss 277.89746
wandb: PPO_1375/train/policy_gradient_loss -0.00022
wandb:                  PPO_1375/train/std 0.45546
wandb:           PPO_1375/train/value_loss 592.44708
wandb:                PPO_1385/global_step 212992
wandb:        PPO_1385/rollout/ep_len_mean 200.0
wandb:        PPO_1385/rollout/ep_rew_mean -563.78027
wandb:                   PPO_1385/time/fps 579.0
wandb:            PPO_1385/train/approx_kl 0.01688
wandb:        PPO_1385/train/clip_fraction 0.209
wandb:           PPO_1385/train/clip_range 0.2
wandb:         PPO_1385/train/entropy_loss -4.112
wandb:   PPO_1385/train/explained_variance 0.98328
wandb:        PPO_1385/train/learning_rate 0.0003
wandb:                 PPO_1385/train/loss 334.57947
wandb: PPO_1385/train/policy_gradient_loss 0.00207
wandb:                  PPO_1385/train/std 0.43632
wandb:           PPO_1385/train/value_loss 358.54614
wandb:                    global_mean_eval -489.50983
wandb:                         global_step 212992
wandb:                       mean_reward_0 -514.48819
wandb:                       mean_reward_1 -481.04265
wandb:                      mean_reward_10 -497.39665
wandb:                      mean_reward_11 -494.10886
wandb:                      mean_reward_12 -496.71401
wandb:                      mean_reward_13 -448.92244
wandb:                      mean_reward_14 -492.59468
wandb:                      mean_reward_15 -463.86185
wandb:                      mean_reward_16 -472.57368
wandb:                      mean_reward_17 -509.47084
wandb:                      mean_reward_18 -532.46186
wandb:                      mean_reward_19 -506.57901
wandb:                       mean_reward_2 -444.46261
wandb:                      mean_reward_20 -487.11064
wandb:                      mean_reward_21 -488.95723
wandb:                      mean_reward_22 -529.75678
wandb:                      mean_reward_23 -516.08127
wandb:                      mean_reward_24 -524.08613
wandb:                      mean_reward_25 -463.41541
wandb:                      mean_reward_26 -489.70427
wandb:                      mean_reward_27 -485.5725
wandb:                      mean_reward_28 -492.30739
wandb:                      mean_reward_29 -479.94762
wandb:                       mean_reward_3 -477.9851
wandb:                      mean_reward_30 -494.01578
wandb:                      mean_reward_31 -499.89792
wandb:                      mean_reward_32 -492.09565
wandb:                      mean_reward_33 -499.12007
wandb:                      mean_reward_34 -480.59567
wandb:                      mean_reward_35 -428.78819
wandb:                       mean_reward_4 -488.96287
wandb:                       mean_reward_5 -492.05985
wandb:                       mean_reward_6 -498.78517
wandb:                       mean_reward_7 -491.63813
wandb:                       mean_reward_8 -495.71076
wandb:                       mean_reward_9 -471.08202
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -935.32892
wandb:                        std_reward_0 217.07336
wandb:                        std_reward_1 199.24149
wandb:                       std_reward_10 193.84336
wandb:                       std_reward_11 220.42389
wandb:                       std_reward_12 216.09829
wandb:                       std_reward_13 157.60935
wandb:                       std_reward_14 213.47707
wandb:                       std_reward_15 184.84948
wandb:                       std_reward_16 185.44002
wandb:                       std_reward_17 195.59267
wandb:                       std_reward_18 216.91978
wandb:                       std_reward_19 207.83127
wandb:                        std_reward_2 151.85514
wandb:                       std_reward_20 192.24181
wandb:                       std_reward_21 210.4755
wandb:                       std_reward_22 233.73616
wandb:                       std_reward_23 204.41619
wandb:                       std_reward_24 226.74668
wandb:                       std_reward_25 165.91779
wandb:                       std_reward_26 210.69303
wandb:                       std_reward_27 185.50719
wandb:                       std_reward_28 198.50791
wandb:                       std_reward_29 199.7123
wandb:                        std_reward_3 198.25532
wandb:                       std_reward_30 216.86648
wandb:                       std_reward_31 214.17662
wandb:                       std_reward_32 182.91711
wandb:                       std_reward_33 215.0763
wandb:                       std_reward_34 205.27147
wandb:                       std_reward_35 145.91546
wandb:                        std_reward_4 202.73808
wandb:                        std_reward_5 202.46485
wandb:                        std_reward_6 210.18573
wandb:                        std_reward_7 196.26885
wandb:                        std_reward_8 177.92859
wandb:                        std_reward_9 175.97682
wandb:                            time/fps 600.0
wandb:                     train/approx_kl 0.01017
wandb:                 train/clip_fraction 0.12781
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.67882
wandb:            train/explained_variance 0.87368
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 7.2388
wandb:          train/policy_gradient_loss -0.01025
wandb:                           train/std 0.83444
wandb:                    train/value_loss 20.95161
wandb: 
wandb: Synced glowing-glitter-45: https://wandb.ai/tidiane/meta_rl_context/runs/2if2cb72
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 13 other file(s)
wandb: Find logs at: ./wandb/run-20230626_052650-2if2cb72/logs
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                PPO_1306/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1306/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1306/rollout/ep_rew_mean ▁▃▄▂▃▅▅▆▆▆▆█
wandb:                   PPO_1306/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1306/train/approx_kl ▄▁▆▃▅▆▅▅█▅▅
wandb:        PPO_1306/train/clip_fraction ▅▁▆▃▅▆▆▄▇▆█
wandb:           PPO_1306/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1306/train/entropy_loss ▁▂▃▄▄▅▅▆▆▇█
wandb:   PPO_1306/train/explained_variance ▆▅██▆▆▇▄▄▁▃
wandb:        PPO_1306/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1306/train/loss ▂▂▁▂▁▂▃▃▅█▃
wandb: PPO_1306/train/policy_gradient_loss ▁▇▂▇▃▄▁█▄▄▃
wandb:                  PPO_1306/train/std █▇▆▅▅▄▄▃▃▂▁
wandb:           PPO_1306/train/value_loss ▁▂▂▂▄▄▅██▆▅
wandb:                PPO_1316/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1316/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1316/rollout/ep_rew_mean ▁▁▁▂▃▃▄▅▆▆▆█
wandb:                   PPO_1316/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1316/train/approx_kl ▃▂▄▃▁▃▅▄▄▇█
wandb:        PPO_1316/train/clip_fraction ▁▂▅▁▄▄▄▅▆▆█
wandb:           PPO_1316/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1316/train/entropy_loss ▁▂▂▃▄▄▅▅▆▇█
wandb:   PPO_1316/train/explained_variance ▁▁▅▄▄▄▃▅▆▇█
wandb:        PPO_1316/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1316/train/loss ▂▃▃█▂▂▂▁▁▁▁
wandb: PPO_1316/train/policy_gradient_loss ▆▄▁█▅▅▃▇▆█▇
wandb:                  PPO_1316/train/std █▇▇▆▅▅▄▄▃▂▁
wandb:           PPO_1316/train/value_loss ▆▇▇███▆▃▁▁▃
wandb:                PPO_1326/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1326/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1326/rollout/ep_rew_mean ▁▃▃▅▅▅▇▆▇▆▆█
wandb:                   PPO_1326/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1326/train/approx_kl ▂▇▃██▅▄▁▄▁▇
wandb:        PPO_1326/train/clip_fraction ▅▇▇▆▆▆█▅▄▁▆
wandb:           PPO_1326/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1326/train/entropy_loss ▁▂▂▃▄▅▅▆▆▇█
wandb:   PPO_1326/train/explained_variance ▇▁▅▆▂▇▆█▆▅▅
wandb:        PPO_1326/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1326/train/loss ▂▃▄▅█▂▁▂▂▆▃
wandb: PPO_1326/train/policy_gradient_loss ▄▂▄▂▁▃▆▁▆▇█
wandb:                  PPO_1326/train/std █▇▇▆▅▄▄▃▃▂▁
wandb:           PPO_1326/train/value_loss ▂▄▁▃▇▆▁▄▅█▄
wandb:                PPO_1336/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1336/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1336/rollout/ep_rew_mean ▁▄▆▅▁▄▇▂▇▂▆█
wandb:                   PPO_1336/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1336/train/approx_kl ▃█▄▄▄▁█▇▁▁▆
wandb:        PPO_1336/train/clip_fraction ▁█▅▅▃▇▃▆▆▅▄
wandb:           PPO_1336/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1336/train/entropy_loss ▁▁▂▃▃▄▄▅▆▆█
wandb:   PPO_1336/train/explained_variance ▂▄▅▂▁▆▂▇▆██
wandb:        PPO_1336/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1336/train/loss ▄▅▁▂█▄▂▂▁▃▂
wandb: PPO_1336/train/policy_gradient_loss ▁█▆▃▆▅█▆▄▆▅
wandb:                  PPO_1336/train/std █▇▇▆▆▅▅▄▃▃▁
wandb:           PPO_1336/train/value_loss ▆▁▁▄▆▄▇▇▅▆█
wandb:                PPO_1346/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1346/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1346/rollout/ep_rew_mean ▄▅▅▁▅▆▄█▂▅█▇
wandb:                   PPO_1346/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1346/train/approx_kl ▆█▂█▇▇▂▁▅▃▅
wandb:        PPO_1346/train/clip_fraction ██▃▄▇█▂▃█▁▃
wandb:           PPO_1346/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1346/train/entropy_loss ▁▂▂▃▄▅▆▆▇▇█
wandb:   PPO_1346/train/explained_variance ▁▅▃▁▄▇▅█▅▇▇
wandb:        PPO_1346/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1346/train/loss ▃▂▄█▃▁▃▅▂█▃
wandb: PPO_1346/train/policy_gradient_loss ▁▂▄▂▆█▆▇▄▂▆
wandb:                  PPO_1346/train/std █▇▇▆▄▄▃▃▂▂▁
wandb:           PPO_1346/train/value_loss ▃▂▅▆▁▁▇▄▄█▅
wandb:                PPO_1356/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1356/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1356/rollout/ep_rew_mean ▅▃▁▄▆▇▂▄▇▆█▇
wandb:                   PPO_1356/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1356/train/approx_kl ▅▃▃▃▃▄▂▂▆▁█
wandb:        PPO_1356/train/clip_fraction ▆▂▃▄▃▇▄▁▆▅█
wandb:           PPO_1356/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1356/train/entropy_loss ▁▁▂▃▃▃▅▅▆▆█
wandb:   PPO_1356/train/explained_variance ▁▁▆▄▂▂▆▁███
wandb:        PPO_1356/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1356/train/loss ▄█▃▃▃▁▂▇▁▃▂
wandb: PPO_1356/train/policy_gradient_loss ▃▃▂▁▁▃▄▁█▄▅
wandb:                  PPO_1356/train/std ██▇▆▇▆▄▄▃▃▁
wandb:           PPO_1356/train/value_loss ▁▅▂▃▅▃▆█▂▄▄
wandb:                PPO_1366/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1366/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1366/rollout/ep_rew_mean ▇███▆▄▆▅▃▁▆▃
wandb:                   PPO_1366/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1366/train/approx_kl ▂▅█▅▆▁▆▅▂█▃
wandb:        PPO_1366/train/clip_fraction ▂▄▄▅▅▃▅▅▁█▃
wandb:           PPO_1366/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1366/train/entropy_loss ▁▁▂▂▁▂▃▄▅▇█
wandb:   PPO_1366/train/explained_variance ▆█▆▄█▅▆█▁▅▆
wandb:        PPO_1366/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1366/train/loss ▁▃▁▁▅▆▅▅█▃█
wandb: PPO_1366/train/policy_gradient_loss ▃▄▆██▅▆▆▁█▂
wandb:                  PPO_1366/train/std ▇█▇▇▇▆▆▄▄▂▁
wandb:           PPO_1366/train/value_loss ▂▁▂▂▁▅▂▂█▃▅
wandb:                PPO_1376/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1376/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1376/rollout/ep_rew_mean ▃▅▅▇▅▅██▁▄▅▃
wandb:                   PPO_1376/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1376/train/approx_kl ▁▁▂▂▂█▆▁▁▂▁
wandb:        PPO_1376/train/clip_fraction ▂▃▅▇▄▆█▁▅▆▆
wandb:           PPO_1376/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1376/train/entropy_loss ▁▂▃▃▄▅▅▅▆▇█
wandb:   PPO_1376/train/explained_variance ▅▄▃▂▁▅█▅▅▆▅
wandb:        PPO_1376/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1376/train/loss ▂▂▂▁▂▄▃▁▃██
wandb: PPO_1376/train/policy_gradient_loss ▃▁▃▃▃▆▇▃▆▅█
wandb:                  PPO_1376/train/std █▇▇▆▅▅▅▃▃▂▁
wandb:           PPO_1376/train/value_loss ▅▅▄▄█▅▁▆▆▄▇
wandb:                PPO_1386/global_step ▁▂▂▃▃▄▅▅▆▆▇█
wandb:        PPO_1386/rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        PPO_1386/rollout/ep_rew_mean █▅▄▁▃▂▇▇▇█▆▅
wandb:                   PPO_1386/time/fps █▄▃▂▂▂▁▁▁▁▁▁
wandb:            PPO_1386/train/approx_kl ▁▇▆▅▅▄▄▅▄█▆
wandb:        PPO_1386/train/clip_fraction ▁▆▄▅▃▁▃▄▄█▃
wandb:           PPO_1386/train/clip_range ▁▁▁▁▁▁▁▁▁▁▁
wandb:         PPO_1386/train/entropy_loss ▁▂▂▃▃▄▄▆▇▇█
wandb:   PPO_1386/train/explained_variance ▃▆█▆▇▆▁█▅▅▇
wandb:        PPO_1386/train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                 PPO_1386/train/loss ▃▅▂▂▁█▅▁▁█▂
wandb: PPO_1386/train/policy_gradient_loss ▄▅▃▄▄▂▁▆▅█▄
wandb:                  PPO_1386/train/std ██▇▆▆▆▄▃▂▂▁
wandb:           PPO_1386/train/value_loss ▃▅▂▅▂▃█▃▅▁▂
wandb:                    global_mean_eval ▁▄▆████▇▇█
wandb:                         global_step ▁▃▅▆████████████████████████████████████
wandb:                       mean_reward_0 ▁▄▆██▇█▇▇▇
wandb:                       mean_reward_1 ▁▄▆▇███▇▇▇
wandb:                      mean_reward_10 ▁▄▆▇██▇▇▇▇
wandb:                      mean_reward_11 ▁▄▆▇▇█▇▇██
wandb:                      mean_reward_12 ▁▄▆▇██▇▇▇█
wandb:                      mean_reward_13 ▁▄▆████▇▇▇
wandb:                      mean_reward_14 ▁▄▆▇██▇██▇
wandb:                      mean_reward_15 ▁▄▆███▇▇▇█
wandb:                      mean_reward_16 ▁▄▆▇█▇▇█▇▇
wandb:                      mean_reward_17 ▁▄▆▇███▇▇█
wandb:                      mean_reward_18 ▁▄▆███▇███
wandb:                      mean_reward_19 ▁▄▆████▇▇▇
wandb:                       mean_reward_2 ▁▄▆▇███▇▇█
wandb:                      mean_reward_20 ▁▄▆▇██▇▇▇▇
wandb:                      mean_reward_21 ▁▄▆██▇▇▇█▇
wandb:                      mean_reward_22 ▁▄▆████▆█▇
wandb:                      mean_reward_23 ▁▄▆▇███▇▇█
wandb:                      mean_reward_24 ▁▄▆▇███▆█▇
wandb:                      mean_reward_25 ▁▄▇████▆▇█
wandb:                      mean_reward_26 ▁▄▆▇█▇▇█▇█
wandb:                      mean_reward_27 ▁▄▆▇██▇▇██
wandb:                      mean_reward_28 ▁▄▆▇██▇▇▇▇
wandb:                      mean_reward_29 ▁▄▆████▇██
wandb:                       mean_reward_3 ▁▄▆███▇▇█▇
wandb:                      mean_reward_30 ▁▄▆███▇█▇█
wandb:                      mean_reward_31 ▁▄▆███▇▇▇▇
wandb:                      mean_reward_32 ▁▄▆███▇█▇█
wandb:                      mean_reward_33 ▁▄▆▇█▇█▇▇█
wandb:                      mean_reward_34 ▁▄▇▇██▇██▇
wandb:                      mean_reward_35 ▁▄▆███████
wandb:                       mean_reward_4 ▁▄▆████▇██
wandb:                       mean_reward_5 ▁▄▆▇▇▇█▆▇█
wandb:                       mean_reward_6 ▁▄▆▇███▇▇▇
wandb:                       mean_reward_7 ▁▄▆▇█▇█▇▇▇
wandb:                       mean_reward_8 ▁▄▆▇██▇▇▇█
wandb:                       mean_reward_9 ▁▄▆█████▇█
wandb:                 rollout/ep_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 rollout/ep_rew_mean ▁▁▂▃▁▂▂▃▄▄▅▆█
wandb:                        std_reward_0 ▂▁▁▂▂▅▆█▇▇
wandb:                        std_reward_1 ▂▁▂▂▂▅▆█▇▇
wandb:                       std_reward_10 ▂▁▂▂▂▅▇███
wandb:                       std_reward_11 ▂▁▂▂▃▄▆██▆
wandb:                       std_reward_12 ▂▁▁▁▂▅▆██▆
wandb:                       std_reward_13 ▂▁▁▁▂▅▅▇█▇
wandb:                       std_reward_14 ▂▁▂▂▂▅▇███
wandb:                       std_reward_15 ▂▁▁▂▂▅▆▇█▆
wandb:                       std_reward_16 ▂▁▂▂▂▅▇▇█▇
wandb:                       std_reward_17 ▂▁▁▂▂▄▅██▆
wandb:                       std_reward_18 ▂▁▁▁▂▅▆▇█▇
wandb:                       std_reward_19 ▂▁▁▂▂▄▆██▇
wandb:                        std_reward_2 ▂▁▁▂▂▅▆█▇▆
wandb:                       std_reward_20 ▂▁▁▁▂▄▆██▆
wandb:                       std_reward_21 ▂▁▁▁▁▆▇██▇
wandb:                       std_reward_22 ▂▁▁▁▂▄▆█▇▆
wandb:                       std_reward_23 ▂▁▁▁▂▄▆▇█▆
wandb:                       std_reward_24 ▂▁▁▂▂▄▅█▆▆
wandb:                       std_reward_25 ▂▁▂▂▂▅▆██▆
wandb:                       std_reward_26 ▂▁▁▂▃▅▇▇█▇
wandb:                       std_reward_27 ▂▁▁▂▂▅▇█▇▇
wandb:                       std_reward_28 ▃▁▂▂▃▆▆██▇
wandb:                       std_reward_29 ▂▁▁▂▂▅▅█▇▆
wandb:                        std_reward_3 ▂▁▂▂▂▆▇███
wandb:                       std_reward_30 ▁▁▁▂▂▅▆▇█▇
wandb:                       std_reward_31 ▂▁▁▂▂▅▆▇█▆
wandb:                       std_reward_32 ▂▁▁▁▂▅▆▆█▅
wandb:                       std_reward_33 ▂▁▁▁▂▅▅██▆
wandb:                       std_reward_34 ▂▁▁▂▃▅▆▇█▇
wandb:                       std_reward_35 ▂▁▂▂▂▆▆██▇
wandb:                        std_reward_4 ▂▁▁▂▂▅▅█▇▆
wandb:                        std_reward_5 ▃▁▁▂▂▄▅█▇▄
wandb:                        std_reward_6 ▂▁▂▁▂▅▅█▇▇
wandb:                        std_reward_7 ▂▁▁▂▂▅▅██▇
wandb:                        std_reward_8 ▂▁▂▂▂▅▆██▆
wandb:                        std_reward_9 ▁▁▁▁▂▆▆▇█▇
wandb:                            time/fps █▄▃▂▂▂▁▁▁▁▁▁▁
wandb:                     train/approx_kl ▂▃▄█▇▆▄▃▁▂▄▄
wandb:                 train/clip_fraction ▂▃▃▄▇▂▁▂▁▂▅█
wandb:                    train/clip_range ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  train/entropy_loss ▁▂▂▃▃▄▄▅▆▆▇█
wandb:            train/explained_variance ▁▁▁▁▁▁▁▁▁▆██
wandb:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                          train/loss █▇▄▃▂▂▁▁▁▁▁▁
wandb:          train/policy_gradient_loss ▆▄▅▅█▆▇▆▅▃▂▁
wandb:                           train/std █▇▇▆▆▅▄▄▃▂▂▁
wandb:                    train/value_loss █▇▅▄▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                PPO_1306/global_step 212992
wandb:        PPO_1306/rollout/ep_len_mean 200.0
wandb:        PPO_1306/rollout/ep_rew_mean -816.2121
wandb:                   PPO_1306/time/fps 581.0
wandb:            PPO_1306/train/approx_kl 0.01167
wandb:        PPO_1306/train/clip_fraction 0.16325
wandb:           PPO_1306/train/clip_range 0.2
wandb:         PPO_1306/train/entropy_loss -7.76121
wandb:   PPO_1306/train/explained_variance 0.94796
wandb:        PPO_1306/train/learning_rate 0.0003
wandb:                 PPO_1306/train/loss 36.41696
wandb: PPO_1306/train/policy_gradient_loss -0.00852
wandb:                  PPO_1306/train/std 0.73255
wandb:           PPO_1306/train/value_loss 99.7776
wandb:                PPO_1316/global_step 212992
wandb:        PPO_1316/rollout/ep_len_mean 200.0
wandb:        PPO_1316/rollout/ep_rew_mean -593.1886
wandb:                   PPO_1316/time/fps 579.0
wandb:            PPO_1316/train/approx_kl 0.01533
wandb:        PPO_1316/train/clip_fraction 0.1949
wandb:           PPO_1316/train/clip_range 0.2
wandb:         PPO_1316/train/entropy_loss -6.76734
wandb:   PPO_1316/train/explained_variance 0.97072
wandb:        PPO_1316/train/learning_rate 0.0003
wandb:                 PPO_1316/train/loss 18.46652
wandb: PPO_1316/train/policy_gradient_loss -0.00647
wandb:                  PPO_1316/train/std 0.63549
wandb:           PPO_1316/train/value_loss 58.5304
wandb:                PPO_1326/global_step 212992
wandb:        PPO_1326/rollout/ep_len_mean 200.0
wandb:        PPO_1326/rollout/ep_rew_mean -555.27991
wandb:                   PPO_1326/time/fps 577.0
wandb:            PPO_1326/train/approx_kl 0.01508
wandb:        PPO_1326/train/clip_fraction 0.19344
wandb:           PPO_1326/train/clip_range 0.2
wandb:         PPO_1326/train/entropy_loss -5.9572
wandb:   PPO_1326/train/explained_variance 0.96809
wandb:        PPO_1326/train/learning_rate 0.0003
wandb:                 PPO_1326/train/loss 22.88289
wandb: PPO_1326/train/policy_gradient_loss -0.00291
wandb:                  PPO_1326/train/std 0.56608
wandb:           PPO_1326/train/value_loss 62.91434
wandb:                PPO_1336/global_step 212992
wandb:        PPO_1336/rollout/ep_len_mean 200.0
wandb:        PPO_1336/rollout/ep_rew_mean -533.91345
wandb:                   PPO_1336/time/fps 577.0
wandb:            PPO_1336/train/approx_kl 0.01591
wandb:        PPO_1336/train/clip_fraction 0.19395
wandb:           PPO_1336/train/clip_range 0.2
wandb:         PPO_1336/train/entropy_loss -5.37655
wandb:   PPO_1336/train/explained_variance 0.98087
wandb:        PPO_1336/train/learning_rate 0.0003
wandb:                 PPO_1336/train/loss 17.70603
wandb: PPO_1336/train/policy_gradient_loss -0.00251
wandb:                  PPO_1336/train/std 0.52185
wandb:           PPO_1336/train/value_loss 77.82939
wandb:                PPO_1346/global_step 212992
wandb:        PPO_1346/rollout/ep_len_mean 200.0
wandb:        PPO_1346/rollout/ep_rew_mean -513.32068
wandb:                   PPO_1346/time/fps 577.0
wandb:            PPO_1346/train/approx_kl 0.01511
wandb:        PPO_1346/train/clip_fraction 0.18688
wandb:           PPO_1346/train/clip_range 0.2
wandb:         PPO_1346/train/entropy_loss -4.8728
wandb:   PPO_1346/train/explained_variance 0.98526
wandb:        PPO_1346/train/learning_rate 0.0003
wandb:                 PPO_1346/train/loss 33.21234
wandb: PPO_1346/train/policy_gradient_loss -0.00157
wandb:                  PPO_1346/train/std 0.48605
wandb:           PPO_1346/train/value_loss 88.65797
wandb:                PPO_1356/global_step 212992
wandb:        PPO_1356/rollout/ep_len_mean 200.0
wandb:        PPO_1356/rollout/ep_rew_mean -490.77365
wandb:                   PPO_1356/time/fps 576.0
wandb:            PPO_1356/train/approx_kl 0.021
wandb:        PPO_1356/train/clip_fraction 0.23682
wandb:           PPO_1356/train/clip_range 0.2
wandb:         PPO_1356/train/entropy_loss -4.30835
wandb:   PPO_1356/train/explained_variance 0.98901
wandb:        PPO_1356/train/learning_rate 0.0003
wandb:                 PPO_1356/train/loss 25.06101
wandb: PPO_1356/train/policy_gradient_loss -0.00016
wandb:                  PPO_1356/train/std 0.44732
wandb:           PPO_1356/train/value_loss 135.26982
wandb:                PPO_1366/global_step 212992
wandb:        PPO_1366/rollout/ep_len_mean 200.0
wandb:        PPO_1366/rollout/ep_rew_mean -537.90881
wandb:                   PPO_1366/time/fps 575.0
wandb:            PPO_1366/train/approx_kl 0.0144
wandb:        PPO_1366/train/clip_fraction 0.21248
wandb:           PPO_1366/train/clip_range 0.2
wandb:         PPO_1366/train/entropy_loss -4.02454
wandb:   PPO_1366/train/explained_variance 0.98803
wandb:        PPO_1366/train/learning_rate 0.0003
wandb:                 PPO_1366/train/loss 210.2908
wandb: PPO_1366/train/policy_gradient_loss -0.00063
wandb:                  PPO_1366/train/std 0.43117
wandb:           PPO_1366/train/value_loss 250.72
wandb:                PPO_1376/global_step 212992
wandb:        PPO_1376/rollout/ep_len_mean 200.0
wandb:        PPO_1376/rollout/ep_rew_mean -533.86444
wandb:                   PPO_1376/time/fps 576.0
wandb:            PPO_1376/train/approx_kl 0.01732
wandb:        PPO_1376/train/clip_fraction 0.2508
wandb:           PPO_1376/train/clip_range 0.2
wandb:         PPO_1376/train/entropy_loss -3.67376
wandb:   PPO_1376/train/explained_variance 0.98863
wandb:        PPO_1376/train/learning_rate 0.0003
wandb:                 PPO_1376/train/loss 266.7616
wandb: PPO_1376/train/policy_gradient_loss 0.00451
wandb:                  PPO_1376/train/std 0.40918
wandb:           PPO_1376/train/value_loss 309.63828
wandb:                PPO_1386/global_step 212992
wandb:        PPO_1386/rollout/ep_len_mean 200.0
wandb:        PPO_1386/rollout/ep_rew_mean -533.75421
wandb:                   PPO_1386/time/fps 575.0
wandb:            PPO_1386/train/approx_kl 0.02263
wandb:        PPO_1386/train/clip_fraction 0.25818
wandb:           PPO_1386/train/clip_range 0.2
wandb:         PPO_1386/train/entropy_loss -3.40334
wandb:   PPO_1386/train/explained_variance 0.99083
wandb:        PPO_1386/train/learning_rate 0.0003
wandb:                 PPO_1386/train/loss 67.47871
wandb: PPO_1386/train/policy_gradient_loss 0.00432
wandb:                  PPO_1386/train/std 0.39404
wandb:           PPO_1386/train/value_loss 247.91537
wandb:                    global_mean_eval -482.746
wandb:                         global_step 212992
wandb:                       mean_reward_0 -505.87851
wandb:                       mean_reward_1 -491.24766
wandb:                      mean_reward_10 -489.77542
wandb:                      mean_reward_11 -456.60061
wandb:                      mean_reward_12 -463.00189
wandb:                      mean_reward_13 -510.02652
wandb:                      mean_reward_14 -504.92905
wandb:                      mean_reward_15 -453.62503
wandb:                      mean_reward_16 -481.0767
wandb:                      mean_reward_17 -476.78736
wandb:                      mean_reward_18 -477.20529
wandb:                      mean_reward_19 -494.5621
wandb:                       mean_reward_2 -467.34303
wandb:                      mean_reward_20 -485.70781
wandb:                      mean_reward_21 -498.25618
wandb:                      mean_reward_22 -510.37371
wandb:                      mean_reward_23 -484.44498
wandb:                      mean_reward_24 -478.70604
wandb:                      mean_reward_25 -479.67113
wandb:                      mean_reward_26 -469.28269
wandb:                      mean_reward_27 -452.36965
wandb:                      mean_reward_28 -518.60769
wandb:                      mean_reward_29 -493.48027
wandb:                       mean_reward_3 -492.95853
wandb:                      mean_reward_30 -478.01261
wandb:                      mean_reward_31 -490.53171
wandb:                      mean_reward_32 -461.418
wandb:                      mean_reward_33 -457.17745
wandb:                      mean_reward_34 -507.74206
wandb:                      mean_reward_35 -484.39386
wandb:                       mean_reward_4 -474.73281
wandb:                       mean_reward_5 -416.98461
wandb:                       mean_reward_6 -491.69007
wandb:                       mean_reward_7 -519.60827
wandb:                       mean_reward_8 -477.79412
wandb:                       mean_reward_9 -482.85264
wandb:                 rollout/ep_len_mean 200.0
wandb:                 rollout/ep_rew_mean -935.32892
wandb:                        std_reward_0 214.892
wandb:                        std_reward_1 209.94877
wandb:                       std_reward_10 212.38388
wandb:                       std_reward_11 187.73337
wandb:                       std_reward_12 181.50613
wandb:                       std_reward_13 202.32466
wandb:                       std_reward_14 220.48591
wandb:                       std_reward_15 181.41019
wandb:                       std_reward_16 186.58904
wandb:                       std_reward_17 189.21552
wandb:                       std_reward_18 204.18918
wandb:                       std_reward_19 212.42168
wandb:                        std_reward_2 195.172
wandb:                       std_reward_20 198.1157
wandb:                       std_reward_21 202.62617
wandb:                       std_reward_22 205.94127
wandb:                       std_reward_23 188.11047
wandb:                       std_reward_24 198.05163
wandb:                       std_reward_25 197.40839
wandb:                       std_reward_26 196.95958
wandb:                       std_reward_27 194.78675
wandb:                       std_reward_28 210.32273
wandb:                       std_reward_29 208.39657
wandb:                        std_reward_3 214.71036
wandb:                       std_reward_30 213.36954
wandb:                       std_reward_31 186.4194
wandb:                       std_reward_32 175.02686
wandb:                       std_reward_33 180.86097
wandb:                       std_reward_34 224.51973
wandb:                       std_reward_35 185.28109
wandb:                        std_reward_4 191.83129
wandb:                        std_reward_5 144.78827
wandb:                        std_reward_6 219.26964
wandb:                        std_reward_7 221.37707
wandb:                        std_reward_8 195.34294
wandb:                        std_reward_9 205.98363
wandb:                            time/fps 600.0
wandb:                     train/approx_kl 0.01017
wandb:                 train/clip_fraction 0.12781
wandb:                    train/clip_range 0.2
wandb:                  train/entropy_loss -8.67882
wandb:            train/explained_variance 0.87368
wandb:                 train/learning_rate 0.0003
wandb:                          train/loss 7.2388
wandb:          train/policy_gradient_loss -0.01025
wandb:                           train/std 0.83444
wandb:                    train/value_loss 20.95161
wandb: 
wandb: Synced deft-thunder-45: https://wandb.ai/tidiane/meta_rl_context/runs/2y87u8r8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 13 other file(s)
wandb: Find logs at: ./wandb/run-20230626_052650-2y87u8r8/logs

real	252m31.971s
user	2467m34.648s
sys	8m57.267s
